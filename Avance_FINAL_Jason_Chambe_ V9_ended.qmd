---
title: "proyecto_grupo3"
format: html
editor: visual
---

```{r}
#hola
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(stringdist)
library(stringi)
```

```{r}
DF <- read.csv("Proyecto de estadística  (Respuestas) - Respuestas de formulario 1.csv") %>% #dirigido a nuestras 3 OEs escogidas 
  
select(-c(1:13,14,16,18,19,20,22,26,27,28,29,38))



```

```{r}
DF
```

```{r}
new_nombres <- c(
  'Dias de asistencia por semana',
  'Turno de ingreso usual',
  'Tipo de transporte principal',
  'Número total de medios utilizados',
  'Tiempo de viaje (min)',
  'Tiempo de espera para abordar (min)',
  'Tráfico percibido (1–5)',
  'Comodidad del transporte (1–5)',
  'Frecuencia con que llegas puntual',
  'Seguridad percibida (1–5)',
  'Principal problema al trasladarte',
  'Cansancio al llegar (1–5)',
  'Estrés percibido durante el viaje (1–5)',
  'Satisfacción con el tiempo que te toma llegar (1–5)'
)
```

```{r}
DF %>% rename_with(~new_nombres)
```

```{r}
DF <- DF %>% rename_with(~new_nombres)
```

```{r}
#pos to the last col, var : dias de asistencia por semana
library(dplyr)

DF <- DF %>%
  select(-`Dias de asistencia por semana`, `Dias de asistencia por semana`)

```

```{r}
summary(DF)
```

Por simple inspeccion vemos que se necesita limpiar las variables iniciales, que se permitio la entrada de caracteres por teclado

```{r}
#eliminamos las filas completamente nulas

DF <- DF %>%
  # convierte cadenas vacías y "NA" (texto) a NA reales
  mutate(across(where(is.character), ~na_if(trimws(.), ""))) %>%
  mutate(across(where(is.character), ~na_if(tolower(.), "na"))) %>%
  # elimina filas que tengan al menos un NA en cualquier columna
  drop_na()
  #explicar por que se hizo esto


```

Limpieza de la variable principal medio de transporte

```{r}
library(dplyr)
library(stringi)

# 1) Lista de valores inválidos (lo que NO queremos como "tipo de transporte")
bad_tokens <- c("si","no","0","1","2","3","4","5","6","7","8","9")

# 2) Guardamos el número de filas antes de filtrar (para reportar cuántas se eliminaron)
n_before <- nrow(DF)

DF <- DF %>%
  mutate(
    # 3) Creamos una columna temporal 'tipo_tmp' para normalizar el texto:
    #    - stri_trans_general(..., "Latin-ASCII") quita tildes: "sí" -> "si"
    #    - tolower() pasa todo a minúsculas: "Si" -> "si"
    #    - trimws() quita espacios sobrantes: "  si  " -> "si"
    tipo_tmp = stringi::stri_trans_general(`Tipo de transporte principal`, "Latin-ASCII"),
    tipo_tmp = tolower(trimws(tipo_tmp))
  ) %>%
  # 4) Nos quedamos solo con las filas donde 'tipo_tmp' NO esté en bad_tokens.
  #    Es decir, eliminamos filas con "si", "no" o un dígito simple 1–9.
  filter(!tipo_tmp %in% bad_tokens) %>%
  # 5) Quitamos la columna temporal porque ya no la necesitamos.
  select(-tipo_tmp)

# 6) Reporte: cuántas filas se eliminaron por este criterio
cat("Filas eliminadas por valor inválido en 'Tipo de transporte principal':",
    n_before - nrow(DF), "\n")
#na previo (valores absurdos)
```

```{r}
library(dplyr)
library(tibble)

library(dplyr)

# Nombre EXACTO de la columna original (tal como aparece en names(DF))
tpo_orig <- "Tipo de transporte principal"

# Verifica que exista
stopifnot(tpo_orig %in% names(DF))

# Duplica y coloca las dos copias justo a la derecha de la original
DF <- DF %>%
  mutate(
    `Tipo de transporte principal_backup` = .data[[tpo_orig]],
    `Tipo de transporte principal_clean`  = .data[[tpo_orig]]
  ) %>%
  relocate(
    `Tipo de transporte principal_backup`,
    `Tipo de transporte principal_clean`,
    .after = all_of(tpo_orig)
  )


# (Opcional) verifica el orden
which(names(DF) %in% c("Tipo de transporte principal",
                       "Tipo de transporte principal_backup",
                       "Tipo de transporte principal_clean"))

```

```{r}
# filas donde la celda es exactamente "buses" (ignora mayúsculas y espacios)
idx_buses <- which(grepl("^\\s*auto(taxi)\\s*$",
                         DF$`Tipo de transporte principal`,
                         ignore.case = TRUE))
idx_buses


DF[idx_buses, , drop = FALSE]

```

Limpieza de Tipo de transporte principal

```{r}
# --- Librerías
library(dplyr)
library(stringr)
library(stringi)
library(tibble)

# --- 0) Nombre exacto de la columna original
col_orig <- "Tipo de transporte principal"
stopifnot(col_orig %in% names(DF))

# --- 1) Duplicar: backup + clean (al lado de la original)
DF <- DF %>%
  mutate(
    `Tipo de transporte principal_backup` = .data[[col_orig]],
    `Tipo de transporte principal_clean`  = .data[[col_orig]]
  ) %>%
  relocate(`Tipo de transporte principal_backup`,
           `Tipo de transporte principal_clean`,
           .after = all_of(col_orig))

# --- 2) Normalizador auxiliar (ASCII, minúsculas, espacio simple, sin punto final)
normalize <- function(x){
  x %>%
    stri_trans_general("Latin-ASCII") %>%       # sí -> si ; eléctrico -> electrico
    str_to_lower() %>%                           # minúsculas
    str_replace_all("\\s+", " ") %>%             # espacios múltiples -> uno
    str_trim() %>%                               # quita espacios extremos
    str_remove("\\.*$")                          # quita punto final suelto
}

# --- 3) Listas EXACTAS (normalizadas) para mapping
bicicleta_vals <- normalize(c("bicicleta","bicicleta de citybikelima."))
tren_vals      <- normalize(c("línea 1","tren línea 1","tren electrico","tren eléctrico",
                              "tren","el tren eléctrico","tren linea1","linea 1","tren linea 1"))
bus_vals       <- normalize(c("corredor azul","bus","metropolitano","metropoloton",
                              "micro pe","micro","corredor rojo","corredor",
                              "bus (corredor azul)","autobus","chama"))
moto_vals      <- normalize(c("moto"))
auto_vals      <- normalize(c("carro","auto","uber","taxi","auto propio","coche",
                              "coche propio","auto particular","auto(taxi)"))
scooter_vals   <- normalize(c("scooter eléctrico","scooter","scooter electrico"))

# --- 4) Combinaciones que deben quedar SOLO en backup (en *_clean -> NA)
combos_backup_only <- normalize(c(
  "bus, auto","auto, bus","bus, auto.","auto y bus",
  "auto y metropolitano",
  "metro, bus",
  "bus, tren","bus y tren","tren y bus","bus, tren, metropolitano",
  "taxi,metropolitano","taxi, metropolitano","taxi y metropolitano",
  "bus y tren","bus, metropolitano","metropolitano, bus","bus, tren",
  "bus y metropolitano",
  "bus y tren electrico",
  "corredor rojo, metropolitano","corredor, metropolitano","corredor y metropolitano",
  "corredor rojo y metropolitano","metropolitano y corredor","metropolitano,colectivo,taxi",
  "metropolitano, bus","metropolitano y taxi",
  "colectivo, bus, metropolitano","bus y auto",
  "metropolitano y expreso san isidro","tren y micro",
  "bici y a pie","bus(corredores y ,micros)","bus,auto","bus ,auto","bus o combis",
  "bus, taxi, metro","auto,corredor rojo , corredor azul",
  "metropolitano y bus","metropolitano, corredor azul",
  "tren, bus y mototaxi","tren(linea 2), bus y metropolitano","tren electrico y bus",
  "linea 1 + translima 1113","combie y metropolitano",
  "tren y bus","bus y metropolitano","metropolitano y bus","bus y bicicleta",
  "micro y bicicleta","buses","bici y a pie"
))

# --- 5) Filas a ELIMINAR COMPLETAMENTE (disyunción o patrones prohibidos)
explicitos_borrar <- normalize(c(
  "auto o metropolitano","corredor y metropolitano","metropolitano/bus","bus o auto",
  "bus - auto","bus o metropolitano","bicicleta o bus. (ahora mas bicicleta)",
  "auto, bus, caminando, estoy cerca…","auto, bus, caminando, estoy cerca..."
))

# patrón disyuntivo general: " x o y ", "/" o "-" entre palabras
pat_disyuncion <- "(\\s+o\\s+|/|\\s+-\\s+)"

# --- 6) Construimos un campo normalizado auxiliar de la columna original
DF <- DF %>%
  mutate(tipo_norm = normalize(.data[[col_orig]]))

# --- 7) Mapping a clases en la COLUMNA *_clean (solo si NO es combo-backup-only)
DF <- DF %>%
  mutate(
    `Tipo de transporte principal_clean` = case_when(
      tipo_norm %in% combos_backup_only ~ NA_character_,                                 # combos: solo backup
      tipo_norm %in% bicicleta_vals     ~ "Bicicleta",
      tipo_norm %in% tren_vals          ~ "Tren",
      tipo_norm %in% bus_vals           ~ "Bus",
      tipo_norm %in% moto_vals          ~ "Moto",
      tipo_norm %in% auto_vals          ~ "Auto",
      tipo_norm %in% scooter_vals       ~ "Scooter",
      TRUE                              ~ `Tipo de transporte principal_clean`            # deja tal cual si no mapeó
    )
  )

# --- 8) Reescribir BACKUP: solo combos listados; resto = NA (no se eliminan filas)
DF <- DF %>%
  mutate(
    `Tipo de transporte principal_backup` = ifelse(
      tipo_norm %in% combos_backup_only, .data[[col_orig]], NA_character_
    )
  ) %>%
  # --- 9) Quitar auxiliar
  select(-tipo_norm)

```

fusionamos ambas columnas para tener una final total limpia

```{r}

library(dplyr)

DF <- DF %>%
  mutate(
    `Tipo de transporte_final` = coalesce(
      `Tipo de transporte principal_backup`,
      `Tipo de transporte principal_clean`
    )
  ) %>%
  relocate(`Tipo de transporte_final`, .after = `Tipo de transporte principal_clean`)

# Verificación estricta: NO debería haber NAs
stopifnot(sum(is.na(DF$`Tipo de transporte_final`)) == 0)

```

```{r}
library(dplyr)
#holaaaa
DF <- DF %>%
  mutate(
    `Tipo de transporte_final` = coalesce(
      `Tipo de transporte principal_backup`,
      `Tipo de transporte principal_clean`
    )
  ) %>%
  relocate(`Tipo de transporte_final`, .after = `Tipo de transporte principal_clean`)

# Verificación estricta: NO debería haber NAs
stopifnot(sum(is.na(DF$`Tipo de transporte_final`)) == 0)

```

Numero total de medios usados.

```{r}
# Crea "Numero total de medios utilizados_preclean" a la derecha de
# "Número total de medios utilizados", dejando NA en todo lo que
# no sea un entero >= 1 o tenga decimales/puntos/comas.

library(dplyr)
library(stringr)

col_medios <- "Número total de medios utilizados"
new_col    <- "Numero total de medios utilizados_preclean"

stopifnot(col_medios %in% names(DF))

DF <- DF %>%
  mutate(
    .raw  = as.character(.data[[col_medios]]),
    .trim = str_trim(.raw),

    # válido solo si son dígitos puros y el entero resultante es ≥ 1
    .is_int   = str_detect(.trim, "^[0-9]+$") & suppressWarnings(as.integer(.trim)) >= 1,

    # nueva columna: enteros válidos -> entero; inválidos (incluye 2.0, 2,0, etc.) -> NA
    !!new_col := ifelse(.is_int, as.integer(.trim), NA_integer_)
  ) %>%
  relocate(all_of(new_col), .after = all_of(col_medios)) %>%
  select(-.raw, -.trim, -.is_int)

# (Opcional) Verifica cuántos quedaron NA en la nueva columna
sum(is.na(DF[[new_col]]))

```

ver todas las entradas (todas las filas) de la variable "Tiempo de viaje (min)" con su número de fila:

```{r}
library(dplyr)
library(stringr)
library(stringi)

col_time <- "Tiempo de viaje (min)"
stopifnot(col_time %in% names(DF))

# --- (Usa las funciones que ya definimos; si no las tienes en el entorno, pega estas:)
norm_txt <- function(x){
  x %>%
    stringi::stri_trans_general("Latin-ASCII") %>%
    tolower() %>%
    str_replace_all("[\u2013\u2014]", "-") %>%
    str_replace_all("\\s+", " ") %>%
    str_trim()
}

parse_tiempo_min <- function(x){
  nx0 <- as.character(x)
  if (is.na(nx0) || nx0 == "") return(list(val = NA_real_, reason = "vacio"))
  nx <- norm_txt(nx0)

  has_range <- str_detect(nx, "(^|\\b)\\d+(?:[\\.,:]\\d+)?\\s*(?:h|hora|horas|m|min|mins|minuto|minutos)?\\b\\s*(?:-|\\ba\\b|\\bhasta\\b)\\s*\\d")
  has_or    <- str_detect(nx, "\\b\\d+\\s*o\\s*\\d+\\b")
  multi_seg <- str_detect(nx, "\\b\\d+\\s*(h|hora|horas|m|min|mins|minuto|minutos).+\\b(y|&)\\b.+\\d+\\s*(h|hora|horas|m|min|mins|minuto|minutos)")
  ctx_words <- str_detect(nx, "sin trafico|con trafico|depende|mejor de los casos|en el mejor de los casos|promedio")
  approx_no_unit <- str_detect(nx, "\\baprox") & !str_detect(nx, "h|hora|min|m\\b")

  if (!str_detect(nx, "\\d")) return(list(val = NA_real_, reason = "texto_puro"))

  if (str_detect(nx, "^\\d+(?:[\\.,]\\d+)?$")) {
    mv <- as.numeric(str_replace(nx, ",", "."))
    return(list(val = mv, reason = "numero_sin_unidad->min"))
  }

  if (str_detect(nx, "\\b\\d+\\s*(m|min|mins|minuto|minutos|minut|mintuos|minuts)\\b")) {
    if (has_range || has_or) return(list(val = NA_real_, reason = "rango"))
    if (multi_seg || ctx_words || approx_no_unit) return(list(val = NA_real_, reason = "contexto"))
    m <- as.numeric(str_extract(nx, "\\d+"))
    return(list(val = m, reason = "min"))
  }

  if (str_detect(nx, "\\b\\d+\\s*:\\s*\\d+")) {
    if (has_range || has_or) return(list(val = NA_real_, reason = "rango"))
    if (multi_seg || ctx_words || approx_no_unit) return(list(val = NA_real_, reason = "contexto"))
    h <- as.numeric(str_extract(nx, "\\d+(?=\\s*:)"))
    m <- as.numeric(str_extract(nx, "(?<=:)\\s*\\d+"))
    return(list(val = h*60 + m, reason = "hh:mm"))
  }

  if (str_detect(nx, "\\b\\d+\\s*h(ora|oras)?\\b") || str_detect(nx, "\\b\\d+\\s*con\\s*\\d+\\s*m(in|ins|minuto|minutos)\\b")) {
    if (has_range || has_or) return(list(val = NA_real_, reason = "rango"))
    if (multi_seg || ctx_words || approx_no_unit) return(list(val = NA_real_, reason = "contexto"))
    if (str_detect(nx, "\\b\\d+\\s*con\\s*\\d+\\s*m(in|ins|minuto|minutos)\\b")) {
      h <- as.numeric(str_extract(nx, "^\\d+"))
      m <- as.numeric(str_match(nx, "con\\s*(\\d+)\\s*m")[,2])
      return(list(val = h*60 + m, reason = "h+min_con"))
    }
    h <- as.numeric(str_extract(nx, "\\d+(?=\\s*h)"))
    m <- suppressWarnings(as.numeric(str_match(nx, "(\\d+)\\s*m(in|ins|minuto|minutos)\\b")[,2]))
    if (!is.na(m)) return(list(val = h*60 + m, reason = "h+min"))
    return(list(val = h*60, reason = "horas"))
  }

  if (str_detect(nx, "\\b\\d+[\\.,]\\d+\\s*h(ora|oras)\\b")) {
    if (has_range || has_or) return(list(val = NA_real_, reason = "rango"))
    if (multi_seg || ctx_words || approx_no_unit) return(list(val = NA_real_, reason = "contexto"))
    m2 <- str_match(nx, "\\b(\\d+)[\\.,](\\d+)\\s*h(ora|oras)\\b")
    H  <- as.numeric(m2[,2]); frac <- m2[,3]
    if (!is.na(H) && !is.na(frac)) {
      if (nchar(frac) <= 2 && as.numeric(frac) <= 59) {
        return(list(val = H*60 + as.numeric(frac), reason = "hh.mm->hh:mm"))
      } else {
        return(list(val = (H + as.numeric(paste0("0.", frac)))*60, reason = "horas_dec"))
      }
    }
  }

  if (has_range || has_or) return(list(val = NA_real_, reason = "rango"))
  if (multi_seg || ctx_words || approx_no_unit) return(list(val = NA_real_, reason = "contexto"))

  n1 <- suppressWarnings(as.numeric(str_replace(str_extract(nx, "\\d+"), ",", ".")))
  if (!is.na(n1)) return(list(val = n1, reason = "num_fallback->min"))

  list(val = NA_real_, reason = "no_parseable")
}

# --- PREVIEW (NO modifica DF)
tmp <- lapply(DF[[col_time]], parse_tiempo_min)

preview_tiempo <- tibble(
  fila     = seq_len(nrow(DF)),
  original = as.character(DF[[col_time]]),
  minutos  = vapply(tmp, function(z) z$val,    numeric(1)),
  motivo   = vapply(tmp, function(z) z$reason, character(1))
)

# Ver todo sin truncar
print(preview_tiempo, n = Inf, width = Inf)

# (opcional) Solo casos que quedarían en NA, para revisar rápido:
# preview_tiempo %>% filter(is.na(minutos)) %>% count(motivo, sort = TRUE)
# preview_tiempo %>% filter(is.na(minutos)) %>% select(fila, original, motivo) %>% print(n = Inf)

```

se procede con la adicion de un analisis a detalle luego de ver a detalle

|          |     |     |
|:---------|----:|-----|
| 1:40 min |   1 |     |

|                        |     |
|:-----------------------|----:|
| en promedio 80 minutos |  NA |

|         |     |     |
|:--------|----:|-----|
| 1:30min |   1 |     |

|            |      |         |
|:-----------|-----:|---------|
| 1.30 horas | 1800 | 546 row |

|                  |     |     |
|:-----------------|----:|-----|
| 2 horas y 30 min |  NA |     |

|                           |     |
|:--------------------------|----:|
| 180 minutos (en promedio) |  NA |

|                 |     |     |
|:----------------|----:|-----|
| sesenta minutos |  NA |     |

|              |     |     |
|:-------------|----:|-----|
| 1 h y 45 min |  NA |     |

|             |     |     |
|:------------|----:|-----|
| 1h y 30 min |  NA |     |

|                  |     |     |
|:-----------------|----:|-----|
| 1 con 20 minutos |   1 |     |

|      |     |     |
|:-----|----:|-----|
| 1:50 | 110 |     |

|            |     |     |
|:-----------|----:|-----|
| media hora |  NA |     |

|                      |     |
|:---------------------|----:|
| 50 bus, 30 bicicleta |  50 |

|           |     |
|:----------|----:|
| 180 aprox |  NA |

|                                                               |     |
|:--------------------------------------------------------------|----:|
| 18 min sin trafico, con trafico doble (no lo llefar a cmbiar) |  NA |

|                |     |
|:---------------|----:|
| 2 horas a 2:30 |  NA |

<div>

# EMPEZAMOS CON LA CORRECIÓN DE LA VARIABLE "Tiempo de viaje (min)"

</div>

```{r}
#creamos una copia por si acaso
DFT<- DF
DFT
```

```{r}
#creamos la copia de la columna Tiempo de viaje (min) a su derecha y le ponemos el nombre de "Tiempo de viaje clean"
DFT <- DFT %>%
  mutate(`Tiempo de viaje clean` = `Tiempo de viaje (min)`) %>%
  relocate(`Tiempo de viaje clean`, .after = `Tiempo de viaje (min)`)

# Mostrar el resultado
DFT

```

```{r}
#elimino los datos CORRECTOS, dejo los INCORRECTOS PARA CORREGIR EN EL SIGUIENTE CHUNK
DFT <- DFT %>%
  mutate(
    `Tiempo de viaje clean` = ifelse(
      # condición: el valor NO es numérico puro (tiene algo más que dígitos)
      !grepl("^\\d+$", `Tiempo de viaje (min)`),
      `Tiempo de viaje (min)`,
      NA
    )
  ) %>%
  relocate(`Tiempo de viaje clean`, .after = `Tiempo de viaje (min)`)

# Mostrar el resultado
DFT
```

# Creación de la columna: Tiempo de viaje (Datos erróneos)

```{r}
#NUEVA COLUMNA "Tiempo de viaje (Datos errónos)" para colocar los datos que se van a descartar por distintas razones

DFT <- DFT %>%
  mutate(`Tiempo de viaje (Datos erróneos)` = `Tiempo de viaje clean`) %>%
  relocate(`Tiempo de viaje (Datos erróneos)`, .after = `Tiempo de viaje clean`)

# Mostrar resultado
DFT
```

# Seguimos con la columna de Tiempo de viaje clean

```{r}
table(DFT$`Tiempo de viaje clean`)
```

```         
#DATOS A ELIMINAR EN LA COLUMNA "Tiempo de viaje clean"
#DATOS QUE SE QUEDAN EN LA COLUMNA "Tiempo de viaje (Datos erróneos)"

En promedio 80 minutos 
de 50 min a 1h 
De 30 a 50 minutos
Caminando 47 minutos y en bus ( contando lo que tardó en llegar al paradero) 30 minutos
50 bus, 30 bicicleta
30 - 40
30-50 minutos
30-35
3 o 4 horas 
20-30 min
2 horas a 2:30
180 minutos (en promedio) 
180 aprox 
18 min sin trafico, con trafico doble 
120 minuts a mas depende
120-180
100(en el mejor de los casos)
Si
```

```{r}
#Eliminar los datos separados anteriormente de la columna "Tiempo de viaje clean"
respuestas_invalidas <- c(
  "En promedio 80 minutos",
  "de 50 min a 1h",
  "De 30 a 50 minutos ",
  "Caminando 47 minutos y en bus ( contando lo que tardó en llegar al paradero) 30 minutos ",
  "50 bus, 30 bicicleta",
  "30 - 40",
  "30-50 minutos",
  "30-35",
  "3 o 4 horas ",
  "20-30 min",
  "2 horas a 2:30",
  "180 minutos (en promedio)",
  "180 aprox",
  "18 min sin trafico, con trafico doble",
  "120 minuts a mas depende",
  "120-180",
  "100(en el mejor de los casos)",
  "Aprox 1 hora ",
  "Si",
  ""
)

# Reemplazar esas respuestas por NA
DFT <- DFT %>%
  mutate(`Tiempo de viaje clean` = ifelse(
    `Tiempo de viaje clean` %in% respuestas_invalidas,
    NA,
    `Tiempo de viaje clean`
  ))

# Mostrar resultado
DFT
table(DFT$`Tiempo de viaje clean`)
```

```{r}
unique(DFT$`Tiempo de viaje clean`)
```

```{r}
#modificar los datos que se quedaron en la columna "Tiempo de viaje clean" por los datos en minutos(sin unidad de medida)
DFT$`Tiempo de viaje clean` <- DFT$`Tiempo de viaje clean` |>
  trimws() |>   # elimina espacios en blanco al inicio o final
  tolower() |>  # pasa todo a minúsculas para evitar problemas con "Sesenta minutos", etc.
  (\(x) replace(x, grepl("90 minutos|90min", x), "90"))() |>
  (\(x) replace(x, grepl("^3 horas", x), "180"))() |>
  (\(x) replace(x, grepl("^40 ?min", x), "40"))() |>
  (\(x) replace(x, grepl("^30 ?min", x), "30"))() |>
  (\(x) replace(x, grepl("1:? ?56", x), "116"))() |>
  (\(x) replace(x, grepl("^10 ?min", x), "10"))() |>
  (\(x) replace(x, grepl("^120", x), "120"))() |>
  (\(x) replace(x, grepl("media hora", x), "30"))() |>
  (\(x) replace(x, grepl("^1 hora$", x), "60"))() |>
  (\(x) replace(x, grepl("^20 ?min", x), "20"))() |>
  (\(x) replace(x, grepl("^2h", x), "120"))() |>
  (\(x) replace(x, grepl("^1:50", x), "110"))() |>
  (\(x) replace(x, grepl("1 con 20 minutos", x), "80"))() |>
  (\(x) replace(x, grepl("1 h.*45", x), "105"))() |>
  (\(x) replace(x, grepl("1h y 30", x), "90"))() |>
  (\(x) replace(x, grepl("sesenta minutos", x), "60"))() |>
  (\(x) replace(x, grepl("50 mintuos", x), "50"))() |>
  (\(x) replace(x, grepl("^2 horas y 30", x), "150"))() |>
  (\(x) replace(x, grepl("^2 horas", x), "120"))() |>
  (\(x) replace(x, grepl("^15 ?min", x), "15"))() |>
  (\(x) replace(x, grepl("^1\\.30 horas", x), "90"))() |>
  (\(x) replace(x, grepl("^45", x), "45"))() |>
  (\(x) replace(x, grepl("^1:30", x), "90"))() |>
  (\(x) replace(x, grepl("^1:40", x), "100"))() |>
  (\(x) replace(x, grepl("^60", x), "60"))() |>
  (\(x) replace(x, grepl("^65", x), "65"))() |>
  (\(x) replace(x, grepl("^2:30", x), "150"))() |>
  (\(x) replace(x, grepl("^1h$", x), "60"))()
DFT
```

# edicion TIEMPO DE VIAJE (DATOS ERRÓNEOS)

```{r}
unique(DFT$`Tiempo de viaje (Datos erróneos)`)
```

```{r}
#DEJA LOS DATOS VALIDOS PARA LA COLUMNA(osea, los datos ERRÓNEOS, datos inexactos)
valores_validos <- c(
  "En promedio 80 minutos",
  "de 50 min a 1h",
  "De 30 a 50 minutos ",
  "Caminando 47 minutos y en bus ( contando lo que tardó en llegar al paradero) 30 minutos ",
  "50 bus, 30 bicicleta",
  "30 - 40",
  "30-50 minutos",
  "30-35",
  "3 o 4 horas ",
  "20-30 min",
  "2 horas a 2:30",
  "180 minutos (en promedio)",
  "180 aprox",
  "18 min sin trafico, con trafico doble",
  "120 minuts a mas depende",
  "120-180",
  "100(en el mejor de los casos)",
  "Aprox 1 hora ",
  "Si",
  ""
)

DFT$`Tiempo de viaje (Datos erróneos)` <- ifelse(
  DFT$`Tiempo de viaje (Datos erróneos)` %in% valores_validos,
  DFT$`Tiempo de viaje (Datos erróneos)`,
  NA
)

DFT
```

```{r}
unique(DFT$`Tiempo de viaje (Datos erróneos)`)

```

```{r}
DFT
```

# CREAMOS LA COLUMNA FINAL (datos originales + datos corregidos, los erróneos se convierten en NA)

```{r}
# Crear la copia
DFT$`Tiempo de viaje FINAL` <- DFT$`Tiempo de viaje (min)`

# Reordenar para colocarla dos columnas a la derecha de la original
col_pos <- which(names(DFT) == "Tiempo de viaje (min)")  # posición de la columna original
nombres <- names(DFT)

# Mover la nueva columna dos posiciones a la derecha
nuevo_orden <- append(nombres[-length(nombres)], "Tiempo de viaje FINAL", after = col_pos + 1)
DFT <- DFT[, nuevo_orden]
DFT
```

```{r}
#COLOCAR LA NUEVA COLUMNA "Tiempo de viaje FINAL" en el lugar correcto (AL LADO DE TIEMPO DE VIAJE (DATOS ERRONEOS))

nombres <- names(DFT)

# Encontrar la posición de la columna "Tiempo de viaje FINAL"
col_pos <- which(nombres == "Tiempo de viaje FINAL")

# Moverla una posición a la derecha
if (col_pos < length(nombres)) {
  nuevo_orden <- append(nombres[-col_pos], nombres[col_pos], after = col_pos)
  DFT <- DFT[, nuevo_orden]
}
DFT
```

```{r}
#ELIMINANOS LOS DATOS ERRÓNEOS (no son validos para la columna final)
valores_erroneos <- c(
  "En promedio 80 minutos",
  "de 50 min a 1h",
  "De 30 a 50 minutos ",
  "Caminando 47 minutos y en bus ( contando lo que tardó en llegar al paradero) 30 minutos ",
  "50 bus, 30 bicicleta",
  "30 - 40",
  "30-50 minutos",
  "30-35",
  "3 o 4 horas ",
  "20-30 min",
  "2 horas a 2:30",
  "180 minutos (en promedio)",
  "180 aprox",
  "18 min sin trafico, con trafico doble",
  "120 minuts a mas depende",
  "120-180",
  "100(en el mejor de los casos)",
  "Aprox 1 hora ",
  "Si",
  ""
)

# Reemplazar por NA en la columna Tiempo de viaje FINAL
DFT$`Tiempo de viaje FINAL` <- ifelse(
  DFT$`Tiempo de viaje FINAL` %in% valores_erroneos,
  NA,
  DFT$`Tiempo de viaje FINAL`
)
DFT
```

```{r}
#CORREGIMOS LOS DATOS Q SE PUSIERON EN LA COLUMNA CLEAN
# Convertir los valores de "Tiempo de viaje FINAL" a minutos
DFT$`Tiempo de viaje FINAL` <- DFT$`Tiempo de viaje FINAL` |> 
  trimws() |> 
  dplyr::recode(
    "90 minutos" = "90",
    "3 horas" = "180",
    "40 min" = "40",
    "30 min" = "30",
    "1: 56" = "116",
    "10 minutos" = "10",
    "120 minutos " = "120",
    "30 minutos " = "30",
    "media hora" = "30",
    "1 hora" = "60",
    "20min" = "20",
    "2h" = "120",
    "40min" = "40",
    "1:50" = "110",
    "1 con 20 minutos" = "80",
    "1 h y 45 min" = "105",
    "1h y 30 min" = "90",
    "sesenta minutos" = "60",
    "50 mintuos" = "50",
    "2 horas " = "120",
    "2 horas" = "120",
    "15 mins" = "15",
    "2 horas y 30 min" = "150",
    "1:20" = "80",
    "15 minutos" = "15",
    "1.30 horas" = "90",
    "45MIN" = "45",
    "10 minutos " = "10",
    "1:30min" = "90",
    "1:40 min" = "100",
    "3 horas " = "180",
    "60 min" = "60",
    "65 min" = "65",
    "90min" = "90",
    "60 mins" = "60",
    "2:30" = "150",
    "1h" = "60"
  )

# Y LO CONVERTIMOS EN DATOS NUMERICOS
DFT$`Tiempo de viaje FINAL` <- as.numeric(DFT$`Tiempo de viaje FINAL`)
DFT
```

```{r}
#write.table(DFT[["Tiempo de espera para abordar (min)"]],
            #"~/Desktop/archivo.txt",
            #row.names = FALSE,
            #col.names = FALSE,
            #quote = FALSE)


```

```{r}
summary(DFT)
```

------------------------------------------------------------------------

Limpieza Variable Tiempo de espera para abordar

## Limpieza: Respaldo la variable original , propuesta de los casos con solución unívoca y Normalizado

## Variable : "tiempo de espera para abordar"

```{r}
# Paquetes
suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
  library(tidyr)
})

# ============
# 1) Preparación
# ============
# Copia de trabajo
DFT2 <- DFT

# Nombre de la columna objetivo (tal cual aparece en tu data)
col_obj <- "Tiempo de espera para abordar (min)"

# Backup intacto
DFT2 <- DFT2 %>%
  mutate(Tiempo_espera_backup = .data[[col_obj]])

# Normalizador base de strings
norm_txt <- function(x) {
  x %>%
    as.character() %>%
    str_trim() %>%
    str_squish() %>%
    str_to_lower()
}

txt <- norm_txt(DFT2[[col_obj]])

# ============
# 2) Reglas determinísticas por columna
#    Cada Sol_k solo llena filas que cumplen SU regla; el resto = NA
#    El valor siempre se calcula desde el texto original (no pisa otras Sol_k)
# ============

is_numeric_puro <- function(s) {
  # Solo dígitos (y opcionalmente decimal), sin letras ni símbolos
  str_detect(s, "^\\d+(?:[\\.,]\\d+)?$")
}
parse_numeric <- function(s) {
  # Sustituye coma decimal por punto y convierte
  as.numeric(str_replace(s, ",", "."))
}

# --- Sol_1: numéricos puros válidos (>= 0)
Sol_1 <- ifelse(is_numeric_puro(txt), parse_numeric(txt), NA_real_)

# --- Sol_2: num + unidad explícita (min|minuto|minutos|min.)
pat_unidades <- "(min(?:utos?)?\\.?|mins?\\.?|min\\b)"
is_num_con_unidad <- function(s) {
  str_detect(s, paste0("^\\s*\\d+(?:[\\.,]\\d+)?\\s*", pat_unidades, "\\s*$"))
}
extract_num_unidad <- function(s) {
  v <- str_match(s, paste0("^(\\d+(?:[\\.,]\\d+)?)\\s*", pat_unidades, "\\s*$"))[,2]
  as.numeric(str_replace(v, ",", "."))
}
Sol_2 <- ifelse(is_num_con_unidad(txt), extract_num_unidad(txt), NA_real_)

# --- Sol_3: forma compacta sin espacio (p.ej., 10min, 15min, 3min)
is_compacto_min <- function(s) {
  str_detect(s, "^\\s*\\d+(?:[\\.,]\\d+)?\\s*min\\s*$")
}
# NOTA: el patrón anterior también capturaría "10 min". Para evitar solape con Sol_2,
# aquí exigimos SIN espacio explícito entre número y 'min' en el texto original:
is_compacto_min_stricto <- function(s) {
  str_detect(s, "^\\s*\\d+(?:[\\.,]\\d+)?min\\s*$")
}
extract_compacto_min <- function(s) {
  v <- str_match(s, "^(\\d+(?:[\\.,]\\d+)?)min\\s*$")[,2]
  as.numeric(str_replace(v, ",", "."))
}
Sol_3 <- ifelse(is_compacto_min_stricto(txt), extract_compacto_min(txt), NA_real_)

# --- Sol_4: horas (1h, 2 h, 1hr, 2hrs) -> minutos
is_horas <- function(s) {
  str_detect(s, "^\\s*\\d+(?:[\\.,]\\d+)?\\s*h(?:rs?)?\\s*$")
}
extract_horas <- function(s) {
  v <- str_match(s, "^(\\d+(?:[\\.,]\\d+)?)(?:\\s*)h(?:rs?)?\\s*$")[,2]
  as.numeric(str_replace(v, ",", ".")) * 60
}
Sol_4 <- ifelse(is_horas(txt), extract_horas(txt), NA_real_)

# --- Sol_5: números en texto (casos presentes en tu columna; ampliable si lo deseas)
mapa_texto_a_num <- c(
  "uno"=1,"una"=1,"dos"=2,"tres"=3,"cuatro"=4,"cinco"=5,"seis"=6,"siete"=7,
  "ocho"=8,"nueve"=9,"diez"=10,"once"=11,"doce"=12,"trece"=13,"catorce"=14,
  "quince"=15,"dieciseis"=16,"dieciséis"=16,"diecisiete"=17,"dieciocho"=18,"diecinueve"=19,
  "veinte"=20,"treinta"=30,"cuarenta"=40,"cincuenta"=50,"sesenta"=60,"setenta"=70,
  "setenta y cinco"=75,"setentaycinco"=75 # por si viniera pegado
)
is_textual_num <- function(s) {
  s %in% names(mapa_texto_a_num)
}
extract_textual_num <- function(s) {
  unname(mapa_texto_a_num[s])
}
Sol_5 <- ifelse(is_textual_num(txt), extract_textual_num(txt), NA_real_)

# ============
# 3) Columna combinada pedida:
#    "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas"
#    - Rangos / alternativas -> promedio
#    - Ruido / no interpretable / no uso -> NA
# ============

# Detectores de rangos/alternativas
is_rango_guion <- function(s) str_detect(s, "^\\s*\\d+(?:[\\.,]\\d+)?\\s*-\\s*\\d+(?:[\\.,]\\d+)?\\s*$")
is_alt_o      <- function(s) str_detect(s, "^\\s*\\d+(?:[\\.,]\\d+)?\\s*o\\s*\\d+(?:[\\.,]\\d+)?\\s*$")
is_alt_u      <- function(s) str_detect(s, "^\\s*\\d+(?:[\\.,]\\d+)?\\s*u\\s*\\d+(?:[\\.,]\\d+)?\\s*$")
is_rango_de_a <- function(s) str_detect(s, "^\\s*de\\s*\\d+(?:[\\.,]\\d+)?\\s*a\\s*\\d+(?:[\\.,]\\d+)?\\s*$")

avg_2nums <- function(a, b) (a + b)/2

parse_promedio_rango <- function(s) {
  if (is_rango_guion(s)) {
    m <- str_match(s, "^(\\d+(?:[\\.,]\\d+)?)\\s*-\\s*(\\d+(?:[\\.,]\\d+)?)$")
    a <- as.numeric(str_replace(m[,2], ",", "."))
    b <- as.numeric(str_replace(m[,3], ",", "."))
    return(avg_2nums(a, b))
  }
  if (is_alt_o(s)) {
    m <- str_match(s, "^(\\d+(?:[\\.,]\\d+)?)\\s*o\\s*(\\d+(?:[\\.,]\\d+)?)$")
    a <- as.numeric(str_replace(m[,2], ",", "."))
    b <- as.numeric(str_replace(m[,3], ",", "."))
    return(avg_2nums(a, b))
  }
  if (is_alt_u(s)) {
    m <- str_match(s, "^(\\d+(?:[\\.,]\\d+)?)\\s*u\\s*(\\d+(?:[\\.,]\\d+)?)$")
    a <- as.numeric(str_replace(m[,2], ",", "."))
    b <- as.numeric(str_replace(m[,3], ",", "."))
    return(avg_2nums(a, b))
  }
  if (is_rango_de_a(s)) {
    m <- str_match(s, "^de\\s*(\\d+(?:[\\.,]\\d+)?)\\s*a\\s*(\\d+(?:[\\.,]\\d+)?)$")
    a <- as.numeric(str_replace(m[,2], ",", "."))
    b <- as.numeric(str_replace(m[,3], ",", "."))
    return(avg_2nums(a, b))
  }
  return(NA_real_)
}

# Ruido / no uso
is_no_uso <- function(s) {
  str_detect(s, "no\\s+utilizo") | str_detect(s, "no\\s+uso") 
}
is_ruido <- function(s) {
  s %in% c("q5","depende")
}

Sol_6_comb <- map_dbl(txt, function(s) {
  # Primero rangos/alternativas -> promedio
  v <- parse_promedio_rango(s)
  if (!is.na(v)) return(v)
  # Luego ruido/no uso -> NA explícito
  if (is_no_uso(s) || is_ruido(s)) return(NA_real_)
  # Caso "0 o 10" ya cae en alternativas y se promedia
  # Si no es ninguno, NA
  NA_real_
})

# ============
# 4) Ensamble en DFT2 con NA donde no aplica la regla
#    (cada columna es "una única manera de solución")
# ============
DFT2 <- DFT2 %>%
  mutate(
    Sol_1 = Sol_1,
    Sol_2 = ifelse(is.na(Sol_1), Sol_2, NA_real_),  # Exclusión lógica por altura
    Sol_3 = ifelse(is.na(Sol_1) & is.na(Sol_2), Sol_3, NA_real_),
    Sol_4 = ifelse(is.na(Sol_1) & is.na(Sol_2) & is.na(Sol_3), Sol_4, NA_real_),
    Sol_5 = ifelse(is.na(Sol_1) & is.na(Sol_2) & is.na(Sol_3) & is.na(Sol_4), Sol_5, NA_real_),
    `Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas` =
      ifelse(is.na(Sol_1) & is.na(Sol_2) & is.na(Sol_3) & is.na(Sol_4) & is.na(Sol_5),
             Sol_6_comb, NA_real_)
  )

# ============
# 5) Consolidación final y validaciones
# ============
# Primer no-NA a la derecha
DFT2 <- DFT2 %>%
  mutate(
    Tiempo_espera_min_final = coalesce(Sol_1, Sol_2, Sol_3, Sol_4, Sol_5,
      `Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas`)
  )

# Validaciones (>=0, umbrales razonables; marcamos outliers si exceden, p. ej., 180 min)
umbral_sup <- 180
DFT2 <- DFT2 %>%
  mutate(
    es_numerico_final = !is.na(Tiempo_espera_min_final),
    es_no_negativo    = ifelse(es_numerico_final, Tiempo_espera_min_final >= 0, NA),
    es_razonable      = ifelse(es_numerico_final, Tiempo_espera_min_final <= umbral_sup, NA),
    flag_outlier      = ifelse(es_numerico_final & Tiempo_espera_min_final > umbral_sup, TRUE, FALSE)
  )

# ============
# 6) Auditoría
# ============
auditoria <- tibble(
  regla = c("Sol_1 (num puro)",
            "Sol_2 (num + unidad)",
            "Sol_3 (compacto '10min')",
            "Sol_4 (horas a min)",
            "Sol_5 (texto numérico)",
            "Sol_6 (ruido/no uso/rangos_promedio)"),
  resueltas = c(
    sum(!is.na(DFT2$Sol_1)),
    sum(!is.na(DFT2$Sol_2)),
    sum(!is.na(DFT2$Sol_3)),
    sum(!is.na(DFT2$Sol_4)),
    sum(!is.na(DFT2$Sol_5)),
    sum(!is.na(DFT2$`Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas`))
  )
) %>%
  mutate(porcentaje = round(100 * resueltas / nrow(DFT2), 1))

pendientes <- sum(is.na(DFT2$Tiempo_espera_min_final))

# Chequeo de exclusividad por “altura” (no debería haber más de una solución por fila)
exclusividad_ok <- with(DFT2, {
  filas_con_multiples <- rowSums(!is.na(select(DFT2, Sol_1,
                                               Sol_2, Sol_3, Sol_4, Sol_5,
                                               `Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas`))) <= 1
  all(filas_con_multiples)
})

list(
  resumen_auditoria = auditoria,
  pendientes_sin_resolver = pendientes,
  exclusividad_ok = exclusividad_ok
)

```

------------------------------------------------------------------------

Limpieza problema al trasladarte

```{r}
#write.table(DFT[["Principal problema al trasladarte"]],
            #"~/Desktop/archivo211.txt",
            #row.names = FALSE,
            #col.names = FALSE,
            #quote = FALSE)

```

## Limpieza: Respaldo la variable original , propuesta de los casos con solución unívoca y Normalizado

## Variable : "Principal problema al trasladarte"

```{r}
suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
  library(tidyr)
})

# =========================================
# 0) Preparación (DFT2 como copia de DFT)
#    Se asume que DFT2 ya existe con 'flag_outlier' (de tu limpieza anterior).
# =========================================
# Si no existiera, descomenta la siguiente línea:
# DFT2 <- DFT

col_obj <- "Principal problema al trasladarte"   # <— CAMBIADO

# Backup del texto original  <— CAMBIADO
DFT2 <- DFT2 %>%
  mutate(Principal_problema_para_trasladarte_backup = .data[[col_obj]])

# Normalizador de texto (lower, trim, espacios)
norm_txt <- function(x) {
  x %>%
    as.character() %>%
    str_to_lower() %>%
    str_replace_all("\\s+", " ") %>%
    str_trim()
}

txt <- norm_txt(DFT2[[col_obj]])

# ==================================================
# 1) Diccionario de categorías canónicas y sinónimos
# ==================================================
CATS <- c(
  "tráfico",
  "tiempo de viaje",
  "falta de puntualidad del transporte",
  "inseguridad",
  "alto costo",
  "tiempo de espera",
  "aglomeración",
  "falta de infraestructura",
  "falta de ciclovías",
  "todos",
  "ninguno"
)

# map_token: usa s00 (antes s0)  <— CAMBIADO
map_token <- function(s) {
  s00 <- s %>% str_trim()

  # General
  if (s00 %in% c("todos", "todos los anteriores", "todos los anteriores, excepto alto costo", "todos los anteriores excepto alto costo"))
    return("todos")
  if (s00 %in% c("ninguno")) return("ninguno")

  # Tráfico
  if (str_detect(s00, "traf")) return("tráfico")
  if (str_detect(s00, "cruzar la pista")) return("tráfico")

  # Tiempo de viaje
  if (str_detect(s00, "tiempo de viaje")) return("tiempo de viaje")
  if (str_detect(s00, "salir temprano para llegar a tiempo")) return("tiempo de viaje")

  # Falta de puntualidad del transporte
  if (str_detect(s00, "falta de puntualidad")) return("falta de puntualidad del transporte")
  if (str_detect(s00, "fallas? y demoras?.*tren")) return("falta de puntualidad del transporte")

  # Inseguridad
  if (str_detect(s00, "insegur")) return("inseguridad")
  if (str_detect(s00, "enfermit")) return("inseguridad")
  if (str_detect(s00, "manejan como bestias")) return("inseguridad")

  # Alto costo
  if (str_detect(s00, "alto costo|costo alto|costo")) return("alto costo")

  # Tiempo de espera
  if (str_detect(s00, "tiempo de espera")) return("tiempo de espera")
  if (str_detect(s00, "cola( |$)|cola para|cola en")) return("tiempo de espera")
  if (str_detect(s00, "paradero")) return("tiempo de espera")

  # Aglomeración
  if (str_detect(s00, "aglomeraci|buses muy llenos|muy llenos")) return("aglomeración")

  # Falta de infraestructura
  if (str_detect(s00, "falta de infraestructura")) return("falta de infraestructura")

  # Falta de ciclovías
  if (str_detect(s00, "ciclov")) return("falta de ciclovías")

  return(NA_character_)
}

# =======================================================
# 2) Tokenización por comas y mapeo a categorías canónicas
# =======================================================
split_tokens <- function(s) {
  if (str_detect(s, ",")) str_split(s, ",")[[1]] %>% str_trim() else s
}

mapped_list <- map(txt, function(s) {
  toks <- split_tokens(s)
  cats <- map_chr(toks, map_token)
  cats <- unique(cats[!is.na(cats)])
  if (length(cats) == 0) return(NA_character_)
  paste(cats, collapse = "; ")
})

mapped_vec <- unlist(mapped_list)

# ==========================================================
# 3) Reglas únicas por columna (Sol_ppat_1 ... Sol_ppat_5)
# ==========================================================
tiene_coma        <- str_detect(txt, ",")
es_todos          <- mapped_vec == "todos"
es_ninguno        <- mapped_vec == "ninguno"
num_cats          <- ifelse(is.na(mapped_vec), NA_integer_,
                            str_count(mapped_vec, ";") + 1)

es_exacto_canonico <- function(s) s %in% CATS

# --- Sol_ppat_1: un canónico exacto, sin coma
Sol_ppat_1 <- ifelse(!tiene_coma & es_exacto_canonico(txt), txt, NA_character_)

# --- Sol_ppat_2: múltiples categorías (con comas)
Sol_ppat_2 <- ifelse(tiene_coma & !is.na(mapped_vec) & num_cats >= 2, mapped_vec, NA_character_)

# --- Sol_ppat_3: frase sinónima (sin coma) mapeada a 1 categoría
frase_mapea_1 <- (!tiene_coma) & !es_exacto_canonico(txt) & !is.na(mapped_vec) & (num_cats == 1)
Sol_ppat_3 <- ifelse(frase_mapea_1, mapped_vec, NA_character_)

# --- Sol_ppat_4: generales (todos/ninguno)
Sol_ppat_4 <- ifelse(!is.na(mapped_vec) & (es_todos | es_ninguno), mapped_vec, NA_character_)

# --- Sol_ppat_5: otros/ruido (queda NA por diseño)
ya_resueltas <- !is.na(Sol_ppat_1) | !is.na(Sol_ppat_2) | !is.na(Sol_ppat_3) | !is.na(Sol_ppat_4)
Sol_ppat_5 <- ifelse(!ya_resueltas, NA_character_, NA_character_)

# Ensamble en DFT2
DFT2 <- DFT2 %>%
  mutate(
    Sol_ppat_1 = Sol_ppat_1,
    Sol_ppat_2 = ifelse(is.na(Sol_ppat_1), Sol_ppat_2, NA_character_),
    Sol_ppat_3 = ifelse(is.na(Sol_ppat_1) & is.na(Sol_ppat_2), Sol_ppat_3, NA_character_),
    Sol_ppat_4 = ifelse(is.na(Sol_ppat_1) & is.na(Sol_ppat_2) & is.na(Sol_ppat_3), Sol_ppat_4, NA_character_),
    Sol_ppat_5 = ifelse(is.na(Sol_ppat_1) & is.na(Sol_ppat_2) & is.na(Sol_ppat_3) & is.na(Sol_ppat_4), Sol_ppat_5, NA_character_),
    Tiempo_traslado_final = coalesce(Sol_ppat_1, Sol_ppat_2, Sol_ppat_3, Sol_ppat_4, Sol_ppat_5)
  )

# ============================================
# 4) Dummies (one-hot) para análisis posteriores
# ============================================
parse_final <- str_split(DFT2$Tiempo_traslado_final %||% NA_character_, ";\\s*")

# Crear nombres de dummies (normalizados)
dummy_names <- CATS %>%
  str_replace_all(" ", "_") %>%
  str_replace_all("ó", "o") %>%
  str_replace_all("í", "i") %>%
  str_replace_all("á", "a") %>%
  str_replace_all("é", "e") %>%
  str_replace_all("ú", "u")
dummy_cols <- paste0("dum__", dummy_names)

# Inicializo todas las dummies en 0
for (nm in dummy_cols) {
  DFT2[[nm]] <- 0L
}

# Relleno dummies cuando la categoría está presente en Tiempo_traslado_final
for (i in seq_along(parse_final)) {
  cats_i <- parse_final[[i]]
  if (length(cats_i) == 0 || all(is.na(cats_i))) next
  for (c in cats_i) {
    if (is.na(c)) next
    c <- str_trim(c)
    if (c %in% CATS) {
      colname <- c %>%
        str_replace_all(" ", "_") %>%
        str_replace_all("ó", "o") %>%
        str_replace_all("í", "i") %>%
        str_replace_all("á", "a") %>%
        str_replace_all("é", "e") %>%
        str_replace_all("ú", "u")
      DFT2[[paste0("dum__", colname)]][i] <- 1L
    }
  }
}

# ============================================
# 5) Reubicar TODAS las columnas nuevas a la derecha de 'flag_outlier'
# ============================================
new_cols1 <- c("Principal_problema_para_trasladarte_backup",
               "Sol_ppat_1","Sol_ppat_2","Sol_ppat_3","Sol_ppat_4","Sol_ppat_5",
               "Tiempo_traslado_final")
new_cols <- c(new_cols1, dummy_cols)

DFT2 <- DFT2 %>%
  relocate(all_of(new_cols), .after = "flag_outlier")

# ==================================================
# 6) Auditoría (nombre solicitado)
# ==================================================
auditoria <- tibble(
  regla = c("Sol_ppat_1 (canónico exacto, 1 categoría)",
            "Sol_ppat_2 (múltiples categorías)",
            "Sol_ppat_3 (frase sinónima → 1 categoría)",
            "Sol_ppat_4 (general: todos/ninguno)",
            "Sol_ppat_5 (otros/ruido)"),
  resueltas = c(
    sum(!is.na(DFT2$Sol_ppat_1)),
    sum(!is.na(DFT2$Sol_ppat_2)),
    sum(!is.na(DFT2$Sol_ppat_3)),
    sum(!is.na(DFT2$Sol_ppat_4)),
    sum(!is.na(DFT2$Sol_ppat_5))
  )
) %>%
  mutate(porcentaje = round(100 * resueltas / nrow(DFT2), 1))

pendientes <- sum(is.na(DFT2$Tiempo_traslado_final))

exclusividad_ok <- with(DFT2, {
  filas_con_multiples <- rowSums(!is.na(dplyr::select(DFT2, Sol_ppat_1, Sol_ppat_2, Sol_ppat_3, Sol_ppat_4, Sol_ppat_5))) <= 1
  all(filas_con_multiples)
})

list(
  resumen_auditoria = auditoria,
  pendientes_sin_resolver = pendientes,
  exclusividad_ok = exclusividad_ok
) -> AUDIT_PRINCIPAL_PROBLEMA_PARA_TRASLADARTE   

```

```{r}
library(dplyr)

DFT2 <- DFT2 %>% rename(`Principal_problema_al_trasladarte_FINAL` = Tiempo_traslado_final)
```

## Limpieza: Respaldo la variable original , propuesta de los casos con solución unívoca y Normalizado

## Variable : "Dias de asistencia por semana"

```{r}
suppressPackageStartupMessages({
  library(dplyr)
  library(stringr)
  library(purrr)
})

# === 0) Nombre fijo de la columna objetivo ===
col_obj <- "Dias de asistencia por semana"

# === 1) Normalización del texto ===
normalize_txt <- function(x){
  x |> as.character() |>
    str_replace_all(",", ".") |>        # coma decimal -> punto
    str_replace_all("\\(.*?\\)", " ") |> # quita paréntesis y su contenido
    str_replace_all("\\s+", " ") |>
    str_trim()
}

x_norm <- normalize_txt(DFT2[[col_obj]])

# === 2) Detectores ===
is_integer_puro <- function(s) str_detect(s, "^\\d+$")
is_decimal      <- function(s) str_detect(s, "^\\d+\\.\\d+$")                # 2.5 / 18.9
is_rango_guion  <- function(s) str_detect(s, "^\\d+(?:\\.\\d+)?\\s*-\\s*\\d+(?:\\.\\d+)?(?:\\s|$)")
is_alt_o        <- function(s) str_detect(s, "^\\d+(?:\\.\\d+)?\\s*(?:o|ó)\\s*\\d+(?:\\.\\d+)?(?:\\s|$)")

avg_from_match <- function(s, pat){
  m <- str_match(s, pat)
  a <- suppressWarnings(as.numeric(m[,2])); b <- suppressWarnings(as.numeric(m[,3]))
  (a + b)/2
}

# === 3) Texto → número ===
map_texto <- c(
  "cero"=0, "uno"=1, "una"=1, "dos"=2, "tres"=3, "cuatro"=4, "cinco"=5, "seis"=6, "siete"=7,
  "ninguna"=0
)
text_to_num <- function(s){
  s0 <- str_to_lower(str_trim(s))
  if (s0 %in% names(map_texto)) map_texto[[s0]] else NA_real_
}

# === 4) Columnas solicitadas ===
# (a) Enteros válidos (0..7); lo demás NA
val_int       <- ifelse(is_integer_puro(x_norm), as.integer(x_norm), NA_integer_)
dias_enteros  <- ifelse(!is.na(val_int) & val_int <= 7, val_int, NA_integer_)

# (b) Outliers
val_dec <- ifelse(is_decimal(x_norm), suppressWarnings(as.numeric(x_norm)), NA_real_)
val_rng <- ifelse(is_rango_guion(x_norm),
                  avg_from_match(x_norm, "^(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)"),
                  ifelse(is_alt_o(x_norm),
                         avg_from_match(x_norm, "^(\\d+(?:\\.\\d+)?)\\s*(?:o|ó)\\s*(\\d+(?:\\.\\d+)?)"),
                         NA_real_))
val_gt7 <- ifelse(!is.na(val_int) & val_int > 7, as.numeric(val_int), NA_real_)

dias_outlier_val  <- coalesce(val_dec, val_rng, val_gt7)
dias_flag_outlier <- !is.na(dias_outlier_val)

# (c) Texto unívoco
solucion_dia1 <- {
  vtxt <- vapply(x_norm, text_to_num, numeric(1))
  ifelse(!is.na(vtxt), as.integer(vtxt), NA_integer_)
}

# (d) Columna final
dias_FINAL <- ifelse(!is.na(dias_enteros), dias_enteros,
                     ifelse(!dias_flag_outlier & !is.na(solucion_dia1), solucion_dia1, NA_integer_))

# === 5) Agregar columnas nuevas en DFT2 ===
DFT2 <- DFT2 %>%
  mutate(
    dias_backup_fuente = DFT2[[col_obj]],
    dias_enteros       = dias_enteros,
    dias_outlier_val   = dias_outlier_val,
    dias_flag_outlier  = dias_flag_outlier,
    solucion_dia1      = solucion_dia1,
    dias_FINAL         = dias_FINAL
  ) %>%
  relocate(dias_backup_fuente:dias_FINAL, .after = all_of(col_obj))


```

```{r}
#####################################################################
# === 5) Generar data frame DFDIA con todas las columnas traza ===
#####################################################################

DFDIA <- data.frame(
  dias_backup_fuente = DFT2[[col_obj]],  # valor original
  dias_enteros       = dias_enteros,
  dias_outlier_val   = dias_outlier_val,
  dias_flag_outlier  = dias_flag_outlier,
  solucion_dia1      = solucion_dia1,
  dias_FINAL         = dias_FINAL
)
```

&&&&&&&&

```{r}
DFT3_NA_finales_expuestos <- DFT2 %>% select("Turno de ingreso usual" | "Tipo de transporte_final" | "Numero total de medios utilizados_preclean" | "Tiempo de viaje FINAL" | "Tiempo_espera_min_final" | 'Tráfico percibido (1–5)' | "Principal_problema_al_trasladarte_FINAL" | 'Comodidad del transporte (1–5)' | "Frecuencia con que llegas puntual" | 'Seguridad percibida (1–5)' | 'Cansancio al llegar (1–5)' | 'Estrés percibido durante el viaje (1–5)' | 'Satisfacción con el tiempo que te toma llegar (1–5)' | 'dias_FINAL' )



```

```{r}
glimpse(DFT3_NA_finales_expuestos)
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Turno de ingreso usual`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Tipo de transporte_final`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Numero total de medios utilizados_preclean`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Tiempo de viaje FINAL`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$Tiempo_espera_min_final))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Tráfico percibido (1–5)`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$Principal_problema_al_trasladarte_FINAL))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Comodidad del transporte (1–5)`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Frecuencia con que llegas puntual`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Seguridad percibida (1–5)`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Cansancio al llegar (1–5)`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Estrés percibido durante el viaje (1–5)`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Satisfacción con el tiempo que te toma llegar (1–5)`))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos$dias_FINAL))
```

```{r}
sum(is.na(DFT3_NA_finales_expuestos))
```

```{r}
# Reemplaza DFT2_trazas y DFT2_final por tus objetos en R
write.csv(DFT2, "DFT2_trazas.csv", row.names = FALSE, na = "")
write.csv(DFT3_NA_finales_expuestos,  "DFT2_final.csv",  row.names = FALSE, na = "")

```

## Definir rangos válidos, Outliers, No imputar por defecto, Trazabilidad -\> Análisis consciente de NA

### (Inicio de limpieza y toma de decision final de NA de respectiva columna final)

### Variable : tiempo de viaje

```{r}
############################################################
# CHUNK 0 — Librerías, rutas y lectura de datos (UTF-8)
# Objetivo: leer la data y tratar celdas en blanco como NA
############################################################

# Instala si te falta alguno:
# install.packages(c("readr","dplyr","stringr","tibble"))

library(readr)
library(dplyr)
library(stringr)
library(tibble)

# (Ajusta estas rutas si lo necesitas)
path_trazas <- "DFT2_trazas.csv"
path_final  <- "DFT2_final.csv"

# Leemos como texto, preservando blancos -> NA
df_trazas <- readr::read_csv(
  file = path_trazas,
  col_types = readr::cols(.default = readr::col_character()),
  na = c("", "NA", "N/A", "null", "Null", "NaN")
)

df_final  <- readr::read_csv(
  file = path_final,
  col_types = readr::cols(.default = readr::col_character()),
  na = c("", "NA", "N/A", "null", "Null", "NaN")
)

# Columnas de interés (trazas)
col_raw   <- "Tiempo de viaje (min)"
col_clean <- "Tiempo de viaje clean"
col_err   <- "Tiempo de viaje (Datos erróneos)"   # solo para caracterizar

```

```{r}
####################################################################################
# CHUNK 7.1 — Función de normalización a minutos y clasificación del tipo de parseo
# Objetivo: convertir horas/minutos y formatos mixtos a MINUTOS y etiquetar el parse
####################################################################################

to_minutes <- function(x) {
  # Retorna lista: list(valor_minutos (numeric o NA_real_), razon_parseo (chr))
  if (is.na(x)) return(list(value = NA_real_, reason = "na"))
  
  s <- str_trim(str_to_lower(as.character(x)))
  if (s == "") return(list(value = NA_real_, reason = "empty"))
  
  # Normalizar separadores decimales tipo "2,5 h" -> "2.5 h"
  s <- str_replace_all(s, ",", ".")
  
  # 1) hh:mm
  if (str_detect(s, "^\\s*\\d+\\s*:\\s*\\d+\\s*$")) {
    parts <- str_split(s, ":", simplify = TRUE)
    h <- as.numeric(parts[1]); m <- as.numeric(parts[2])
    return(list(value = h*60 + m, reason = "hh:mm"))
  }
  
  # 2) RANGOS / intervalos (ej.: "2 horas a 2:30", "de 90 a 120", "90-120")
  # Política: si no hay 100% certeza, NO promediar -> NA (criterio asesoría)
  if (str_detect(s, "\\b(a|hasta|–|\\-|de)\\b") && str_detect(s, "\\d")) {
    nums <- str_extract_all(s, "\\d+(?:\\.\\d+)?")[[1]]
    if (length(nums) >= 2) return(list(value = NA_real_, reason = "range_text"))
  }
  
  # 3) Texto con horas y/o minutos (orden importa)
  h_match <- str_match(s, "(\\d+(?:\\.\\d+)?)\\s*(h|hr|hrs|hora|horas)\\b")
  m_match <- str_match(s, "(\\d+(?:\\.\\d+)?)\\s*(min|mins|minutos?)\\b")
  
  # 3a) ambas presentes: "1 hora 30 min"
  if (!any(is.na(h_match)) && !any(is.na(m_match))) {
    h <- as.numeric(h_match[,2]); mins <- as.numeric(m_match[,2])
    return(list(value = h*60 + mins, reason = "hours_and_minutes_text"))
  }
  # 3b) solo horas (ej. "2.5 horas")
  if (!any(is.na(h_match)) && any(is.na(m_match))) {
    h <- as.numeric(h_match[,2])
    return(list(value = h*60, reason = "hours_text"))
  }
  # 3c) solo minutos (ej. "120 min")
  if (any(is.na(h_match)) && !any(is.na(m_match))) {
    mins <- as.numeric(m_match[,2])
    return(list(value = mins, reason = "minutes_text"))
  }
  
  # 4) Compacto "2h30", "1h 15"
  if (str_detect(s, "^\\s*\\d+\\s*h\\s*\\d+\\s*$")) {
    nums <- as.numeric(unlist(str_extract_all(s, "\\d+")))
    return(list(value = nums[1]*60 + nums[2], reason = "h_m_compact"))
  }
  
  # 5) Número puro -> asumir MINUTOS
  if (str_detect(s, "^\\d+(?:\\.\\d+)?$")) {
    return(list(value = as.numeric(s), reason = "plain_number_min"))
  }
  
  # 6) Heurística “contiene h” o “min” con un número suelto
  if (str_detect(s, "hora|horas|\\bhr\\b|\\bhrs\\b|\\bh\\b|h\\s*$")) {
    nums <- str_extract_all(s, "\\d+(?:\\.\\d+)?")[[1]]
    if (length(nums) == 1) return(list(value = as.numeric(nums[1])*60, reason = "hours_text_guess"))
  }
  if (str_detect(s, "min")) {
    nums <- str_extract_all(s, "\\d+(?:\\.\\d+)?")[[1]]
    if (length(nums) == 1) return(list(value = as.numeric(nums[1]), reason = "minutes_text_guess"))
  }
  
  # 7) No se pudo parsear
  return(list(value = NA_real_, reason = "unparsed"))
}

```

```{r}
##################################################################################
# CHUNK 7.2 — Aplicar parseo a las dos trazas (raw y clean) y construir "candidato"
# Objetivo: priorizar CLEAN; si no hay, usar RAW. Guardar fuente y razón.
##################################################################################

stopifnot(all(c(col_raw, col_clean, col_err) %in% names(df_trazas)))

parse_col <- function(v) {
  res <- lapply(v, to_minutes)
  tibble(
    minutos = sapply(res, `[[`, "value"),
    reason  = sapply(res, `[[`, "reason")
  )
}

parsed_raw   <- parse_col(df_trazas[[col_raw]])
parsed_clean <- parse_col(df_trazas[[col_clean]])

# Candidato: primero clean, si NA usar raw
minutos_numeric <- ifelse(!is.na(parsed_clean$minutos), parsed_clean$minutos, parsed_raw$minutos)
fuente          <- ifelse(!is.na(parsed_clean$minutos), "clean",
                          ifelse(!is.na(parsed_raw$minutos), "raw", "none"))
parse_reason    <- ifelse(fuente == "clean", parsed_clean$reason,
                          ifelse(fuente == "raw", parsed_raw$reason, "na"))

# Data intermedia para evidencias (p. ej. tabla de problemáticos)
evidencia_problematicos <- tibble(
  raw           = df_trazas[[col_raw]],
  raw_reason    = parsed_raw$reason,
  clean         = df_trazas[[col_clean]],
  clean_reason  = parsed_clean$reason,
  err_text      = df_trazas[[col_err]],
  minutos_numeric = minutos_numeric
) %>%
  filter(is.na(minutos_numeric)) %>%
  slice_head(n = 30)

# Muestra de problemáticos (para mostrar en clase)
print(evidencia_problematicos)

```

```{r}
# seteo de fuente numerica
vec_num <- suppressWarnings(as.numeric(minutos_numeric))  # ← ÚNICA fuente numérica

```

```{r}
#################################################################################################
# CHUNK 7.3 — Diagnóstico de completitud y cardinalidad (completitud = %NA; “cardinalidad” en err)
# Objetivo: %NA final del candidato; caracterización de “Datos erróneos” (top valores)
#################################################################################################

N_total <- nrow(df_trazas)
n_non_na <- sum(!is.na(minutos_numeric))
pct_na   <- 100 * (1 - n_non_na / N_total)

cat(sprintf("N total = %d | Válidos = %d | %%NA candidato = %.2f%%\n", 
            N_total, n_non_na, pct_na))

# Cardinalidad de "Datos erróneos": útil para ver patrones de rango/etiquetas que generan NA
top_err <- df_trazas[[col_err]] %>%
  factor() %>% table(useNA = "ifany") %>% sort(decreasing = TRUE) %>% head(15)

print(top_err)

```

```{r}
#######################################################################################
# CHUNK 8.1 — Cálculo de outliers por IQR (Q1, Q3, IQR, límites) y conteo de atípicos
# Objetivo: justificar con números la política de outliers
#######################################################################################

# Resumen robusto
vec <- vec_num
Q1 <- as.numeric(stats::quantile(vec, 0.25, na.rm = TRUE))
Q3 <- as.numeric(stats::quantile(vec, 0.75, na.rm = TRUE))
IQR_val <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR_val
upper_bound <- Q3 + 1.5 * IQR_val

n_outliers <- sum(vec < lower_bound | vec > upper_bound, na.rm = TRUE)
p95 <- as.numeric(stats::quantile(vec, 0.95, na.rm = TRUE))
p99 <- as.numeric(stats::quantile(vec, 0.99, na.rm = TRUE))
med <- as.numeric(stats::median(vec, na.rm = TRUE))
mean_val <- as.numeric(mean(vec, na.rm = TRUE))
min_val <- min(vec, na.rm = TRUE)
max_val <- max(vec, na.rm = TRUE)

cat(sprintf(paste0(
  "Q1 = %.2f | Q3 = %.2f | IQR = %.2f\n",
  "Límite inferior = %.2f | Límite superior = %.2f\n",
  "Atípicos (IQR) = %d\n",
  "p95 = %.2f | p99 = %.2f | mediana = %.2f | media = %.2f\n",
  "min = %.2f | max = %.2f\n"
), Q1, Q3, IQR_val, lower_bound, upper_bound, n_outliers, p95, p99, med, mean_val, min_val, max_val))

```

```{r}

########################################################################################################
# CHUNK 8.2 — Marcar outliers y mostrar extremos (asegurando tipado numérico y evitando NA en maxs)
# Requiere: 'minutos_numeric' ya creado en 7.2 y límites de IQR de 8.1 (Q1, Q3, upper_bound).
# Si no existen los límites (por correr este chunk aislado), los recalcula sobre 'vec'.
########################################################################################################

# 1) Asegura que el vector sea NUMÉRICO (si viene como character, lo coercionamos)
vec <- vec_num

# 2) Si los límites del IQR no existen en el entorno (por ejecución aislada), recalcúlalos
if (!exists("upper_bound") || !exists("Q1") || !exists("Q3")) {
  Q1 <- as.numeric(stats::quantile(vec, 0.25, na.rm = TRUE))
  Q3 <- as.numeric(stats::quantile(vec, 0.75, na.rm = TRUE))
  IQR_val <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR_val
  upper_bound <- Q3 + 1.5 * IQR_val
}

# 3) Flag de outlier según IQR (solo marca extremos altos para este caso de tiempos)
es_outlier_IQR <- ifelse(!is.na(vec) & vec > upper_bound, TRUE, FALSE)

# 4) Construye extremos evitando NA y no finitos (para que maxs no sea NA)
vec_valid <- vec[is.finite(vec)]
vals_sorted <- sort(vec_valid, na.last = NA)

mins_5 <- head(vals_sorted, 5)
maxs_5 <- tail(vals_sorted, 5)

extremos <- tibble::tibble(
  mins = paste(mins_5, collapse = ", "),
  maxs = paste(maxs_5, collapse = ", ")
)

print(extremos)

# 5) Conteo de outliers y muestra de los más grandes (útil para exponer en clase)
cat("Outliers (IQR) =", sum(es_outlier_IQR, na.rm = TRUE), "\n")

which_out <- which(es_outlier_IQR %in% TRUE)

dplyr::tibble(
  fila = which_out,
  minutos_numeric = vec[which_out],
  fuente = fuente[which_out],
  parse_reason = parse_reason[which_out]
) |>
  dplyr::arrange(dplyr::desc(minutos_numeric)) |>
  dplyr::slice_head(n = 10) |>
  print()

```

```{r}
#######################################################################################################
# CHUNK 9.1 — Impacto de NA-omit ESPECÍFICO para esta variable (no imputar por % bajo, criterio asesoría)
# Objetivo: decidir NA-omit o imputación (→ acá: NA-omit, impacto ~1.56%)
#######################################################################################################

N_total <- nrow(df_trazas)

remain_count <- sum(!is.na(vec_num) & is.finite(vec_num))
remain_frac  <- remain_count / N_total
cat(sprintf("Si elimino filas con NA en esta variable, quedo con %d observaciones (%.2f%%).\n",
            remain_count, 100 * remain_frac))

# Decisión:
# - %NA ~ 1.56% -> bajo
# - Política asesoría: imputar solo como último recurso; si el recorte no compromete N, preferir NA-omit
decision_na <- "Eliminar (NA-omit específico para esta variable). No imputar."
cat(sprintf("Decisión NA: %s\n", decision_na))

```

```{r}
idx_mismatch <- which(!is.na(minutos_numeric) &
                      is.na(suppressWarnings(as.numeric(minutos_numeric))))
length(idx_mismatch)  # si es 1, ahí estuvo tu +1
tibble::tibble(
  fila = idx_mismatch,
  valor_textual = minutos_numeric[idx_mismatch],
  fuente = fuente[idx_mismatch],
  parse_reason = parse_reason[idx_mismatch]
) |> print()

```

```{r}
################################################################################################
# CHUNK 9.2 — Construcción del data frame final de análisis y escritura del CSV solicitado
# Objetivo: replicar/entregar 'Tiempo_viaje_analisis.csv' para el informe y auditoría
################################################################################################

analisis <- tibble(
  `Tiempo_viaje_raw`             = df_trazas[[col_raw]],
  `Tiempo_viaje_clean`           = df_trazas[[col_clean]],
  `Tiempo_viaje_datos_erroneos`  = df_trazas[[col_err]],
  `minutos_numeric`              = vec,
  `fuente`                       = fuente,        # de dónde salió: clean/raw/none
  `parse_reason`                 = parse_reason,  # hh:mm / hours_text / range_text / unparsed / etc.
  `es_outlier_IQR`               = es_outlier_IQR # TRUE si > límite superior
)

# Guardamos el CSV final (idéntico en estructura al que te compartí)
out_csv <- "Tiempo_viaje_analisis.csv"
readr::write_csv(analisis, out_csv, na = "")

cat(sprintf("Archivo escrito: %s\n", out_csv))

# Mostrar una vista para el informe (head y tail)
print(head(analisis, 10))
print(tail(analisis, 10))

```

Inicialmente teniamos

```{r}
sum(is.na(DFT3_NA_finales_expuestos$`Tiempo de viaje FINAL`))
```

Luego de analisis de NAs.

```{r}
NATVA <- read.csv("Tiempo_viaje_analisis.csv", stringsAsFactors = FALSE)
NATVA
```

```{r}
sum(complete.cases(NATVA$minutos_numeric))
```

756 filas completas de 767 filas totales, 11 NAs finalmente que borrar

```{r}
sum(is.na(NATVA$minutos_numeric))

```

------------------------------------------------------------------------

## BACKUP, CHANTADO y POSTERIOR ANOTADO DE FILAS A ELIMINAR en **DFT3_bakup_2** 

### 1) BACKUP, CHANTADO (var : Tiempo de viaje FINAL)

```{r}
# 1) Vector numérico canónico (por si la columna llegó como character)
min_num <- suppressWarnings(as.numeric(NATVA$minutos_numeric))

# 2) Máscara lógica de filas a eliminar
MASK_ELIM_TIEMPO_VIAJE <- is.na(min_num) | !is.finite(min_num)

# 3) Índices (números de fila) a eliminar
FILAS_ELIM_TIEMPO_VIAJE <- which(MASK_ELIM_TIEMPO_VIAJE)

# 4) (Opcional) Identificadores de esas filas, si existen columnas clave
cols_id_posibles <- c("id","ID","Id","id_encuesta","grupo","Grupo")
cols_id <- intersect(names(NATVA), cols_id_posibles)
IDS_ELIM_TIEMPO_VIAJE <- if (length(FILAS_ELIM_TIEMPO_VIAJE) > 0 && length(cols_id) > 0) {
  NATVA[FILAS_ELIM_TIEMPO_VIAJE, cols_id, drop = FALSE]
} else {
  NULL
}

# 5) (Opcional) Resumen para verificar en consola (no afecta a las variables)
cat("Filas a eliminar:", length(FILAS_ELIM_TIEMPO_VIAJE), "de", nrow(NATVA), "\n")
```

```{r}
FILAS_ELIM_TIEMPO_VIAJE
which(MASK_ELIM_TIEMPO_VIAJE)
```

```{r}
#creado de backup

#DFT2 pasara a ser el backup que no se tocara, lo que se tocara sera el DFT3_backup_2

DFT3_backup_2 <- DFT2
```

```{r}
#NUMERO DE NAs FINALES para Tiempo de viaje FINAL = NATVA$minutos_numeric
sum(is.na(DFT3_backup_2$`Tiempo de viaje FINAL`))
```

```{r}


#####################################################################
# CHUNK — Chancar una columna de DFT3_backup_2 con NATVA
# - Fuente: NATVA[[source_col]]
# - Destino: DFT3_backup_2[[target_col]]
# - Alineación: por IDs comunes (robusto); si no hay, por posición 1:1
# - Al final: cuenta NAs del destino
#####################################################################

library(dplyr)

# 0) Parámetros
source_col <- "minutos_numeric"        # en NATVA
target_col <- "Tiempo de viaje FINAL"  # en DFT3_backup_2  (ajusta si tu nombre difiere)

stopifnot(exists("NATVA"), exists("DFT3_backup_2"))

# 1) Coerciona a numérico por si llegó como character
src_vec_viaje <- suppressWarnings(as.numeric(NATVA[[source_col]]))

# 2) Posibles IDs para alinear
KEYS_NA_JOIN <- c(
  "id","ID","Id","id_encuesta","id_respuesta","uuid",
  "dni","DNI","matricula","Matricula",
  "grupo","Grupo",
  ".rowid",".ROWID"
)

# 3) Detecta claves comunes
keys_common_nv <- intersect(
  intersect(names(NATVA), names(DFT3_backup_2)),
  KEYS_NA_JOIN
)

cat("DFT3_backup_2 filas:", nrow(DFT3_backup_2),
    "| NATVA filas:", nrow(NATVA), "\n")

if (length(keys_common_nv) > 0) {
  # ---------- Reemplazo ROBUSTO por clave(s) ----------
  cat("Usando clave(s):", paste(keys_common_nv, collapse = ", "), "\n")

  # Verifica unicidad en NATVA por las claves
  if (any(duplicated(NATVA[, keys_common_nv, drop = FALSE]))) {
    stop("NATVA tiene duplicados en la(s) clave(s): ",
         paste(keys_common_nv, collapse = ", "),
         ". Asegura 1 fila por ID antes del join.")
  }

  # (Opcional) Aviso si hay duplicados en la base destino
  if (any(duplicated(DFT3_backup_2[, keys_common_nv, drop = FALSE]))) {
    message("Aviso: DFT3_backup_2 tiene duplicados en la(s) clave(s). El left_join puede generar matches 1:N.")
  }

  # Tabla auxiliar (IDs + valor fuente)
  nat_aux <- NATVA %>%
    mutate(.src_value_viaje = src_vec_viaje) %>%
    select(all_of(keys_common_nv), .src_value_viaje)

  # Asegura existencia del destino
  if (!(target_col %in% names(DFT3_backup_2))) {
    DFT3_backup_2[[target_col]] <- NA_real_
  }

  # Join + chantar (sobre la copia)
  DFT3_backup_2 <- DFT3_backup_2 %>%
    left_join(nat_aux, by = keys_common_nv) %>%
    mutate(!!target_col := .src_value_viaje) %>%
    select(-.src_value_viaje)

  cat("Reemplazo por IDs completado.\n")

} else {
  # ---------- Reemplazo por POSICIÓN (1:1) ----------
  cat("No se encontraron IDs comunes. Reemplazo por posición 1:1...\n")

  if (nrow(DFT3_backup_2) != nrow(NATVA)) {
    stop("No hay IDs y el número de filas difiere (DFT3_backup_2 vs NATVA). ",
         "No es seguro chantar por posición. Agrega una clave común y vuelve a intentar.")
  }

  # Asegura existencia del destino
  if (!(target_col %in% names(DFT3_backup_2))) {
    DFT3_backup_2[[target_col]] <- NA_real_
  }

  # Chantar por posición
  DFT3_backup_2[[target_col]] <- src_vec_viaje
  cat("Reemplazo 1:1 por posición completado.\n")
}

# 4) Conteo de NAs del destino (para tu política NA-omit)
na_destino_viaje <- sum(is.na(DFT3_backup_2[[target_col]]))
cat("NA en", shQuote(target_col), "después de chantar:", na_destino_viaje, "\n")


```

En fecto es la nueva cantidad de NAs analizado en NATVA. (col : minutos_numeric)

```{r}
#NUMERO DE NAs FINALES para Tiempo de viaje FINAL = NATVA$minutos_numeric
sum(is.na(DFT3_backup_2$`Tiempo de viaje FINAL`))
```

```{r}
#ya corrido
#minutos_numeric = Tiempo de viaje FINAL 2
#DFT3_backup_2 <- DFT3_backup_2 %>% rename(`Tiempo de viaje FINAL 2` = `Tiempo de viaje FINAL`)

```

Recordemos

```{r}
FILAS_ELIM_TIEMPO_VIAJE
which(MASK_ELIM_TIEMPO_VIAJE)
```

### 2) FILAS A ELIMINAR en **DFT3_bakup_2 (var :** Tiempo de viaje FINAL)

```{r}
#####################################################################
# Eliminar por ÍNDICES (opción B) usando tu máscara existente
# - No toca NATVA ni DFT3_backup
# - Evita filas NA "fantasma" (which() ignora NA)
#####################################################################

stopifnot(exists("DFT3_backup_2"), exists("MASK_ELIM_TIEMPO_VIAJE"))

# 1) Convierte la máscara lógica a índices (NA se ignoran automáticamente)
idx_elim_viaje <- which(MASK_ELIM_TIEMPO_VIAJE)

# 2) Resumen previo
cat("Filas actuales:", nrow(DFT3_backup_2), 
    "| A eliminar por viaje:", length(idx_elim_viaje), "\n")

# 3) Eliminar por índices
if (length(idx_elim_viaje) > 0) {
  DFT3_backup_2 <- DFT3_backup_2[-idx_elim_viaje, , drop = FALSE]
}

# 4) Verificación
cat("Filas después:", nrow(DFT3_backup_2), "\n")



```

Funciono, corroboremos

```{r}
dim(DFT3_backup_2)

```

```{r}
sum(is.na(DFT3_backup_2$`Tiempo de viaje FINAL 2`))
```

Finalizamos con la eliminacion de NA de la variable Tiempo de viaje

------------------------------------------------------------------------

### Nuevos csv. para la variable Tiempo de espera,

se procede con. la elminacion de esas **11 filas de ambos csv.** usados para eliminar los NA de la anterior variable

```{r}
#####################################################################
# PLAN B — Eliminar por ÍNDICES (1:1) en DFT2_trazas.csv y DFT2_final.csv
# Requisitos:
#   - Ambos CSVs están alineados (mismo orden y nrow)
#   - Existen 'idx_elim_viaje' o 'FILAS_ELIM_TIEMPO_VIAJE'
# Salida:
#   - DFT2_trazas_sin11.csv
#   - DFT2_final_sin11.csv
#####################################################################

# 0) Entradas (ajusta rutas si están en otra carpeta)
path_trazas <- "DFT2_trazas.csv"
path_final  <- "DFT2_final.csv"

stopifnot(file.exists(path_trazas), file.exists(path_final))

# 1) Indices a eliminar (toma de lo que ya tienes en el entorno)
if (exists("idx_elim_viaje")) {
  idx_eliminar <- idx_elim_viaje
} else if (exists("FILAS_ELIM_TIEMPO_VIAJE")) {
  idx_eliminar <- FILAS_ELIM_TIEMPO_VIAJE
} else {
  stop("No encuentro 'idx_elim_viaje' ni 'FILAS_ELIM_TIEMPO_VIAJE'.")
}

# Sanitiza índices
idx_eliminar <- unique(sort(as.integer(idx_eliminar)))

# 2) Carga de CSVs
trazas <- read.csv(path_trazas, stringsAsFactors = FALSE)
final  <- read.csv(path_final,  stringsAsFactors = FALSE)

cat("[CARGA] trazas:", nrow(trazas), "x", ncol(trazas),
    " | final:", nrow(final), "x", ncol(final), "\n")

# 3) Verificación de alineación (requisito Plan B)
if (nrow(trazas) != nrow(final)) {
  stop("Plan B requiere archivos alineados: nrow(trazas) != nrow(final).")
}

# 4) Validar índices en rango
n <- nrow(trazas)
idx_valid <- idx_eliminar[idx_eliminar >= 1 & idx_eliminar <= n]

if (length(idx_valid) == 0) {
  warning("Ningún índice válido dentro del rango 1..", n, ". No se eliminará nada.")
}

# 5) Eliminar por índices (idénticos) en ambos CSVs
n0_t <- nrow(trazas); n0_f <- nrow(final)

trazas_sin <- if (length(idx_valid) > 0) trazas[-idx_valid, , drop = FALSE] else trazas
final_sin  <- if (length(idx_valid) > 0) final[-idx_valid, , drop = FALSE]  else final

cat("[ELIM] Removidas en trazas:", n0_t - nrow(trazas_sin),
    " | Removidas en final:", n0_f - nrow(final_sin), "\n")

# 6) Guardar resultados (dejo nuevos archivos para conservar originales)
write.csv(trazas_sin, "DFT2_trazas_sin11.csv", row.names = FALSE, na = "")
write.csv(final_sin,  "DFT2_final_sin11.csv",  row.names = FALSE, na = "")

cat("[OK] Exportados:\n - DFT2_trazas_sin11.csv\n - DFT2_final_sin11.csv\n")

```

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

## Definir rangos válidos, Outliers, No imputar por defecto, Trazabilidad -\> Análisis consciente de NA

### (Snapshot correspondiente, Inicio de limpieza y toma de decision final de NA de respectiva columna final)

### Variable : Tiempo_espera_min_final :

```{r}
#####################################################################
# 7.1 — Snapshot + Carga + Preparación (tags únicos *_ESP2)
#####################################################################

library(dplyr)
options(stringsAsFactors = FALSE)

# 1) Snapshot de la base de trabajo actual (por si quieres auditar)
stopifnot(exists("DFT3_backup_2"))
stamp_esp2 <- format(Sys.time(), "%Y%m%d_%H%M%S")
snap_esp2  <- paste0("DFT3_backup_2_snapshot_ESP2_", stamp_esp2, ".csv")
write.csv(DFT3_backup_2, snap_esp2, row.names = FALSE, na = "")
cat("[SNAPSHOT] Exportado:", snap_esp2, " | Dim:", nrow(DFT3_backup_2), "x", ncol(DFT3_backup_2), "\n\n")

# 2) Rutas insumo (las versiones sin 11 filas)
path_trazas_ESP2 <- "DFT2_trazas_sin11.csv"
path_final_ESP2  <- "DFT2_final_sin11.csv"
stopifnot(file.exists(path_trazas_ESP2), file.exists(path_final_ESP2))

# 3) Carga
TRZ_ESP2 <- read.csv(path_trazas_ESP2)
FIN_ESP2 <- read.csv(path_final_ESP2)

cat("[CARGA] TRZ_ESP2:", nrow(TRZ_ESP2), "x", ncol(TRZ_ESP2), "\n")
cat("[CARGA] FIN_ESP2:", nrow(FIN_ESP2), "x", ncol(FIN_ESP2), "\n\n")

# 4) Columnas traza para TIEMPO DE ESPERA (usa los nombres reales del CSV)
cols_traza_espera_ESP2 <- c(
  "Tiempo_espera_backup",
  "Sol_1","Sol_2","Sol_3","Sol_4","Sol_5",
  "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas"
)
cols_traza_espera_ESP2 <- intersect(cols_traza_espera_ESP2, names(TRZ_ESP2))

# 5) Columna final del CSV (referencia/contraste)
col_final_csv_ESP2 <- "Tiempo_espera_min_final"  # existe en FIN_ESP2

# 6) Claves para unir si existieran
KEYS_ESP2 <- c("id","ID","Id","id_encuesta","id_respuesta","uuid",
               "dni","DNI","matricula","Matricula","grupo","Grupo",
               ".rowid",".ROWID")
keys_common_ESP2 <- intersect(intersect(names(TRZ_ESP2), names(FIN_ESP2)), KEYS_ESP2)

# 7) Armar tabla base TEA_ESP2 con trazas + (opcional) final_csv como referencia
if (length(keys_common_ESP2) > 0) {
  cat("[JOIN] Uniendo por claves:", paste(keys_common_ESP2, collapse = ", "), "\n")
  TRZ_red_ESP2 <- TRZ_ESP2 %>% select(all_of(keys_common_ESP2), all_of(cols_traza_espera_ESP2))
  FIN_red_ESP2 <- FIN_ESP2 %>% select(all_of(keys_common_ESP2), all_of(col_final_csv_ESP2))
  TEA_ESP2 <- TRZ_red_ESP2 %>% left_join(FIN_red_ESP2, by = keys_common_ESP2)
} else {
  cat("[JOIN] Sin claves comunes, uniendo por posición 1:1...\n")
  stopifnot(nrow(TRZ_ESP2) == nrow(FIN_ESP2))
  TEA_ESP2 <- bind_cols(
    TRZ_ESP2[, cols_traza_espera_ESP2, drop = FALSE],
    FIN_ESP2[, col_final_csv_ESP2, drop = FALSE]
  )
}

# Renombrar la columna final CSV para evitar colisión con nuestro candidato
if ("Tiempo_espera_min_final" %in% names(TEA_ESP2)) {
  names(TEA_ESP2)[names(TEA_ESP2) == "Tiempo_espera_min_final"] <- "Tiempo_espera_min_final_csv_ESP2"
}

cat("[TEA_ESP2] Dim:", nrow(TEA_ESP2), "x", ncol(TEA_ESP2), "\n")
cat("[TEA_ESP2] Trazas:", paste(cols_traza_espera_ESP2, collapse = ", "), "\n")
if ("Tiempo_espera_min_final_csv_ESP2" %in% names(TEA_ESP2)) {
  cat("[TEA_ESP2] Incluye referencia CSV: Tiempo_espera_min_final_csv_ESP2\n\n")
}

```

```{r}
#####################################################################
# 7.2 — Función parser: texto (rangos, horas/minutos, ruido) → minutos
#####################################################################

# Regla de estudio: si hay rangos (p.ej., "5-8", "2 horas a 2:30"), usamos el **punto medio**
# porque es una inferencia conservadora y replicable (no sesga al extremo).
# “No utilizo / no uso” transporte que implique espera -> 0 minutos (justificable según reglas).
# Si no podemos asegurar, devolvemos NA (NA por "ruido" o no interpretable).

parse_min_ESP2 <- function(x) {
  x0 <- trimws(tolower(as.character(x)))
  x0 <- gsub(",", ".", x0)                 # decimales con coma -> punto
  x0 <- gsub("\\s+", " ", x0)              # espacios múltiples
  x0 <- gsub("minutos|minuto|min\\b", "min", x0)
  x0 <- gsub("horas|hora|hrs|hr|h\\b", "h", x0)
  x0 <- gsub("a ", "-", x0)                # "2 horas a 2:30" -> "2 horas - 2:30" (normalizar)
  
  # Casos 0 explícito (según reglas de estudio)
  if (grepl("no uso|no utilizo|no utilizó|sin espera", x0)) return(0)

  # Rango tipo "5-8" (minutos) o "10 - 15"
  if (grepl("\\b\\d+(?:\\.\\d+)?\\s*-\\s*\\d+(?:\\.\\d+)?\\b", x0)) {
    nums <- as.numeric(unlist(regmatches(x0, gregexpr("\\d+(?:\\.\\d+)?", x0, perl=TRUE))))
    if (length(nums) >= 2) return(mean(nums[1:2]))
  }
  
  # Rango mixto con horas "2h-2:30" o "2 h - 150 min"
  if (grepl("h.*-.*", x0) || grepl("-.*h", x0)) {
    parts <- strsplit(x0, "-")[[1]]
    parts <- trimws(parts)
    mins <- c()
    for (p in parts) {
      # 2:30 -> 150
      if (grepl("^\\d+\\s*:\\s*\\d+$", p)) {
        hhmm <- as.numeric(unlist(strsplit(gsub("\\s","", p), ":")))
        mins <- c(mins, hhmm[1]*60 + hhmm[2])
      } else if (grepl("\\d+(?:\\.\\d+)?\\s*h\\b", p)) {
        h <- as.numeric(gsub("[^0-9\\.]", "", p))
        mins <- c(mins, h*60)
      } else {
        # ¿minutos simples en esta parte?
        n <- suppressWarnings(as.numeric(gsub("[^0-9\\.]", "", p)))
        if (!is.na(n)) mins <- c(mins, n)
      }
    }
    if (length(mins) >= 2) return(mean(mins[1:2]))
  }
  
  # Formato HH:MM (p.ej., "0:05", "1:30")
  if (grepl("^\\d+\\s*:\\s*\\d+$", x0)) {
    hhmm <- as.numeric(unlist(strsplit(gsub("\\s","", x0), ":")))
    return(hhmm[1]*60 + hhmm[2])
  }
  
  # "2h", "1.5h"
  if (grepl("\\d+(?:\\.\\d+)?\\s*h\\b", x0)) {
    h <- as.numeric(gsub("[^0-9\\.]", "", x0))
    return(h*60)
  }
  
  # "30 min" o número suelto interpretable como minutos
  n2 <- suppressWarnings(as.numeric(gsub("[^0-9\\.]", "", x0)))
  if (!is.na(n2)) return(n2)
  
  # No interpretable -> NA
  return(NA_real_)
}

```

```{r}
#####################################################################
# 7.3 — Construir candidato numérico de espera (min)
# Prioridad de fuentes:
# 1) Sol_1..Sol_5 (si alguna es numérica válida)
# 2) Sol_6__Ruido... (si ya viene numérica derivada)
# 3) Tiempo_espera_backup (parsear texto original)
#####################################################################

# Helper: tomar la primera no-NA de un vector de columnas
first_non_na_ESP2 <- function(df, cols) {
  out <- rep(NA_real_, nrow(df))
  for (cn in cols) {
    if (!cn %in% names(df)) next
    v <- suppressWarnings(as.numeric(df[[cn]]))
    take <- is.na(out) & !is.na(v)
    out[take] <- v[take]
  }
  out
}

# 1) Intento con Sol_1..Sol_5
sol_cols_ESP2 <- intersect(c("Sol_1","Sol_2","Sol_3","Sol_4","Sol_5"), names(TEA_ESP2))
cand_1_ESP2 <- first_non_na_ESP2(TEA_ESP2, sol_cols_ESP2)

# 2) Si aún NA, usar Sol_6 (suele traer midpoint/rango ya cuantificado)
sol6_col_ESP2 <- "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas"
cand_2_ESP2 <- cand_1_ESP2
if (sol6_col_ESP2 %in% names(TEA_ESP2)) {
  sol6_num <- suppressWarnings(as.numeric(TEA_ESP2[[sol6_col_ESP2]]))
  fill2 <- is.na(cand_2_ESP2) & !is.na(sol6_num)
  cand_2_ESP2[fill2] <- sol6_num[fill2]
}

# 3) Si aún NA, parsear la original backup
cand_3_ESP2 <- cand_2_ESP2
if ("Tiempo_espera_backup" %in% names(TEA_ESP2)) {
  parsed_bkp <- vapply(TEA_ESP2[["Tiempo_espera_backup"]], parse_min_ESP2, numeric(1))
  fill3 <- is.na(cand_3_ESP2) & !is.na(parsed_bkp)
  cand_3_ESP2[fill3] <- parsed_bkp[fill3]
}

# Candidato final
TEA_ESP2$tiempo_espera_min_candidato_ESP2 <- cand_3_ESP2

# Indicadores base
TEA_ESP2$es_numerico_final_ESP2   <- !is.na(TEA_ESP2$tiempo_espera_min_candidato_ESP2)
TEA_ESP2$es_no_negativo_ESP2      <- TEA_ESP2$tiempo_espera_min_candidato_ESP2 >= 0 | is.na(TEA_ESP2$tiempo_espera_min_candidato_ESP2)

# “Razonable” amplio para espera (tope técnico): <= 120 min (2h), por práctica y sentido común;
# y dejaremos "outlier" separado por IQR.
TEA_ESP2$es_razonable_ESP2        <- TEA_ESP2$tiempo_espera_min_candidato_ESP2 <= 120 | is.na(TEA_ESP2$tiempo_espera_min_candidato_ESP2)

# Copia de referencia del CSV (si existe)
ref_csv_esp2 <- if ("Tiempo_espera_min_final_csv_ESP2" %in% names(TEA_ESP2)) {
  suppressWarnings(as.numeric(TEA_ESP2$Tiempo_espera_min_final_csv_ESP2))
} else rep(NA_real_, nrow(TEA_ESP2))

```

```{r}
#####################################################################
# 8.IQR_Razonables_ESP2 — Análisis IQR y definición de “razonable”
#####################################################################

stopifnot(exists("TEA_ESP2"))
v_esp <- TEA_ESP2$tiempo_espera_min_candidato_ESP2

# 1) Estadísticos base
Q1_esp <- as.numeric(quantile(v_esp, 0.25, na.rm = TRUE))
Q3_esp <- as.numeric(quantile(v_esp, 0.75, na.rm = TRUE))
IQR_esp <- Q3_esp - Q1_esp
low_w_esp <- Q1_esp - 1.5*IQR_esp
up_w_esp  <- Q3_esp + 1.5*IQR_esp

cat(sprintf("[IQR] Q1=%.2f | Q3=%.2f | IQR=%.2f | low_w=%.2f | up_w=%.2f\n",
            Q1_esp, Q3_esp, IQR_esp, low_w_esp, up_w_esp))

# 2) Ajuste por criterio sustantivo:
#    - Inferior: como espera no puede ser negativa, truncamos en 0
#    - Superior: tope técnico 120 min (2h) para “razonable” por práctica
lim_inf_esp <- 0
lim_sup_tecnico_esp <- 120
lim_sup_iqr_esp <- up_w_esp
lim_sup_final_esp <- min(lim_sup_iqr_esp, lim_sup_tecnico_esp)

cat(sprintf("[Límites razonables] inf=%d | sup_IQR=%.2f | sup_técnico=%d | sup_final=%.2f\n",
            lim_inf_esp, lim_sup_iqr_esp, lim_sup_tecnico_esp, lim_sup_final_esp))

# 3) Flags
TEA_ESP2$flag_outlier_IQR_ESP2 <- ifelse(!is.na(v_esp) & (v_esp < low_w_esp | v_esp > up_w_esp), TRUE, FALSE)

TEA_ESP2$flag_razonable_ESP2 <- ifelse(
  !is.na(v_esp) & v_esp >= lim_inf_esp & v_esp <= lim_sup_final_esp,
  TRUE, 
  ifelse(is.na(v_esp), NA, FALSE)
)

# 4) Resumen de calidad / distribución
N_esp   <- length(v_esp)
NA_esp  <- sum(is.na(v_esp))
OK_esp  <- sum(TEA_ESP2$flag_razonable_ESP2 == TRUE, na.rm = TRUE)
OUT_esp <- sum(TEA_ESP2$flag_outlier_IQR_ESP2 == TRUE, na.rm = TRUE)
NEG_esp <- sum(!is.na(v_esp) & v_esp < 0)
GT_SUP  <- sum(!is.na(v_esp) & v_esp > lim_sup_final_esp)  # por encima del límite razonable final

cat("\n[RESUMEN]\n")
cat("N total:", N_esp, 
    "| NA:", NA_esp, 
    "| razonables:", OK_esp, sprintf("(%.2f%%)", 100*OK_esp/N_esp),
    "| outliers_IQR:", OUT_esp, 
    "| <0:", NEG_esp, 
    "| >", sprintf("%.0f", lim_sup_final_esp), ":", GT_SUP, "\n")

# 5) Auditoría: ejemplos de outliers (arriba del bigote superior)
idx_out_hi <- which(!is.na(v_esp) & v_esp > up_w_esp)
idx_out_lo <- which(!is.na(v_esp) & v_esp < low_w_esp)
cat("\n[Auditoría] outliers altos:", length(idx_out_hi), " | outliers bajos:", length(idx_out_lo), "\n")

# Muestra hasta 10 casos con su valor y (si están) columnas de traza
cols_show_traza <- intersect(
  c("Tiempo_espera_backup","Sol_1","Sol_2","Sol_3","Sol_4","Sol_5",
    "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas"),
  names(TEA_ESP2)
)

if (length(idx_out_hi) > 0) {
  cat("\n[Ejemplos outliers altos] (máx 10)\n")
  print(
    cbind(
      data.frame(idx = idx_out_hi, valor = v_esp[idx_out_hi])[1:min(10, length(idx_out_hi)), , drop = FALSE],
      TEA_ESP2[ idx_out_hi[1:min(10, length(idx_out_hi))], cols_show_traza, drop = FALSE]
    )
  )
}

if (length(idx_out_lo) > 0) {
  cat("\n[Ejemplos outliers bajos] (máx 10)\n")
  print(
    cbind(
      data.frame(idx = idx_out_lo, valor = v_esp[idx_out_lo])[1:min(10, length(idx_out_lo)), , drop = FALSE],
      TEA_ESP2[ idx_out_lo[1:min(10, length(idx_out_lo))], cols_show_traza, drop = FALSE]
    )
  )
}

# 6) (Opcional) Si deseas una etiqueta final útil para informes:
TEA_ESP2$calidad_espera_ESP2 <- ifelse(
  is.na(v_esp), "NA",
  ifelse(TEA_ESP2$flag_razonable_ESP2, "RAZONABLE",
         ifelse(TEA_ESP2$flag_outlier_IQR_ESP2, "OUTLIER_IQR", "FUERA_RANGO"))
)
table_calidad <- sort(table(TEA_ESP2$calidad_espera_ESP2), decreasing = TRUE)
cat("\n[Tabla calidad]\n"); print(table_calidad)

```

```{r}
summary(TEA_ESP2)
```

```{r}
#####################################################################
# 8.2 — Chequeos adicionales de calidad
#####################################################################

n_total <- nrow(TEA_ESP2)
n_a    <- nrow(TEA_ESP2) - sum(!is.na(TEA_ESP2$Tiempo_espera_min_final_csv_ESP2))
n_neg   <- sum(!is.na(v_esp) & v_esp < 0)
n_gt30  <- sum(!is.na(v_esp) & v_esp > up_w_esp)   # > 30 min típicamente, dada la IQR
n_gt60  <- sum(!is.na(v_esp) & v_esp > 60)
n_gt120 <- sum(!is.na(v_esp) & v_esp > 120)

cat("Total:", n_total, "| NA:", n_a , "| <0:", n_neg,
    "| >", round(up_w_esp), ":", n_gt30, "| >60:", n_gt60, "| >120:", n_gt120, "\n")

# Verificación: zeros (mins)
n_zero <- sum(!is.na(v_esp) & v_esp == 0)
cat("Ceros (espera=0):", n_zero, "\n")


```

```{r}
summary(TEA_ESP2) #4 NAs
```

```{r}
#####################################################################
# 9.1 — Decisión sobre NA (política)
#####################################################################

n_total <- nrow(TEA_ESP2)
n_na    <- nrow(TEA_ESP2) - sum(!is.na(TEA_ESP2$Tiempo_espera_min_final_csv_ESP2))
p_na    <- 100 * n_na / n_total

cat(sprintf("NA: %d de %d (%.2f%%)\n", n_na, n_total, p_na))

# Justificación (texto en comentarios):
# - Tu asesoría indica que la base final debe quedar SIN NA.
# - Imputar es el ÚLTIMO recurso; con p_na bajo (<< 5%), NA-omit minimiza sesgo.
# - Hay NA “válidos” por ausencia (saltos lógicos), pero para el análisis final se recomienda
#   o bien mapear explícito a 0 (si se justifica “sin espera” por reglas de estudio),
#   o eliminar esas filas si no hay certeza (preferible a imputar).
# - Con p_na ~ 0.5% (4 de 756 en tu CSV), la decisión conservadora es ELIMINAR esas filas.

DECISION_ELIMINAR_NA_ESP2 <- (p_na <= 5)  # umbral de confort; ajusta si deseas

if (DECISION_ELIMINAR_NA_ESP2) {
  cat("Decisión: ELIMINAR filas con NA en tiempo_espera_min_candidato_ESP2.\n")
} else {
  cat("Decisión: Evaluar imputación (no recomendado salvo necesidad) o revisión manual.\n")
}

# Guardar máscara/índices para aplicar después
# --- Parámetros
col_objetivo <- "Tiempo_espera_min_final_csv_ESP2"

# --- Utilidad: devuelve un lógico del largo n con un valor por defecto
mask_len <- function(x, n, default = FALSE){
  if (length(x) == n) return(x)
  # si x es logical(0) u otra longitud, rellena con default
  rep(default, n)
}

# --- Construcción creativa y segura de la máscara
nfilas <- nrow(TEA_ESP2)

MASCARA_ELIM_TIEMPO_ESPERA_ESP2 <- (function(df, col){
  # 1) Si el df no tiene filas, devuelve logical(0)
  if (nrow(df) == 0L) return(logical(0))

  # 2) Si la columna no existe, crea máscara de "no eliminar" (todo FALSE)
  if (!rlang::has_name(df, col)) {
    message(sprintf("Aviso: columna '%s' no existe. Máscara = FALSE.", col))
    return(rep(FALSE, nrow(df)))
  }

  x <- df[[col]]

  # 3) Detecta NA reales y NA “parecidos” (texto)
  na_text <- if (is.character(x)) trimws(x) %in% c("NA","N/A","na","Na","") else FALSE
  mask_raw <- is.na(x) | na_text

  # 4) Garantiza longitud correcta aunque mask_raw sea logical(0)
  mask_len(mask_raw, nrow(df), default = FALSE)
})(TEA_ESP2, col_objetivo)

# (Opcional) chequeo rápido
cat("NA detectados para eliminar:", sum(MASCARA_ELIM_TIEMPO_ESPERA_ESP2), "de", nrow(TEA_ESP2), "\n")




INDICES_ELIM_TIEMPO_ESPERA_ESP2 <- which(MASCARA_ELIM_TIEMPO_ESPERA_ESP2)
cat("Indices a eliminar (NA en espera):", length(INDICES_ELIM_TIEMPO_ESPERA_ESP2), "\n")



```

```{r}
INDICES_ELIM_TIEMPO_ESPERA_ESP2
# 100 -> no utilizo
# 155 -> no uso transporte publico, 0
# 483 -> q5
# 725 -> 5min depende

#Recuperacion de data (proposito)

# 100 -> 0
# 155 -> 0
# 483 -> 5
# 725 -> 5

```

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

```{r}
#####################################################################
# 9.2 — Snapshot + Carga + Preparación (ESP2)
#####################################################################

library(dplyr)
options(stringsAsFactors = FALSE)

# 1) Snapshot de la base de trabajo actual (por auditoría)
stopifnot(exists("DFT3_backup_2"))
stamp_esp2 <- format(Sys.time(), "%Y%m%d_%H%M%S")
snap_esp2  <- paste0("DFT3_backup_2_snapshot_ESP2_", stamp_esp2, ".csv")
write.csv(DFT3_backup_2, snap_esp2, row.names = FALSE, na = "")
cat("[SNAPSHOT] Exportado:", snap_esp2, " | Dim:", nrow(DFT3_backup_2), "x", ncol(DFT3_backup_2), "\n\n")

# 2) Rutas de los insumos ya sin 11 filas
path_trazas_ESP2 <- "DFT2_trazas_sin11.csv"
path_final_ESP2  <- "DFT2_final_sin11.csv"
stopifnot(file.exists(path_trazas_ESP2), file.exists(path_final_ESP2))

# 3) Carga
TRZ_ESP2 <- read.csv(path_trazas_ESP2)
FIN_ESP2 <- read.csv(path_final_ESP2)
cat("[CARGA] TRZ_ESP2:", nrow(TRZ_ESP2), "x", ncol(TRZ_ESP2), "\n")
cat("[CARGA] FIN_ESP2:", nrow(FIN_ESP2), "x", ncol(FIN_ESP2), "\n\n")

# 4) Columnas traza para TIEMPO DE ESPERA
cols_traza_espera_ESP2 <- c(
  "Tiempo_espera_backup",
  "Sol_1","Sol_2","Sol_3","Sol_4","Sol_5",
  "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas"
)
cols_traza_espera_ESP2 <- intersect(cols_traza_espera_ESP2, names(TRZ_ESP2))

# 5) Columna final del CSV (referencia, no se pisa)
col_final_csv_ESP2 <- "Tiempo_espera_min_final"

# 6) Intento unir por claves si existieran (robusto); si no, por posición 1:1
KEYS_ESP2 <- c("id","ID","Id","id_encuesta","id_respuesta","uuid",
               "dni","DNI","matricula","Matricula","grupo","Grupo",
               ".rowid",".ROWID")
keys_common_ESP2 <- intersect(intersect(names(TRZ_ESP2), names(FIN_ESP2)), KEYS_ESP2)

if (length(keys_common_ESP2) > 0) {
  cat("[JOIN] Por claves:", paste(keys_common_ESP2, collapse = ", "), "\n")
  TRZ_red_ESP2 <- TRZ_ESP2 %>% select(all_of(keys_common_ESP2), all_of(cols_traza_espera_ESP2))
  FIN_red_ESP2 <- FIN_ESP2 %>% select(all_of(keys_common_ESP2), all_of(col_final_csv_ESP2))
  TEA_ESP2 <- TRZ_red_ESP2 %>% left_join(FIN_red_ESP2, by = keys_common_ESP2)
} else {
  cat("[JOIN] Sin claves; uniendo por posición 1:1...\n")
  stopifnot(nrow(TRZ_ESP2) == nrow(FIN_ESP2))
  TEA_ESP2 <- bind_cols(
    TRZ_ESP2[, cols_traza_espera_ESP2, drop = FALSE],
    FIN_ESP2[, col_final_csv_ESP2, drop = FALSE]
  )
}

# Renombrar la ref. CSV para comparación
if ("Tiempo_espera_min_final" %in% names(TEA_ESP2)) {
  names(TEA_ESP2)[names(TEA_ESP2) == "Tiempo_espera_min_final"] <- "Tiempo_espera_min_final_csv_ESP2"
}
cat("[TEA_ESP2] Dim:", nrow(TEA_ESP2), "x", ncol(TEA_ESP2), "\n\n")

```

```{r}
#####################################################################
# 9.3 — Candidato numérico e indicadores base (ESP2)
#####################################################################

# Toma la primera no-NA de un set de columnas numéricas
first_non_na_ESP2 <- function(df, cols) {
  out <- rep(NA_real_, nrow(df))
  for (cn in cols) {
    if (!cn %in% names(df)) next
    v <- suppressWarnings(as.numeric(df[[cn]]))
    take <- is.na(out) & !is.na(v)
    out[take] <- v[take]
  }
  out
}

# 1) Sol_1..Sol_5
sol_cols_ESP2 <- intersect(c("Sol_1","Sol_2","Sol_3","Sol_4","Sol_5"), names(TEA_ESP2))
cand_1_ESP2 <- first_non_na_ESP2(TEA_ESP2, sol_cols_ESP2)

# 2) Sol_6 (si trae numérico consolidado)
sol6_col_ESP2 <- "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas"
cand_2_ESP2 <- cand_1_ESP2
if (sol6_col_ESP2 %in% names(TEA_ESP2)) {
  sol6_num <- suppressWarnings(as.numeric(TEA_ESP2[[sol6_col_ESP2]]))
  fill2 <- is.na(cand_2_ESP2) & !is.na(sol6_num)
  cand_2_ESP2[fill2] <- sol6_num[fill2]
}

# 3) Backup crudo parseado
cand_3_ESP2 <- cand_2_ESP2
if ("Tiempo_espera_backup" %in% names(TEA_ESP2)) {
  parsed_bkp <- vapply(TEA_ESP2[["Tiempo_espera_backup"]], parse_min_ESP2, numeric(1))
  fill3 <- is.na(cand_3_ESP2) & !is.na(parsed_bkp)
  cand_3_ESP2[fill3] <- parsed_bkp[fill3]
}

# Resultado candidato
TEA_ESP2$tiempo_espera_min_candidato_ESP2 <- cand_3_ESP2

# Indicadores base
TEA_ESP2$es_numerico_final_ESP2 <- !is.na(TEA_ESP2$tiempo_espera_min_candidato_ESP2)
TEA_ESP2$es_no_negativo_ESP2    <- is.na(cand_3_ESP2) | cand_3_ESP2 >= 0
# "Razonable" amplio (tope técnico): <= 120 min (2h); los IQR se usan luego para outliers
TEA_ESP2$es_razonable_ESP2      <- is.na(cand_3_ESP2) | cand_3_ESP2 <= 120

cat("Construido candidato y flags base.\n\n")

```

```{r}
#####################################################################
# 8.1 — Perfil estadístico e IQR de (ESP2)
#####################################################################

v <- TEA_ESP2$tiempo_espera_min_candidato_ESP2
Q1  <- as.numeric(quantile(v, 0.25, na.rm = TRUE))
Q3  <- as.numeric(quantile(v, 0.75, na.rm = TRUE))
IQR <- Q3 - Q1
low_w  <- Q1 - 1.5*IQR
up_w   <- Q3 + 1.5*IQR
outer_low  <- Q1 - 3.0*IQR
outer_high <- Q3 + 3.0*IQR

# Ajustes por sustantivo (espera no negativa)
low_w_adj   <- max(0, low_w)
outer_low_adj <- max(0, outer_low)

cat(sprintf("Q1=%.2f | Q3=%.2f | IQR=%.2f | inner=(%.2f, %.2f) | outer=(%.2f, %.2f)\n",
            Q1, Q3, IQR, low_w, up_w, outer_low, outer_high))
cat(sprintf("Ajuste no-negativo: inner_low=%.2f | outer_low=%.2f\n\n", low_w_adj, outer_low_adj))

```

```{r}
#####################################################################
# 8.2 — Flags de outliers/extremos + política aplicada (ESP2)
#####################################################################

# Flags Tukey
TEA_ESP2$flag_outlier_inner_ESP2 <- ifelse(!is.na(v) & (v < low_w_adj | v > up_w), TRUE, FALSE)
TEA_ESP2$flag_extremo_outer_ESP2 <- ifelse(!is.na(v) & (v < outer_low_adj | v > outer_high), TRUE, FALSE)

# Conteos
N_tot <- length(v)
NA_ct <- sum(is.na(v))
OUT_in <- sum(TEA_ESP2$flag_outlier_inner_ESP2, na.rm = TRUE)
EXT_ot <- sum(TEA_ESP2$flag_extremo_outer_ESP2, na.rm = TRUE)
cat("[RESUMEN] N:", N_tot, "| NA:", NA_ct, "| outliers(inner):", OUT_in, "| extremos(outer):", EXT_ot, "\n")

# --- Política elegida ---
# - Conservar OUTLIERS (inner) tal cual (solo marcados)
# - Winsorizar SOLO EXTREMOS (> outer_high) hacia outer_high
v_win <- v
# No negativos:
v_win <- pmax(v_win, 0)
# Winsor extremo superior:
mask_ext_sup <- !is.na(v_win) & v_win > outer_high
v_win[mask_ext_sup] <- outer_high

# Guardar columna winsor resultante
TEA_ESP2$tiempo_espera_min_winsor_ESP2 <- v_win

cat("Aplicado: winsor solo extremos hacia outer_high.\n\n")
```

```{r}
#####################################################################
# 9.2 — Export final de análisis (ESP2)
#####################################################################

out_cols_ESP2 <- c(
  "Tiempo_espera_backup","Sol_1","Sol_2","Sol_3","Sol_4","Sol_5",
  "Sol_6__Ruido_o_texto_no_interpretable__rangos_o_alternativas",
  "Tiempo_espera_min_final_csv_ESP2",
  "tiempo_espera_min_candidato_ESP2",
  "tiempo_espera_min_winsor_ESP2",
  "es_numerico_final_ESP2","es_no_negativo_ESP2","es_razonable_ESP2",
  "flag_outlier_inner_ESP2","flag_extremo_outer_ESP2"
)
out_cols_ESP2 <- intersect(out_cols_ESP2, names(TEA_ESP2))

ESP2_OUT <- TEA_ESP2[, out_cols_ESP2, drop = FALSE]
write.csv(ESP2_OUT, "Tiempo_espera_analisis_sin11.csv", row.names = FALSE, na = "")

cat("[EXPORT] Tiempo_espera_analisis_sin11.csv | Dim:", nrow(ESP2_OUT), "x", ncol(ESP2_OUT), "\n\n")

```

```{r}
#####################################################################
# 9.3 — (Opcional) Chancar en DFT3_backup_2 y eliminar NA por índices (ESP2)
#####################################################################
#####################################################################
# CHUNK — Chancar ambas opciones (winsor vs real)
# - Primero: crear copia DFT3_backup_2_winsor y chantar WINSOR
# - Luego: chantar CANDIDATO en el DFT3_backup_2 original
#####################################################################

# Requisitos previos
stopifnot(exists("DFT3_backup_2"), exists("TEA_ESP2"))

# Vectores fuente
stopifnot("tiempo_espera_min_winsor_ESP2"   %in% names(TEA_ESP2))
stopifnot("tiempo_espera_min_candidato_ESP2" %in% names(TEA_ESP2))

winsor_vec   <- TEA_ESP2$tiempo_espera_min_winsor_ESP2
candidato_vec <- TEA_ESP2$tiempo_espera_min_candidato_ESP2

# Asegurar alineación por posición 1:1
stopifnot(nrow(DFT3_backup_2) == length(winsor_vec),
          nrow(DFT3_backup_2) == length(candidato_vec))

## ---------------------------------------------------------------
## A) COPIA y CHANTAR WINSOR en DFT3_backup_2_winsor
## ---------------------------------------------------------------
DFT3_backup_2_winsor <- DFT3_backup_2   # copia limpia

# Crear/overwrittear columna operativa con WINSOR
target_winsor <- "Tiempo_espera_min_final"
if (!(target_winsor %in% names(DFT3_backup_2_winsor))) {
  DFT3_backup_2_winsor[[target_winsor]] <- NA_real_
}
DFT3_backup_2_winsor[[target_winsor]] <- winsor_vec

cat("[WINSOR] Chantado", shQuote(target_winsor), 
    "en DFT3_backup_2_winsor. NA:", sum(is.na(DFT3_backup_2_winsor[[target_winsor]])), "\n")

## ---------------------------------------------------------------
## B) CHANTAR CANDIDATO REAL en el DFT3_backup_2 ORIGINAL
## ---------------------------------------------------------------
target_cand <- "Tiempo_espera_min_final"
if (!(target_cand %in% names(DFT3_backup_2))) {
  DFT3_backup_2[[target_cand]] <- NA_real_
}
DFT3_backup_2[[target_cand]] <- candidato_vec



```

```{r}
"_tiempo_espera_min_candidato_ESP2_"

DFT3_backup_2 <- DFT3_backup_2 %>%
  rename("_tiempo_espera_min_candidato_ESP2_" = "Tiempo_espera_min_final")
```

```{r}
cat("[REAL] Chantado", shQuote(target_cand), 
    "en DFT3_backup_2. NA:", sum(is.na(DFT3_backup_2[[target_cand]])), "\n")

```

comparativa winsor vs real

```{r}
#####################################################################
# RESUMEN — Impacto de usar WINsor vs Candidato (para informe) [FIX]
#####################################################################

stopifnot(
  exists("TEA_ESP2"),
  "tiempo_espera_min_candidato_ESP2" %in% names(TEA_ESP2),
  "tiempo_espera_min_winsor_ESP2" %in% names(TEA_ESP2)
)

cand <- TEA_ESP2$tiempo_espera_min_candidato_ESP2
win  <- TEA_ESP2$tiempo_espera_min_winsor_ESP2

# Fences a partir del candidato (distribución "real")
Q1  <- as.numeric(quantile(cand, 0.25, na.rm = TRUE))
Q3  <- as.numeric(quantile(cand, 0.75, na.rm = TRUE))
IQR <- Q3 - Q1
inner_low   <- max(0, Q1 - 1.5*IQR)
inner_high  <- Q3 + 1.5*IQR
outer_low   <- max(0, Q1 - 3.0*IQR)
outer_high  <- Q3 + 3.0*IQR

summarize_series <- function(x) {
  c(
    N       = length(x),
    NA_ct   = sum(is.na(x)),
    Min     = suppressWarnings(min(x, na.rm = TRUE)),
    Q1      = as.numeric(quantile(x, 0.25, na.rm = TRUE)),
    Med     = suppressWarnings(median(x, na.rm = TRUE)),
    Mean    = suppressWarnings(mean(x, na.rm = TRUE)),
    Q3      = as.numeric(quantile(x, 0.75, na.rm = TRUE)),
    SD      = suppressWarnings(sd(x, na.rm = TRUE)),
    p95     = as.numeric(quantile(x, 0.95, na.rm = TRUE)),
    p99     = as.numeric(quantile(x, 0.99, na.rm = TRUE)),
    Max     = suppressWarnings(max(x, na.rm = TRUE))
  )
}

res_cand <- summarize_series(cand)
res_win  <- summarize_series(win)

resumen <- round(rbind(CANDIDATO = res_cand, WINSOR = res_win), 2)
print(resumen)

# Deltas útiles para el texto del informe
delta <- round(resumen["WINSOR", c("Mean","SD","p95","p99","Max")] - 
               resumen["CANDIDATO", c("Mean","SD","p95","p99","Max")], 2)

cat("\n=== Cambios (WINSOR - CANDIDATO) ===\n")
print(delta)

# Conteos de outliers/extremos (en base al candidato)
out_inner <- sum(!is.na(cand) & (cand < inner_low | cand > inner_high))
out_outer <- sum(!is.na(cand) & (cand < outer_low | cand > outer_high))

# Cuántos valores fueron efectivamente capados por winsor
capped <- sum(!is.na(cand) & !is.na(win) & cand != win)

cat("\n=== Fences (a partir del CANDIDATO) ===\n")
cat(sprintf("Q1=%.2f | Q3=%.2f | IQR=%.2f | Inner=[%.2f, %.2f] | Outer=[%.2f, %.2f]\n",
            Q1, Q3, IQR, inner_low, inner_high, outer_low, outer_high))
cat(sprintf("Outliers inner: %d | Extremos outer: %d | Capados por winsor: %d\n",
            out_inner, out_outer, capped))

# (Opcional) resumen corto para slide
cat(sprintf("\nSlide-tip → Media: %.2f→%.2f | SD: %.2f→%.2f | p95: %.2f→%.2f | Max: %.2f→%.2f | Capados: %d\n",
            resumen["CANDIDATO","Mean"], resumen["WINSOR","Mean"],
            resumen["CANDIDATO","SD"],   resumen["WINSOR","SD"],
            resumen["CANDIDATO","p95"],  resumen["WINSOR","p95"],
            resumen["CANDIDATO","Max"],  resumen["WINSOR","Max"],
            capped))

```

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

```{r}
summary(DFT3_backup_2)
```

## Definir rangos válidos, Outliers, No imputar por defecto, Trazabilidad -\> Análisis consciente de NA

### (Snapshot correspondiente, Inicio de limpieza y toma de decision final de NA de respectiva columna final)

### Variable : dias:

```{r}
#####################################################################
# === 5) Generar data frame DFDIA con todas las columnas traza ===
#####################################################################

DFDIA <- data.frame(
  dias_backup_fuente = DFT2[[col_obj]],  # valor original
  dias_enteros       = dias_enteros,
  dias_outlier_val   = dias_outlier_val,
  dias_flag_outlier  = dias_flag_outlier,
  solucion_dia1      = solucion_dia1,
  dias_FINAL         = dias_FINAL
)

#recordemos que se elimino 11 filas, anteriormente

#realizamos la misma accion con DFDIA para el analisis conciente de NAs


```

```{r}
write.csv(DFDIA, "DFDIA.csv", row.names = FALSE, na = "", fileEncoding = "UTF-8")

```

```{r}
#####################################################################
# 7.1 — Carga + Snapshot + Preparación (DÍAS)
# Objetivo:
#   - Cargar DFT2_trazas.csv y DFT2_final.csv
#   - Extraer columnas traza para "días"
#   - Traer la columna histórica dias_FINAL para comparar
#   - Dejar un snapshot de tu base operativa (si existe) para auditoría
#####################################################################

library(dplyr)
options(stringsAsFactors = FALSE)

# --- (Opcional) Snapshot de tu base operativa si existe (para auditoría) ---
if (exists("DFT3_backup_2")) {
  stamp_dia <- format(Sys.time(), "%Y%m%d_%H%M%S")
  snap_dia  <- paste0("DFT3_backup_2_snapshot_DIA_", stamp_dia, ".csv")
  write.csv(DFT3_backup_2, snap_dia, row.names = FALSE, na = "")
  cat("[SNAPSHOT] Exportado:", snap_dia, " | Dim:", nrow(DFT3_backup_2), "x", ncol(DFT3_backup_2), "\n\n")
}

# --- Rutas de entrada (ajusta si tus archivos tienen otro nombre) ---
path_trazas_DIA <- "DFT2_trazas.csv"
path_final_DIA  <- "DFT2_final.csv"

stopifnot(file.exists(path_trazas_DIA), file.exists(path_final_DIA))

TRZ_DIA <- read.csv(path_trazas_DIA)
FIN_DIA <- read.csv(path_final_DIA)

cat("[CARGA] TRZ_DIA:", nrow(TRZ_DIA), "x", ncol(TRZ_DIA), "\n")
cat("[CARGA] FIN_DIA:", nrow(FIN_DIA), "x", ncol(FIN_DIA), "\n\n")

# --- Columnas traza específicas de "días" ---
cols_traza_dias_DIA <- c("dias_backup_fuente", "solucion_dia1")
cols_traza_dias_DIA <- intersect(cols_traza_dias_DIA, names(TRZ_DIA))

# --- Columna final histórica (para comparación, no para guiarnos) ---
col_final_csv_DIA <- "dias_FINAL"
stopifnot(col_final_csv_DIA %in% names(FIN_DIA))

# --- Intento de unión por claves comunes; si no las hay, por posición 1:1 ---
KEYS_DIA <- c("id","ID","Id","id_encuesta","id_respuesta","uuid",
              "dni","DNI","matricula","Matricula","grupo","Grupo",
              ".rowid",".ROWID")
keys_common_DIA <- intersect(intersect(names(TRZ_DIA), names(FIN_DIA)), KEYS_DIA)

if (length(keys_common_DIA) > 0) {
  cat("[JOIN] Por claves:", paste(keys_common_DIA, collapse = ", "), "\n")
  TRZ_red_DIA <- TRZ_DIA %>% select(all_of(keys_common_DIA), all_of(cols_traza_dias_DIA))
  FIN_red_DIA <- FIN_DIA %>% select(all_of(keys_common_DIA), all_of(col_final_csv_DIA))
  DIA_TECH <- TRZ_red_DIA %>% left_join(FIN_red_DIA, by = keys_common_DIA)
} else {
  cat("[JOIN] Sin claves; uniendo por posición 1:1...\n")
  stopifnot(nrow(TRZ_DIA) == nrow(FIN_DIA))
  DIA_TECH <- bind_cols(
    TRZ_DIA[, cols_traza_dias_DIA, drop = FALSE],
    FIN_DIA[, col_final_csv_DIA, drop = FALSE]
  )
}

# Renombrar la columna histórica para comparar sin pisarla
names(DIA_TECH)[names(DIA_TECH) == "dias_FINAL"] <- "dias_FINAL_csv_DIA"

cat("[DIA_TECH] Dim:", nrow(DIA_TECH), "x", ncol(DIA_TECH), "\n\n")

```

```{r}
#####################################################################
# 7.2 — Parser robusto para "días" (texto → número)
# Reglas de negocio (del asesor y tu consigna):
#   - Dominio válido: enteros en [0,7]
#   - Fracciones (p. ej., 7.1) no existen → inválido (NA)
#   - Mapas de frases comunes: "todos los días"=7, "lunes a viernes"=5, "fin de semana"=2, "nunca"=0
#   - Si no es inequívoco → NA
#####################################################################

parse_dias_DIA <- function(x) {
  if (is.na(x)) return(NA_real_)
  s <- tolower(trimws(as.character(x)))
  s <- gsub("\\s+", " ", s)

  # Frases inequívocas
  if (grepl("todos los dias|todos los días|diario|cada dia|cada día", s)) return(7)
  if (grepl("lunes a viernes|de lunes a viernes|entre semana", s))       return(5)
  if (grepl("fin de semana|fines de semana|sabado y domingo|sábado y domingo", s)) return(2)
  if (grepl("^nunca$|^ninguno$|^cero$|sin dias|sin días", s))            return(0)

  # Números sueltos (permitimos "7" o "7.0" pero no fracciones != .0)
  num <- suppressWarnings(as.numeric(gsub(",", ".", s)))
  if (!is.na(num)) return(num)

  # Nada interpretable con certeza
  return(NA_real_)
}

```

```{r}
#####################################################################
# 7.3 (FIX) — Construir dias_candidato de forma segura y vectorizada
#####################################################################

# 1) Primera traza: solucion_dia1 (si numérica)
cand1_DIA <- rep(NA_real_, nrow(DIA_TECH))
if ("solucion_dia1" %in% names(DIA_TECH)) {
  v_sol <- suppressWarnings(as.numeric(gsub(",", ".", DIA_TECH$solucion_dia1)))
  cand1_DIA[!is.na(v_sol)] <- v_sol[!is.na(v_sol)]
}

# 2) Completar con parseo de dias_backup_fuente
parsed_bkp <- if ("dias_backup_fuente" %in% names(DIA_TECH)) {
  vapply(DIA_TECH$dias_backup_fuente, parse_dias_DIA, numeric(1))
} else {
  rep(NA_real_, nrow(DIA_TECH))
}
cand2_DIA <- cand1_DIA
fill2 <- is.na(cand2_DIA) & !is.na(parsed_bkp)
cand2_DIA[fill2] <- parsed_bkp[fill2]

# 3) Normalizar SOLO si es entero exacto (7.0 -> 7; 7.1 -> NA)
entero_ok_vec <- !is.na(cand2_DIA) & abs(cand2_DIA - round(cand2_DIA)) < 1e-9
cand_int <- rep(NA_real_, length(cand2_DIA))
cand_int[entero_ok_vec] <- round(cand2_DIA[entero_ok_vec])

# 4) Validar dominio [0,7]
dentro_dom <- !is.na(cand_int) & cand_int >= 0 & cand_int <= 7
cand_final <- rep(NA_real_, length(cand_int))
cand_final[dentro_dom] <- cand_int[dentro_dom]

# 5) Escribir columnas (y NO tocar nada más)
DIA_TECH$dias_candidato_raw <- cand2_DIA
DIA_TECH$dias_candidato     <- cand_final
DIA_TECH$dias_enteros       <- !is.na(DIA_TECH$dias_candidato_raw) &
                               abs(DIA_TECH$dias_candidato_raw - round(DIA_TECH$dias_candidato_raw)) < 1e-9
DIA_TECH$dentro_0_7         <- !is.na(DIA_TECH$dias_candidato) &
                               DIA_TECH$dias_candidato >= 0 & DIA_TECH$dias_candidato <= 7
DIA_TECH$dias_outlier_val   <- ifelse(!is.na(DIA_TECH$dias_candidato_raw) &
                                      (abs(DIA_TECH$dias_candidato_raw - round(DIA_TECH$dias_candidato_raw)) >= 1e-9 |
                                       DIA_TECH$dias_candidato_raw < 0 | DIA_TECH$dias_candidato_raw > 7),
                                      DIA_TECH$dias_candidato_raw, NA_real_)
DIA_TECH$dias_flag_outlier  <- !is.na(DIA_TECH$dias_outlier_val)
DIA_TECH$flag_outlier       <- DIA_TECH$dias_flag_outlier

```

-   **Entero exacto** y **\[0,7\]** son **criterios de negocio** (del asesor).

-   Fracciones o fuera de rango → **inválidos (NA)**. No redondeamos ni “capamos” porque sería **sesgo**.

    ![](images/clipboard-2072054962.png)

despues

```{r}
#####################################################################
# 8.1 — Perfil descriptivo + IQR (informativo; NO para cortar)
#####################################################################

v_dias <- DIA_TECH$dias_candidato
Q1_d   <- as.numeric(quantile(v_dias, 0.25, na.rm = TRUE))
Q3_d   <- as.numeric(quantile(v_dias, 0.75, na.rm = TRUE))
IQR_d  <- Q3_d - Q1_d
inner_low_d  <- Q1_d - 1.5*IQR_d
inner_high_d <- Q3_d + 1.5*IQR_d
outer_low_d  <- Q1_d - 3.0*IQR_d
outer_high_d <- Q3_d + 3.0*IQR_d

cat(sprintf("DÍAS — Q1=%.2f | Q3=%.2f | IQR=%.2f | Inner=[%.2f, %.2f] | Outer=[%.2f, %.2f]\n",
            Q1_d, Q3_d, IQR_d, inner_low_d, inner_high_d, outer_low_d, outer_high_d))
cat("Nota: IQR solo como referencia; el criterio decisor es negocio (enteros en [0,7]).\n\n")

```

```{r}
#####################################################################
# 8.2 — Conteos: NA, fuera de dominio, fracciones; comparación vs histórico
#####################################################################

NA_cand <- sum(is.na(DIA_TECH$dias_candidato))
viol_fuera <- sum(!is.na(DIA_TECH$dias_outlier_val)) # fracciones o <0 o >7

hist_csv <- suppressWarnings(as.numeric(gsub(",", ".", DIA_TECH$dias_FINAL_csv_DIA)))
recuperados <- sum(is.na(hist_csv) & !is.na(DIA_TECH$dias_candidato))
todavia_na  <- sum(is.na(DIA_TECH$dias_candidato))  # NA final

cat(sprintf("CANDIDATO: NA=%d | Violaciones dominio (fracción/<0/>7)=%d | Recuperados vs histórico=%d | NA final=%d\n",
            NA_cand, viol_fuera, recuperados, todavia_na))

```

```{r}
#####################################################################
# 9.1 — Política NA-omit: preparar MÁSCARA e ÍNDICES (sin borrar aún)
#####################################################################
stopifnot(exists("DIA_TECH"))

MASCARA_ELIM_DIAS_DIA <- is.na(DIA_TECH$dias_candidato)
INDICES_ELIM_DIAS_DIA <- which(MASCARA_ELIM_DIAS_DIA)

cat("Filas a eliminar por NA en 'dias_candidato':", length(INDICES_ELIM_DIAS_DIA), 
    "de", nrow(DIA_TECH), "\n")



```

```{r}
cat("notese aun no eliminamos las 11 filas que se eliminaron anterior mente, necesitamos llegar a 756 filas en DIA_TECH, se procedera primero a ello antes de elminar esas 12 filas recomendadas")

```

continua

```{r}
stopifnot(exists("DIA_TECH"))

N_DIA <- nrow(DIA_TECH)


na_cnt <- length(INDICES_ELIM_DIAS_DIA)
na_pct <- if (N_DIA > 0) round(100 * na_cnt / N_DIA, 2) else NA_real_

cat("== DÍAS | NA-omit propuesto ==\n")
cat("Filas totales:", N_DIA, "\n")
cat("Filas a eliminar por NA en 'dias_candidato':", na_cnt, sprintf("(%.2f%%)\n\n", na_pct))

# --- 2) Motivos de invalidez (a partir de 'raw') ---
raw <- DIA_TECH$dias_candidato_raw
es_frac <- !is.na(raw) & abs(raw - round(raw)) >= 1e-9   # 7.3, 6.5, etc.
es_lt0  <- !is.na(raw) & raw < 0                         # negativos
es_gt7  <- !is.na(raw) & raw > 7                         # mayores a 7
texto_no_interpretable <- is.na(raw) & MASCARA_ELIM_DIAS_DIA
# (raw NA y terminó NA en candidato -> no interpretable)

motivos_tab_9_1 <- data.frame(
  motivo      = c("fraccion", "<0", ">7", "texto_no_interpretable"),
  conteo      = c(sum(es_frac), sum(es_lt0), sum(es_gt7), sum(texto_no_interpretable)),
  porcentaje  = round(100 * c(sum(es_frac), sum(es_lt0), sum(es_gt7), sum(texto_no_interpretable)) / N_DIA, 2),
  check.names = FALSE
)

cat("Motivos de NA (auditoría):\n")
print(motivos_tab_9_1); cat("\n")

# --- 3) Muestra de filas a eliminar (si hay columnas ID) ---
id_cands <- intersect(names(DIA_TECH), c("id","ID","Id","id_encuesta","uuid","grupo","Grupo"))
if (na_cnt > 0) {
  cat("Muestra de filas a eliminar (máx 10):\n")
  cols_show <- c(id_cands, "dias_backup_fuente", "solucion_dia1",
                 "dias_candidato_raw", "dias_candidato")
  cols_show <- intersect(cols_show, names(DIA_TECH))
  print(utils::head(DIA_TECH[INDICES_ELIM_DIAS_DIA, cols_show, drop = FALSE], 10))
} else {
  cat("No hay filas a eliminar por esta variable.\n")
}

#####################################################################
# Objetos listos para usar después:
#   - MASCARA_ELIM_DIAS_DIA (lógico: TRUE = eliminar)
#   - INDICES_ELIM_DIAS_DIA (enteros: posiciones a eliminar)
#   - motivos_tab_9_1       (tabla con desglose de motivos)
#####################################################################
```

#se tienen 12 NA filas recomendadas por eliminar

Notese la variable dias en una semana de asistencia a utec, tiene que ser limpiada dentro, usemos los 11 indices anteriores que fueron elminados en la variable anterior (donde se guardaron sus indices)

# Evidencia donde se hace este acoplamiento

```{r}
DIA_TECH_2_backup <- DIA_TECH
```

```{r}
#####################################################################
# Eliminar en DIA_TECH las filas de FILAS_ELIM_TIEMPO_VIAJE
# - Backup: DIA_TECH_backup
# - Valida índices y elimina solo los válidos
#####################################################################

# 0) Requisitos
stopifnot(exists("DIA_TECH"))
stopifnot(exists("FILAS_ELIM_TIEMPO_VIAJE"))

# 1) Respaldo por seguridad
DIA_TECH_backup <- DIA_TECH

# 2) Normalizar y validar los índices
idx_raw <- FILAS_ELIM_TIEMPO_VIAJE
if (!is.integer(idx_raw)) idx_raw <- as.integer(round(idx_raw))
idx <- sort(unique(idx_raw))

n_before <- nrow(DIA_TECH)
valid_range <- seq_len(n_before)
idx_invalid <- idx[!(idx %in% valid_range)]
idx_keep    <- idx[(idx %in% valid_range)]

cat("Filas en DIA_TECH (antes):", n_before, "\n")
cat("Índices recibidos:", paste(idx, collapse = ", "), "\n")
cat("Válidos:", length(idx_keep), " | Inválidos (fuera de rango):", length(idx_invalid), "\n")
if (length(idx_invalid) > 0) {
  cat("Índices fuera de rango ignorados ->", paste(idx_invalid, collapse = ", "), "\n")
}

# 3) Eliminar
if (length(idx_keep) > 0) {
  DIA_TECH <- DIA_TECH[-idx_keep, , drop = FALSE]
}

n_after <- nrow(DIA_TECH)   # << corregido (antes usabas DIA_TECH_2 por error)
cat("Filas en DIA_TECH (después):", n_after, "\n")

# 4) Chequeo esperado (767 - 11 = 756) si ese era tu N inicial
if (n_before == 767 && length(idx_keep) == 11) {
  if (n_after == 756) {
    cat("OK: quedó en 756 observaciones como esperabas.\n")
  } else {
    cat("Aviso: no quedó en 756. Revisa N inicial o índices fuera de rango.\n")
  }
}

# 5) (Opcional) Exporta evidencia de lo que se eliminó
# write.csv(DIA_TECH_backup[idx_keep, , drop = FALSE],
#           "DIA_TECH_filas_eliminadas.csv", row.names = FALSE, na = "")
# cat("Exportado: DIA_TECH_filas_eliminadas.csv\n")

```

```{r}

#como estamos trabajando con DIA_TECH

#BASTA con correr el mismo codigo para ver cuantas fueron eliminadas por efecto de la variable depurada anteriormente 

#esto me dara los NAs finales a eliminar luego de actualizar DIA_TECH

MASCARA_ELIM_DIAS_DIA_fin <- is.na(DIA_TECH$dias_candidato)
INDICES_ELIM_DIAS_DIA_fin <- which(MASCARA_ELIM_DIAS_DIA_fin)

cat("Filas a eliminar por NA en 'dias_candidato':", length(INDICES_ELIM_DIAS_DIA_fin), 
    "de", nrow(DIA_TECH), "\n")

INDICES_ELIM_DIAS_DIA_fin
```

```{r}
#creemos y actualicemos un vector de inidices totales eliminados
```

```{r}
#INDICES A ELIMINAR POR ORDEN
#1
INDICES_ELIM_TIEMPO_ESPERA_ESP2
#2 propuesta (no se usa, dado que se tiene una actualizacion pendiente)
INDICES_ELIM_DIAS_DIA
#los indices cambiaran, al tener menos filas, pero es de notar que se fue 1 NA, luego de la actulizacion.
INDICES_ELIM_DIAS_DIA_fin
```

El numero de NA bajo a 11 lo que nos indica que 1 NA, fue eliminado al eliminar las once filas que guardamos.

Procedemos con el orden habitual

```{r}
#####################################################################
# 9.1 — Política NA-omit (DÍAS): preparar MÁSCARA e ÍNDICES (NO BORRA)
# Objetivo:
#   - Construir la máscara de filas a eliminar por 'dias_candidato' NA
#   - Resumir impacto (conteos y %)
#   - Desglosar motivos (fracción / <0 / >7 / texto no interpretable) desde 'raw'
#   - Dejar objetos listos para 9.3 (aplicación real)
#####################################################################

stopifnot(exists("DIA_TECH"))

N_DIA <- nrow(DIA_TECH)

# --- 1) Máscara e índices por NA en candidato final ---
MASCARA_ELIM_DIAS_DIA_fin <- is.na(DIA_TECH$dias_candidato)
INDICES_ELIM_DIAS_DIA_fin <- which(MASCARA_ELIM_DIAS_DIA_fin)

na_cnt <- length(INDICES_ELIM_DIAS_DIA_fin)
na_pct <- if (N_DIA > 0) round(100 * na_cnt / N_DIA, 2) else NA_real_

cat("== DÍAS | NA-omit propuesto ==\n")
cat("Filas totales:", N_DIA, "\n")
cat("Filas a eliminar por NA en 'dias_candidato':", na_cnt, sprintf("(%.2f%%)\n\n", na_pct))

# --- 2) Motivos de invalidez (a partir de 'raw') ---
raw <- DIA_TECH$dias_candidato_raw
es_frac <- !is.na(raw) & abs(raw - round(raw)) >= 1e-9   # 7.3, 6.5, etc.
es_lt0  <- !is.na(raw) & raw < 0                         # negativos
es_gt7  <- !is.na(raw) & raw > 7                         # mayores a 7
texto_no_interpretable <- is.na(raw) & MASCARA_ELIM_DIAS_DIA_fin
# (raw NA y terminó NA en candidato -> no interpretable)

motivos_tab_9_1 <- data.frame(
  motivo      = c("fraccion", "<0", ">7", "texto_no_interpretable"),
  conteo      = c(sum(es_frac), sum(es_lt0), sum(es_gt7), sum(texto_no_interpretable)),
  porcentaje  = round(100 * c(sum(es_frac), sum(es_lt0), sum(es_gt7), sum(texto_no_interpretable)) / N_DIA, 2),
  check.names = FALSE
)

cat("Motivos de NA (auditoría):\n")
print(motivos_tab_9_1); cat("\n")

# --- 3) Muestra de filas a eliminar (si hay columnas ID) ---
id_cands <- intersect(names(DIA_TECH), c("id","ID","Id","id_encuesta","uuid","grupo","Grupo"))
if (na_cnt > 0) {
  cat("Muestra de filas a eliminar (máx 10):\n")
  cols_show <- c(id_cands, "dias_backup_fuente", "solucion_dia1",
                 "dias_candidato_raw", "dias_candidato")
  cols_show <- intersect(cols_show, names(DIA_TECH))
  print(utils::head(DIA_TECH[INDICES_ELIM_DIAS_DIA_fin, cols_show, drop = FALSE], 11))
} else {
  cat("No hay filas a eliminar por esta variable.\n")
}

#####################################################################
# Objetos listos para usar después:
#   - MASCARA_ELIM_DIAS_DIA (lógico: TRUE = eliminar)
#   - INDICES_ELIM_DIAS_DIA (enteros: posiciones a eliminar)
#   - motivos_tab_9_1       (tabla con desglose de motivos)
#####################################################################

```

```{r}
#####################################################################
# 9.2 — EXPORT de ANÁLISIS (DÍAS) — PRE NA-omit (NO BORRA)
# Objetivo:
#   - Construir un dataset de evidencia con trazas, candidato y flags
#   - Exportar a CSV para auditoría
#   - Imprimir resúmenes (dimensiones, frecuencias, recuperados, NA)
#####################################################################

stopifnot(exists("DIA_TECH"))

# 0) Columnas candidatas a exportar (se incluyen solo si existen)
cols_posibles <- c(
  # TRAZAS
  "dias_backup_fuente", "solucion_dia1",
  # HISTÓRICO (si lo conservaste)
  "dias_FINAL_csv_DIA",
  # RAW y CANDIDATO
  "dias_candidato_raw",      # antes de validar (puede tener 7.5, 8, etc.)
  "dias_candidato",          # final (entero en [0,7] o NA)
  # INDICADORES/FLAGS
  "dias_enteros", "dentro_0_7",
  "dias_outlier_val", "dias_flag_outlier", "flag_outlier"
)
cols_out <- intersect(cols_posibles, names(DIA_TECH))

# 1) Data frame de salida
DIA_OUT <- DIA_TECH[, cols_out, drop = FALSE]

# 2) Guardar CSV (prefijo PRE_omit para distinguir esta etapa)
outfile <- "Dias_analisis_PRE_omit.csv"
write.csv(DIA_OUT, outfile, row.names = FALSE, na = "")

# 3) Resúmenes útiles
N <- nrow(DIA_OUT)
cat("[EXPORT] ", outfile, " | Dim:", N, "x", ncol(DIA_OUT), "\n", sep = "")

# Frecuencias de candidato (incluye NA)
if ("dias_candidato" %in% names(DIA_OUT)) {
  cat("\nFrecuencias de 'dias_candidato' (incluye NA):\n")
  print(table(DIA_OUT$dias_candidato, useNA = "ifany"))
} else {
  cat("\nAviso: falta 'dias_candidato' en DIA_TECH; revisa 7.3/8.2.\n")
}

# Recuperados vs histórico (si lo guardaste)
if (all(c("dias_candidato","dias_FINAL_csv_DIA") %in% names(DIA_OUT))) {
  hist <- suppressWarnings(as.numeric(gsub(",", ".", DIA_OUT$dias_FINAL_csv_DIA)))
  rec  <- sum(is.na(hist) & !is.na(DIA_OUT$dias_candidato))
  cat("\nRecuperados (CSV tenía NA y candidato es válido):", rec, "\n")
}

# Conteo de NA en candidato (impacto potencial de NA-omit)
if ("dias_candidato" %in% names(DIA_OUT)) {
  na_cand <- sum(is.na(DIA_OUT$dias_candidato))
  na_pct  <- if (N > 0) round(100 * na_cand / N, 2) else NA_real_
  cat("NA en 'dias_candidato':", na_cand, sprintf("(%.2f%%)\n", na_pct))
}

# Motivos de invalidez si existe 'dias_candidato_raw'
if ("dias_candidato_raw" %in% names(DIA_OUT)) {
  raw <- DIA_OUT$dias_candidato_raw
  es_frac <- !is.na(raw) & abs(raw - round(raw)) >= 1e-9
  es_lt0  <- !is.na(raw) & raw < 0
  es_gt7  <- !is.na(raw) & raw > 7
  texto_no_interpretable <- is.na(raw) & ("dias_candidato" %in% names(DIA_OUT)) &
                            is.na(DIA_OUT$dias_candidato)

  motivos <- data.frame(
    motivo     = c("fraccion", "<0", ">7", "texto_no_interpretable"),
    conteo     = c(sum(es_frac), sum(es_lt0), sum(es_gt7), sum(texto_no_interpretable)),
    porcentaje = round(100 * c(sum(es_frac), sum(es_lt0), sum(es_gt7), sum(texto_no_interpretable)) / N, 2),
    check.names = FALSE
  )
  cat("\nMotivos de invalidez (sobre 'raw'):\n")
  print(motivos)

  # Muestra de filas problemáticas (si hay trazas)
  idx_inv <- which(es_frac | es_lt0 | es_gt7 | texto_no_interpretable)
  if (length(idx_inv) > 0) {
    show_cols <- intersect(
      c("dias_backup_fuente","solucion_dia1","dias_candidato_raw","dias_candidato"),
      names(DIA_OUT)
    )
    cat("\nMuestra de filas inválidas (máx 10):\n")
    print(utils::head(DIA_OUT[idx_inv, show_cols, drop = FALSE], 10))
  }
}

# 4) Extremos válidos (dentro de [0,7])
if ("dias_candidato" %in% names(DIA_OUT)) {
  v <- DIA_OUT$dias_candidato[!is.na(DIA_OUT$dias_candidato)]
  if (length(v) > 0) cat(sprintf("\nExtremos válidos: Min=%d | Max=%d\n", min(v), max(v)))
}

# 5) Preview
cat("\nPreview (head):\n")
print(utils::head(DIA_OUT, 8))

#####################################################################
# Objetos listos:
#   - DIA_OUT (lo mismo que se exporta)
#   - motivos (si existió 'dias_candidato_raw')
#   - outfile (nombre del CSV generado)
#####################################################################
```

```{r}
#####################################################################
# 9.3 — APLICAR NA-omit (DÍAS) y generar versión filtrada + evidencia
# Requisitos: haber corrido 9.1 (MASCARA_ELIM_DIAS_DIA / INDICES_ELIM_DIAS_DIA)
# Resultado:
#   - DIA_TECH_filtrado  : copia SIN las filas a eliminar
#   - CSV con filas eliminadas (opcional)
#   - Resumen antes/después
#####################################################################

stopifnot(exists("DIA_TECH"))
stopifnot(exists("MASCARA_ELIM_DIAS_DIA_fin"))
stopifnot(exists("INDICES_ELIM_DIAS_DIA_fin"))

# 0) Backup (por seguridad; no se modifica DIA_TECH)
DIA_TECH_backup_9_3 <- DIA_TECH

# 1) Normalizar índices válidos (por si acaso)
n_before <- nrow(DIA_TECH)
idx <- sort(unique(INDICES_ELIM_DIAS_DIA_fin))
idx <- idx[idx >= 1 & idx <= n_before]          # filtra fuera de rango (si los hubiera)

# 2) Aplicar NA-omit en una COPIA (no tocamos DIA_TECH original)
if (length(idx) > 0) {
  DIA_TECH_filtrado <- DIA_TECH[-idx, , drop = FALSE]
} else {
  DIA_TECH_filtrado <- DIA_TECH
}

# 3) Resumen antes/después
cat("== DÍAS | NA-omit aplicado sobre copia ==\n")
cat("Filas antes:", n_before, " | Filas eliminadas:", length(idx),
    " | Filas después:", nrow(DIA_TECH_filtrado), "\n")

# 4) (Opcional) Exportar evidencia de filas eliminadas
#    Incluye trazas útiles si existen
cols_evid <- intersect(
  c("id","ID","Id","id_encuesta","uuid","grupo","Grupo",
    "dias_backup_fuente","solucion_dia1","dias_candidato_raw","dias_candidato"),
  names(DIA_TECH)
)
if (length(idx) > 0 && length(cols_evid) > 0) {
  elim_out <- DIA_TECH[idx, cols_evid, drop = FALSE]
  write.csv(elim_out, "Dias_filas_eliminadas.csv", row.names = FALSE, na = "")
  cat("Exportado: Dias_filas_eliminadas.csv (evidencia de eliminadas)\n")
}

# 5) (Opcional) Verificación rápida de NA remanentes en la copia filtrada
if ("dias_candidato" %in% names(DIA_TECH_filtrado)) {
  na_left <- sum(is.na(DIA_TECH_filtrado$dias_candidato))
  cat("NA remanentes en 'dias_candidato' (copia filtrada):", na_left, "\n")
}

#####################################################################
# OBJETOS QUE QUEDAN LISTOS:
#   - DIA_TECH_filtrado  : dataset depurado para análisis con 'días'
#   - Dias_filas_eliminadas.csv : evidencia (si hubo eliminadas)
#####################################################################

# ===================================================================
# (Opcional) Aplicar lo mismo a tu base MAESTRA por POSICIÓN 1:1
#  - Úsalo solo si confirmas que la maestra y DIA_TECH están alineadas.
# ===================================================================
# if (exists("DFT3_backup_2")) {
#   stopifnot(nrow(DFT3_backup_2) == n_before)  # misma N y orden
#   DFT3_backup_2_filtrado <- if (length(idx) > 0) {
#     DFT3_backup_2[-idx, , drop = FALSE]
#   } else DFT3_backup_2
#   cat("Base maestra filtrada por posición — filas:", nrow(DFT3_backup_2), "->",
#       nrow(DFT3_backup_2_filtrado), "\n")
# }

```

Nos aseguramos de dejar limpia tambien la fila de DFT3_back_up 2 le pasamos las filas que eliminamos en DIA_TECH_filtrado

```{r}
#####################################################################
# APLICAR eliminación por ÍNDICES en tu base maestra: DFT3_backup_2
# - Usa índices guardados para "días" (preferente: INDICES_ELIM_DIAS_VALIDOS)
# - Deja backup, valida rango, elimina y exporta evidencia (opcional)
# - Objetivo esperado: 745 filas (si partías de 756 y eliminas 11)
#####################################################################

# 0) Requisitos
stopifnot(exists("DFT3_backup_2"))

# Tomamos los índices desde el objeto que tengas en memoria
if (exists("INDICES_ELIM_DIAS_VALIDOS_fin")) {
  idx_raw <- INDICES_ELIM_DIAS_DIA_fin
} else if (exists("INDICES_ELIM_DIAS_DIA_fin")) {
  idx_raw <- INDICES_ELIM_DIAS_DIA_fin
} else {
  stop("No encuentro índices de eliminación para 'días'. ",
       "Corre antes el 9.1/9.3 para generar INDICES_ELIM_DIAS_fin.")
}

# 1) Backup por seguridad
DFT3_backup_2_before_dias <- DFT3_backup_2

# 2) Normalizar y validar índices respecto a DFT3_backup_2
n_before <- nrow(DFT3_backup_2)
if (!is.integer(idx_raw)) idx_raw <- as.integer(round(idx_raw))
idx_raw <- idx_raw[!is.na(idx_raw)]
idx_sorted <- sort(unique(idx_raw))

valid_range <- seq_len(n_before)
idx_invalid <- idx_sorted[!(idx_sorted %in% valid_range)]
idx_keep    <- idx_sorted[(idx_sorted %in% valid_range)]

cat("DFT3_backup_2 — filas antes:", n_before, "\n")
cat("Índices recibidos:", paste(idx_sorted, collapse = ", "), "\n")
cat("Válidos:", length(idx_keep), " | Fuera de rango:", length(idx_invalid), "\n")
if (length(idx_invalid) > 0) {
  cat("Se ignorarán índices fuera de rango ->",
      paste(idx_invalid, collapse = ", "), "\n")
}

# 3) (Opcional) Exportar evidencia de lo que vas a eliminar
cols_id_candidatas <- c("id","ID","Id","id_encuesta","uuid","grupo","Grupo")
cols_evid <- intersect(cols_id_candidatas, names(DFT3_backup_2))
if (length(idx_keep) > 0 && length(cols_evid) > 0) {
  evidencia_maestra_dias <- DFT3_backup_2[idx_keep, cols_evid, drop = FALSE]
  evidencia_maestra_dias$.indice_original <- idx_keep
  write.csv(evidencia_maestra_dias,
            "DFT3_backup_2_filas_eliminadas_por_dias.csv",
            row.names = FALSE, na = "")
  cat("Exportado: DFT3_backup_2_filas_eliminadas_por_dias.csv\n")
}

# 4) Eliminar en la base maestra (por ÍNDICE)
if (length(idx_keep) > 0) {
  DFT3_backup_2 <- DFT3_backup_2[-idx_keep, , drop = FALSE]
}

n_after <- nrow(DFT3_backup_2)
cat("DFT3_backup_2 — filas después:", n_after, "\n")

# 5) Chequeo de objetivo (esperado: 745 si partías de 756 y eliminaste 11 válidos)
if (n_before == 756 && length(idx_keep) == 11) {
  if (n_after == 745) {
    cat("OK: quedó en 745 observaciones, como esperabas.\n")
  } else {
    cat("Aviso: no quedó en 745. Revisa si el N inicial era 756 o la cantidad de índices válidos.\n")
  }
}

#####################################################################
# (OPCIONAL) Paso siguiente: chantar 'dias_FINAL' por POSICIÓN 1:1
#  - Úsalo solo si tu fuente (p.ej. DIA_TECH_filtrado$dias_candidato)
#    tiene el mismo N y orden que DFT3_backup_2 DESPUÉS del filtrado.
#####################################################################
# if (exists("DIA_TECH_filtrado") && "dias_candidato" %in% names(DIA_TECH_filtrado)) {
#   stopifnot(nrow(DFT3_backup_2) == nrow(DIA_TECH_filtrado))
#   if (!("dias_FINAL" %in% names(DFT3_backup_2))) {
#     DFT3_backup_2$dias_FINAL <- NA_integer_
#   }
#   DFT3_backup_2$dias_FINAL <- as.integer(DIA_TECH_filtrado$dias_candidato)
#   cat("Chancado 'dias_FINAL' desde 'dias_candidato' aplicado por posición 1:1.\n")
# }

```

```{r}
#esto deberia dar 0 #todo nuestro procedimiento implica haber chancado indirectamente
#dias final, esta listo
sum(is.na(DFT3_backup_2$dias_FINAL))
```

```{r}

summary(DFT3_backup_2)
```

```{r}
library(dplyr)

# 0) Vector con los nombres EXACTOS (tal como están en DFT3_backup_2)
cols_solicitadas <- c(
  "_tiempo_espera_min_candidato_ESP2_",
  "dias_FINAL",
  "Tiempo de viaje FINAL",
  "Satisfacción con el tiempo que te toma llegar (1–5)",
  "Estrés percibido durante el viaje (1–5)",
  "Cansancio al llegar (1–5)",
  "Seguridad percibida (1–5)",
  "Frecuencia con que llegas puntual",
  "Comodidad del transporte (1–5)",
  "Principal_problema_al_trasladarte_FINAL",
  "Tráfico percibido (1–5)",
  "Numero total de medios utilizados_preclean",
  "Tipo de transporte_final",
  "Turno de ingreso usual"
)

# 1) Verificación de existencia
cols_exist   <- intersect(cols_solicitadas, names(DFT3_backup_2))
cols_missing <- setdiff(cols_solicitadas, names(DFT3_backup_2))

cat("Columnas encontradas:", length(cols_exist), "/", length(cols_solicitadas), "\n")
if (length(cols_missing) > 0) {
  cat("⚠️ Faltan en DFT3_backup_2 (se omiten):\n  - ",
      paste(cols_missing, collapse = "\n  - "), "\n", sep = "")
}

# 2) Construir BASE_FINAL en el orden solicitado (solo con las que existen)
BASE_FINAL <- DFT3_backup_2 %>%
  select(all_of(cols_exist))

# 3) Resumen rápido
cat("BASE_FINAL creada. Dimensiones:", nrow(BASE_FINAL), "x", ncol(BASE_FINAL), "\n")

```

```{r}
summary(BASE_FINAL)
```

```{r}
summary(is.na(BASE_FINAL$`Numero total de medios utilizados_preclean`)) #estos NA no son elmininados pues son NA por NA por criterio de elegibilidad (fuera de dominio / fuera del marco del estudio)
```

```{r}
sum(is.na(BASE_FINAL))
```

Eliminacion de filas de las variables cualitativas y razon

```{r}
sum(is.na(BASE_FINAL$Principal_problema_al_trasladarte_FINAL))
```

```{r}
sum(is.na(BASE_FINAL$`Numero total de medios utilizados_preclean`))
```

Veamos si hay solapamiento para calcular el porcentaje total de NAs estructural

```{r}
## ================== SOLAPAMIENTO DE NA EN BASE_FINAL ==================
## Configura las columnas a comparar (ejemplos):
col_a <- "Numero total de medios utilizados_preclean"                      # <-- cámbialo por tu col A en BASE_FINAL
col_b <- "Principal_problema_al_trasladarte_FINAL"  # <-- cámbialo por tu col B en BASE_FINAL

stopifnot(exists("BASE_FINAL"))

# Verificación de existencia y ayuda si faltan
if (!(col_a %in% names(BASE_FINAL)) || !(col_b %in% names(BASE_FINAL))) {
  cat("⚠️ Alguna columna no existe en BASE_FINAL.\n")
  cat("Nombres disponibles en BASE_FINAL:\n"); print(names(BASE_FINAL))
  stop("Revisa 'col_a' y 'col_b'.")
}

N <- nrow(BASE_FINAL)

## Índices de NA por columna
idx_a <- which(is.na(BASE_FINAL[[col_a]]))
idx_b <- which(is.na(BASE_FINAL[[col_b]]))

## Métricas de solapamiento
idx_inter   <- intersect(idx_a, idx_b)  # filas NA en ambas
idx_union   <- union(idx_a, idx_b)      # filas NA en al menos una
na_a        <- length(idx_a)
na_b        <- length(idx_b)
na_overlap  <- length(idx_inter)
na_unicas   <- length(idx_union)
pct_unicas  <- if (N > 0) 100 * na_unicas / N else NA_real_

cat("== Diagnóstico NA en BASE_FINAL ==\n")
cat("N total:", N, "\n")
cat(sprintf("NA en %s: %d\n", col_a, na_a))
cat(sprintf("NA en %s: %d\n", col_b, na_b))
cat(sprintf("Solapamiento (NA en ambas): %d\n", na_overlap))
cat(sprintf("Filas únicas con NA en %s o %s: %d (%.2f%%)\n",
            col_a, col_b, na_unicas, pct_unicas))

## (Opcional) Ver filas exactas del solapamiento con IDs útiles (si existen)
cols_id <- intersect(c("id","ID","Id","id_encuesta","uuid","grupo","Grupo"), names(BASE_FINAL))
if (na_overlap > 0) {
  vista_overlap <- BASE_FINAL[idx_inter, c(cols_id, col_a, col_b), drop = FALSE]
  cat("\nMuestra de filas solapadas (NA en ambas) — máx 10:\n")
  print(utils::head(vista_overlap, 10))
}

## (Opcional) Exportar evidencia a CSV
# if (na_overlap > 0) {
#   write.csv(vista_overlap, "BASE_FINAL_NA_solapamiento.csv", row.names = FALSE, na = "")
#   cat("Exportado: BASE_FINAL_NA_solapamiento.csv\n")
# }

```

## Qué hacer con cada NA restante (y por qué)

-   Consideraciones:

    -   **No imputar.**

    -   **No borrar filas globalmente.**

    -   **Sí** aplicar **NA-omit por variable** **solo** cuando analices el **Objetivo 2** (Puntualidad × Transbordos).

        **Por qué:** esos 5 NA son **fuera de dominio**/**no aplicables** (missing estructural). Imputarlos inventaría un “número de transbordos” inexistente. Borrarlos listwise te hace perder potencia en análisis que **no** usan transbordos.

        **Impacto declarado :**

    -   para Obj.2, el **N analítico** será **745 − 5 = 740**. En el resto de análisis, se mantiene **N = 745**.

-   si eliminamos los 5 NAs que respectan a Numero total de medios utilizados_preclean, terminamos quitando en total 5 \* (14 cols) = 70 celdas.

-   si eliminamos los 10 NAs que respectan a Principal_problema_al_trasladarte_FINAL, terminamos quitando en total 10 \* (14 cols) = 140 celdas.

    ```{r}
    ## === Evidencia de NA en "Numero total de medios utilizados_preclean" (5 filas × 14 columnas) ===

    stopifnot(exists("BASE_FINAL"))

    # 0) Las 14 columnas que componen BASE_FINAL (en el orden solicitado)
    COLS_14 <- c(
      "_tiempo_espera_min_candidato_ESP2_",
      "dias_FINAL",
      "Tiempo de viaje FINAL",
      "Satisfacción con el tiempo que te toma llegar (1–5)",
      "Estrés percibido durante el viaje (1–5)",
      "Cansancio al llegar (1–5)",
      "Seguridad percibida (1–5)",
      "Frecuencia con que llegas puntual",
      "Comodidad del transporte (1–5)",
      "Principal_problema_al_trasladarte_FINAL",
      "Tráfico percibido (1–5)",
      "Numero total de medios utilizados_preclean",
      "Tipo de transporte_final",
      "Turno de ingreso usual"
    )

    # 1) Verifica que todas existan en BASE_FINAL (por si el entorno cambió)
    faltantes <- setdiff(COLS_14, names(BASE_FINAL))
    if (length(faltantes) > 0) {
      stop("Faltan columnas en BASE_FINAL: ", paste(faltantes, collapse = ", "))
    }

    # 2) Índices de las 5 filas con NA en "Numero total de medios utilizados_preclean"
    idx_na_medios <- which(is.na(BASE_FINAL$`Numero total de medios utilizados_preclean`))

    cat("Filas con NA en 'Numero total de medios utilizados_preclean':",
        length(idx_na_medios), "\n")

    # 3) Data frame de evidencia (5 × 14)
    EVID_MEDIOS_5 <- BASE_FINAL[idx_na_medios, COLS_14, drop = FALSE]

    # 4) Chequeo de dimensiones y celdas
    n_filas <- nrow(EVID_MEDIOS_5)
    n_cols  <- ncol(EVID_MEDIOS_5)
    celdas  <- n_filas * n_cols

    cat("Dimensiones evidencia:", n_filas, "x", n_cols, 
        " | Celdas:", celdas, "\n")

    # 5) Exporta respaldo
    write.csv(EVID_MEDIOS_5, "EVID_MEDIOS_5.csv", row.names = FALSE, na = "")
    cat("Exportado: EVID_MEDIOS_5.csv\n")

    # (Opcional) Ver primeras filas en consola
    print(utils::head(EVID_MEDIOS_5, 5))
    ```

    ```{r}
    ## === Evidencia de NA en "Principal_problema_al_trasladarte_FINAL" (10 filas × 14 columnas) ===

    stopifnot(exists("BASE_FINAL"))

    # 0) Las 14 columnas del panel final (mismo orden que definiste)
    COLS_14 <- c(
      "_tiempo_espera_min_candidato_ESP2_",
      "dias_FINAL",
      "Tiempo de viaje FINAL",
      "Satisfacción con el tiempo que te toma llegar (1–5)",
      "Estrés percibido durante el viaje (1–5)",
      "Cansancio al llegar (1–5)",
      "Seguridad percibida (1–5)",
      "Frecuencia con que llegas puntual",
      "Comodidad del transporte (1–5)",
      "Principal_problema_al_trasladarte_FINAL",
      "Tráfico percibido (1–5)",
      "Numero total de medios utilizados_preclean",
      "Tipo de transporte_final",
      "Turno de ingreso usual"
    )

    # 1) Verifica que todas existan en BASE_FINAL
    faltantes <- setdiff(COLS_14, names(BASE_FINAL))
    if (length(faltantes) > 0) {
      stop("Faltan columnas en BASE_FINAL: ", paste(faltantes, collapse = ", "))
    }

    # 2) Índices de filas con NA en "Principal_problema_al_trasladarte_FINAL"
    idx_na_problema <- which(is.na(BASE_FINAL$Principal_problema_al_trasladarte_FINAL))

    cat("Filas con NA en 'Principal_problema_al_trasladarte_FINAL':",
        length(idx_na_problema), "\n")

    # 3) Data frame de evidencia (10 × 14)
    EVID_PROBLEMA_10 <- BASE_FINAL[idx_na_problema, COLS_14, drop = FALSE]

    # 4) Chequeo de dimensiones y celdas
    n_filas <- nrow(EVID_PROBLEMA_10)
    n_cols  <- ncol(EVID_PROBLEMA_10)
    celdas  <- n_filas * n_cols

    cat("Dimensiones evidencia:", n_filas, "x", n_cols,
        " | Celdas:", celdas, "\n")   # debería ser 10 x 14 = 140

    # 5) Exporta respaldo
    write.csv(EVID_PROBLEMA_10, "EVID_PROBLEMA_10.csv", row.names = FALSE, na = "")
    cat("Exportado: EVID_PROBLEMA_10.csv\n")

    # (Opcional) Ver primeras filas en consola
    print(utils::head(EVID_PROBLEMA_10, 10))

    ```

    -   210 celdas de (745\*14) 10430 celdas que nos quedaron finalmente,

        (210/10430)\*100 = 2.01 %, la data de los 210 - 15NA, es data relevante con firmesa que se perderia .

-   2\) `Principal_problema_al_trasladarte_FINAL` — **10 NA**

**Qué hacer:**

-   **No imputar** **No borrar filas globalmente.**

    -   **Sí** aplicar **NA-omit por variable** **solo** cuando analices el **Objetivo 3** (Transporte × Bienestar **controlando** por problema principal).

    -   **Por qué:** es un **covariable** que ajusta el bienestar; si está “fuera de contexto”, no tiene un valor verdadero recuperable. Filtrar **solo** donde se usa evita sesgo y conserva potencia en el resto.

    **Impacto declarado:** para Obj.3, **N analítico = 745 − 10 = 735**. Los demás análisis siguen en **745** (salvo otros filtros propios).

-   3\) Solapamiento = 0 → **no se eliminara 15 filas listwise**

**Qué hacer:** **No** a la eliminación global de 15/745 (2.01%)

**Por qué:** es pérdida de N innecesaria en todos los objetivos. Las dos variables **no** comparten las mismas filas faltantes; por eso el **mejor diseño** es **filtrar por-variable/por-objetivo**.

```{r}
library(dplyr)

# Punto de partida
stopifnot(exists("BASE_FINAL"))

# Máscaras por-variable (NA-omit solo donde corresponde)
M_obj2 <- !is.na(BASE_FINAL[["Numero total de medios utilizados_preclean"]])
M_obj3 <- !is.na(BASE_FINAL[["Principal_problema_al_trasladarte_FINAL"]])

# Datasets analíticos por objetivo
BASE_obj1 <- BASE_FINAL                                   # Transporte × Tiempo (por turno)
BASE_obj2 <- BASE_FINAL %>% filter(M_obj2)                # Puntualidad × Transbordos
BASE_obj3 <- BASE_FINAL %>% filter(M_obj3)                # Transporte × Bienestar (ajustado por problema)

cat("N Obj1:", nrow(BASE_obj1), "\n")  # esperado 745, salvo otros NA propios del objetivo
cat("N Obj2:", nrow(BASE_obj2), "\n")  # esperado 740
cat("N Obj3:", nrow(BASE_obj3), "\n")  # esperado 735

```

Justificacion visual ilustrativa de Na - omit, regresemos al ver estos NAs variable por variable (el encuestado trato de responder dentro de contexto pero de todas formas las demas respuesta si fueron bien respondidas, analizado anteriormente.

```{r}
# 1. Identificar las filas donde 'Numero total de medios utilizados_preclean' se convirtió en NA
filas_con_na_en_preclean <- which(is.na(DFT3_backup_2$`Numero total de medios utilizados_preclean`))

# 2. Extraer los valores originales de esas filas
valores_originales_que_se_hicieron_na <- DFT3_backup_2$`Número total de medios utilizados`[filas_con_na_en_preclean]

# 3. Crear el data frame con los resultados
# La columna 1 contendrá los valores originales (antes de ser NA)
# La columna 2 contendrá los NAs resultantes (solo para confirmar la transformación)
tabla_valores_que_fueron_a_na <- data.frame(
  Valor_Original_Antes_de_NA = valores_originales_que_se_hicieron_na,
  Resultado_en_Preclean = DFT3_backup_2$`Numero total de medios utilizados_preclean`[filas_con_na_en_preclean]
)

# 4. Mostrar la tabla
# La longitud de las filas será igual al número de NAs en la columna preclean.
print(tabla_valores_que_fueron_a_na)
```

Conclusion final para los 5NA:

**Se mantiene la política de *NA-omit* (por variable) para**\

**Numero total de medios utilizados_preclean**. **Por qué (explicación exhaustiva y coherente con todo lo que hicimos)**

**#Definición y dominio de la variable**\

Esta variable es un **conteo** del número de medios usados en un trayecto. Por construcción:

-   Es **discreta** (enteros),

-   **No negativa**, y

-   Contextualmente **≥ 1** si el estudiante efectivamente viajó (0 implicaría “no viajó”, fuera del marco de esta medición de trayecto).

-   En el diagnóstico que generamos juntos quedaron **exactamente 5 casos** como NA tras la depuración, con valores originales:

-   **Rangos**: “2 a 3”, “2 hasta 3” → *ambigüedad intrínseca* (no podemos saber si fue 2 o 3).

-   **Decimales**: “1.6”, “2.5” → *incompatibles con conteo entero*.

-   **Cero**: “0” → *fuera del dominio* para un trayecto reportado.

Estos 5 registros son **“missing estructural”** (también llamado *fuera de contexto/eligibilidad*): no es que falte el dato, sino que el valor **no pertenece al espacio válido** de la variable.

**#Por qué no imputar**

-   Imputar (p. ej., redondear 2.5→2/3, partir rangos en promedios o elegir un extremo) **introduce arbitrariedad** y **distorsiona** la distribución de un **conteo** (que suele modelarse como Poisson/nbin, donde los enteros importan).

-   El “0” equivale a *no aplica* para el estudio del trayecto; convertirlo en 1 o cualquier otro número **falsearía** el fenómeno observado.

-   Desde el punto de vista de **validez de medición**, convertir respuestas *fuera de dominio* en números válidos **mezcla error de medición con dato real** y sesga cualquier análisis posterior (particularmente el Objetivo 2, que usa transbordos derivados de este conteo).

**#Por qué no eliminar filas listwise en la base completa**

-   Lo comprobamos con el cruce de NA que armamos: **no hay solapamiento** entre estos 5 NA y los 10 NA de Principal_problema_al_trasladarte_FINAL.

-   Si borráramos 15 filas globalmente, **perderíamos 2.01% del total de celdas analíticas** (210 de 10,430), **sin necesidad**.

-   La eliminación listwise **reduce potencia** y **puede introducir sesgo** en objetivos que **no** usan esta variable (Obj. 1 y Obj. 3), porque retirarías observaciones válidas para ellos.

**#Política recomendada (implementado en código)**

-   **Conservar los NA como “estructurales”** en la base maestra. Dejamos la trazabilidad en los CSV de evidencia (qué valor original → terminó en NA y por qué).

-   **Aplicar \_NA-omit solo en los análisis que lo requieren**:

    -   En el **Objetivo 2 (Puntualidad × Transbordos)**, filtrar con !is.na(Numero total de medios utilizados_preclean) y trabajar con **N=740** (745−5).

    -   En **Objetivo 1** y **Objetivo 3**, mantener **N=745** (no filtrar por esta variable, porque no es necesaria para el modelo/contraste).

**#Ventajas estadísticas y de reporte**

-   **Transparencia**: en los reportes dejamos el **rastro documental** de qué y por qué se marcó como NA (tablas de evidencia con los 5 valores: rangos, decimales, cero).

-   **Consistencia**: respetamos el **dominio de medición** (conteo entero ≥1), evitando “inventar” datos.

-   **Potencia**: mantenemos el mayor N posible en cada objetivo (evitamos pérdida innecesaria por listwise)

```{r}
## ==== Evidencia: valores que se convirtieron en NA para
##      'Principal_problema_al_trasladarte_FINAL' ====

stopifnot(exists("DFT3_backup_2"))

# 0) Detecta la columna "FINAL"
final_col <- "Principal_problema_al_trasladarte_FINAL"
if (!(final_col %in% names(DFT3_backup_2))) {
  stop("No existe la columna ", shQuote(final_col), " en DFT3_backup_2.")
}

# 1) Sugerencias de nombre para la columna ORIGINAL (ajusta si tu naming difiere)
candidatas_origen <- c("Principal_problema_para_trasladarte_backup")

col_origen <- intersect(candidatas_origen, names(DFT3_backup_2))
if (length(col_origen) == 0) {
  stop(
    "No se encontró automáticamente la columna ORIGEN de 'problema'. ",
    "Indica el nombre exacto (p.ej. crea una variable: col_origen <- '...') y vuelve a correr."
  )
}
# Si hay varias candidatas, usa la primera (puedes cambiarla si prefieres otra)
col_origen <- col_origen[[1]]
cat("Usando columna de origen:", shQuote(col_origen), "\n")

# 2) Filas donde el FINAL quedó en NA
filas_con_na_en_problema <- which(is.na(DFT3_backup_2[[final_col]]))
cat("Filas con NA en", shQuote(final_col), ":", length(filas_con_na_en_problema), "\n")

# 3) Extraer valores originales de esas filas (antes de volverse NA)
valores_originales_que_se_hicieron_na_problema <- DFT3_backup_2[[col_origen]][filas_con_na_en_problema]

# 4) Armar tabla de evidencia (2 columnas): valor original vs resultado FINAL (NA)
tabla_valores_problema_que_fueron_a_na <- data.frame(
  Valor_Original_Antes_de_NA = valores_originales_que_se_hicieron_na_problema,
  Resultado_en_FINAL         = DFT3_backup_2[[final_col]][filas_con_na_en_problema],
  stringsAsFactors = FALSE
)

# 5) Mostrar tabla en consola y (opcional) exportar CSV
print(tabla_valores_problema_que_fueron_a_na)
write.csv(tabla_valores_problema_que_fueron_a_na,
          "EVID_Problema_valores_que_fueron_a_NA.csv",
          row.names = FALSE, na = "")
cat("Exportado: EVID_Problema_valores_que_fueron_a_NA.csv\n")

```

Conclusion final para los 10 NA:

**Se mantiene la decisión de tratar esos 10 casos como *NA por elegibilidad* y aplicar NA-omit solo cuando uses Principal_problema_al_trasladarte_FINAL (OE3)**. Razones, bien explícitas:

#Qué son esos “NA”

-   Los textos “**tráfico, alto costo**”, “**tráfico, falta de puntualidad**”, “**al menos 3**”, etc. revelan que el/la estudiante **listó múltiples problemas** (o una cantidad), cuando la pregunta pedía **un problema principal (uno solo)**.

-   Eso no es “dato perdido” al azar, sino **respuesta fuera del dominio de la variable** (la variable es *una sola categoría principal*).

-   Al recodificarlos a NA, lo que estás marcando es **“no aplicable/indeterminado para ‘principal’”**. Es exactamente el uso de *NA por criterio de elegibilidad* que venimos siguiendo.

#Por qué **no** imputar

-   No hay regla válida para decidir cuál de los varios listados es “el principal”.\

    – Tomar el **primer término** introduce **error sistemático** dependiente del orden de escritura.\

    – Hacer un **reparto aleatorio** viola la medición: estariamos inventando el “principal”.\

    – Asignar una **categoría “multicausal”** cambiaría la definición del constructo (ya no sería “principal”), y rompería la comparabilidad con quienes sí dieron una única opción.

-   Por tanto, **no existe imputación honesta** que respete el constructo “principal”.

#Por qué **no** eliminar filas globalmente

-   Esos 10 casos son **faltantes estructurales en una variable puntual**.

-   Eliminarlos **listwise** te hace perder **N** y potencia en análisis **que no usan** esta covariable (Obj. 1 y 2), sin beneficio alguno.

-   Nuestra política ya acordada: **filtrar por-variable/por-objetivo**. Resultado:\

    – Obj. 3 (usa “principal” como covariable): **N = 745 − 10 = 735**.\

    – Obj. 1 y 2: **N se mantiene en 745**.

#Validez inferencial (por qué NA-omit es lo correcto aquí)

-   Al forzar una categoría a esos casos “multirrespuesta” estarías **mezclando unidades con definición distinta** del constructo “principal”, generando **sesgo de medición** (measurement error) y **atenuación** de efectos.

-   NA-omit **preserva la interpretación**: todas las observaciones usadas en OE3 tienen **un** “principal” claramente definido.

#Alternativas y por qué no son preferibles

-   **Crear dummies multi-etiqueta** (un set de indicadores por problema) sería otro análisis (modelar “mencionó X”), no “principal”. Útil para un anexo exploratorio, **no** para resolver OE3.

-   **Agrupar en “Multicausal”** solo tendría sentido si el diseño la contempló como **opción de respuesta** (no fue el caso). Introducirla post hoc cambia la variable y complica la lectura.

#Impacto y transparencia

-   Impacto declarado: 10 casos → 10 × 14 celdas = **140 celdas** marcadas NA (2.01% junto con los otros 5 NA de transbordos).

-   Lo correcto es **reportarlo** (tabla de evidencia que ya generaste) y **explicar** que la exclusión es **por elegibilidad** únicamente cuando la variable es requerida para un objetivo específico.

#Checklist de buenas prácticas

-   Clasificar como **“NA (fuera de dominio / fuera de marco)”**.

-   **No imputar**.

-   **No eliminar listwise**.

-   **Aplicar drop_na() solo en OE3** al construir el set analítico.

-   **Documentar ejemplos** (p.ej., “tráfico, alto costo”, “al menos 3”).

**Conclusión:** mantener esas 10 respuestas como **NA estructural** y **aplicar NA-omit únicamente en OE3** es metodológicamente correcto y más transparente. Preserva el significado de “principal”, evita sesgo por imputaciones arbitrarias, y minimiza la pérdida de potencia donde no corresponde.

Consideracion para descriptores graficos:

Si algún modelo requiere **casos completos** en un conjunto específico de variables, arma una **vista mínima** con solo esas columnas y aplica `drop_na()` sobre ese subconjunto, no sobre toda la base.

Ademas lo sabido:

“Los NA en `Numero total de medios utilizados_preclean` (n = 5) y en `Principal_problema_al_trasladarte_FINAL` (n = 10) corresponden a **NA por criterio de elegibilidad (fuera de dominio)**. Por definición no se imputan. Para evitar pérdida de potencia innecesaria, se aplicó **NA-omit por variable/análisis** únicamente en los objetivos que requerían dichas variables (Obj.2 y Obj.3, respectivamente). Así, los N analíticos son: Obj.1 = 745; Obj.2 = 740; Obj.3 = 735.”

# JUSTIFICACION DESCRIPTORES GRAFICOS

Nuestros objetivos son explícitamente relacionales (A×B), y además piden **segmentación** (por turno) o **ajuste** (covariables). El bivariado/multivariado materializa esto en pruebas y modelos.

## Objetivo 1:

## Puntualidad × Transbordos (controlando por días de asistencia) Caracterizar los hábitos de puntualidad y examinar su asociación con el número de transbordos, ajustando por la cantidad de días de asistencia semanal a UTEC como covariable (periodo 2025-II).

```{r}
# Supongamos que tu data se llama "mi_data"
# Cambia "mi_data" por el nombre exacto de tu data frame en el panel de Environment

write.csv(BASE_FINAL, "BASE_FINAL.csv", row.names = FALSE)

```

```{r}
# Paquetes
library(dplyr); library(tidyr); library(ggplot2)
library(readr); library(stringr); library(forcats)

# --- Helper para guardar plots de forma uniforme
save_plot <- function(p, file, w = 14, h = 8, dpi = 120) {
  dir.create(dirname(file), showWarnings = FALSE, recursive = TRUE)
  ggsave(filename = file, plot = p, width = w, height = h, dpi = dpi)
  message("Plot exportado: ", file)
}

# --- Chequeo de columnas necesarias
need <- c("Numero total de medios utilizados_preclean",
          "Frecuencia con que llegas puntual",
          "dias_FINAL")
stopifnot(all(need %in% names(BASE_FINAL)))

# --- Alias cortos
col_transb <- "Numero total de medios utilizados_preclean"
col_punt   <- "Frecuencia con que llegas puntual"
col_dias   <- "dias_FINAL"

# Carpeta de salida
outdir <- "OE1_uni_plots"
dir.create(outdir, showWarnings = FALSE)

```

## Univariado previo:

```{r}
# --- U1 (fix): Univariado previo OE1 (transbordos, puntualidad, días) ---
library(dplyr)
library(readr)

# Mapea los nombres reales de tu BASE_FINAL
col_transb <- "Numero total de medios utilizados_preclean"   # conteo (tiene 5 NA estructurales)
col_punt   <- "Frecuencia con que llegas puntual"            # escala ordinal 1–5 (ajusta si difiere)
col_dias   <- "dias_FINAL"                                   # 1..7

# Función genérica de resumen (sin usar nombre de columna 'NA')
resumir_var <- function(nombre, x) {
  x_num <- suppressWarnings(as.numeric(x))
  tibble::tibble(
    variable = nombre,
    N_total  = length(x_num),
    N_valid  = sum(!is.na(x_num)),
    n_na     = sum(is.na(x_num)),
    min      = ifelse(all(is.na(x_num)), NA, min(x_num, na.rm = TRUE)),
    p25      = ifelse(all(is.na(x_num)), NA, quantile(x_num, 0.25, na.rm = TRUE, names = FALSE)),
    mediana  = ifelse(all(is.na(x_num)), NA, median(x_num, na.rm = TRUE)),
    p75      = ifelse(all(is.na(x_num)), NA, quantile(x_num, 0.75, na.rm = TRUE, names = FALSE)),
    max      = ifelse(all(is.na(x_num)), NA, max(x_num, na.rm = TRUE))
  )
}

# Construye el resumen para las tres variables clave del OE1
OE1_resumen_univariado <- bind_rows(
  resumir_var("transbordos",       BASE_FINAL[[col_transb]]),
  resumir_var("puntualidad",       BASE_FINAL[[col_punt]]),
  resumir_var("dias_asistencia",   BASE_FINAL[[col_dias]])
)

print(OE1_resumen_univariado)

# Exporta (opcional)
write_csv(OE1_resumen_univariado, "OE1_resumen_univariado.csv", na = "")
cat("Exportado: OE1_resumen_univariado.csv\n")


```

Distribución de **Transbordos** (conteo entero)

```{r}
df_transb <- BASE_FINAL %>%
  transmute(transb = suppressWarnings(as.integer(.data[[col_transb]]))) %>%
  drop_na(transb)

p_hist_transb <- ggplot(df_transb, aes(x = transb)) +
  geom_histogram(binwidth = 1, boundary = -0.5, color = "grey30", fill = "grey80") +
  scale_x_continuous(breaks = 0:max(df_transb$transb, na.rm = TRUE)) +
  labs(title = "Distribución de transbordos",
       x = "Número total de medios utilizados", y = "Frecuencia") +
  theme_minimal(base_size = 12)

p_bar_transb <- df_transb %>%
  count(transb) %>%
  ggplot(aes(x = factor(transb), y = n)) +
  geom_col() +
  labs(title = "Conteo de casos por número de transbordos",
       x = "Transbordos (conteo)", y = "n") +
  theme_minimal(base_size = 12)

save_plot(p_hist_transb, file.path(outdir, "OE1_hist_transbordos.png"))
save_plot(p_bar_transb,  file.path(outdir, "OE1_bar_transbordos.png"))

```

![](images/clipboard-1660460843.png)

![](images/clipboard-1124115342.png)

Aplicando filter, para los 5NA

```{r}
# --- U2 (fix): Data de trabajo OE1 con NA-omit SOLO en transbordos ---
library(dplyr)
library(ggplot2)

col_transb <- "Numero total de medios utilizados_preclean"
col_punt   <- "Frecuencia con que llegas puntual"
col_dias   <- "dias_FINAL"

U2_df <- BASE_FINAL %>%
  transmute(
    transb = suppressWarnings(as.integer(!!sym(col_transb))),
    punt   = suppressWarnings(as.numeric(!!sym(col_punt))),  # se conserva aunque tenga NA
    dias   = suppressWarnings(as.integer(!!sym(col_dias)))   # se conserva aunque tenga NA
  ) %>%
  filter(!is.na(transb))   # <-- AQUÍ se omiten SOLO los 5 NA de transbordos

# (Opcional) chequeo rápido
cat("N después de NA-omit en transbordos:", nrow(U2_df), "\n")
cat("NA en punt (no filtrados):", sum(is.na(U2_df$punt)), "\n")
cat("NA en dias (no filtrados):", sum(is.na(U2_df$dias)), "\n")

# (Opcional) un par de vistas rápidas
ggplot(U2_df, aes(x = transb)) + geom_bar() + theme_minimal() +
  labs(title = "Distribución de transbordos (con NA-omit solo en transbordos)",
       x = "N° de transbordos", y = "Frecuencia")

```

```{r}
# --- U2 (complemento): gráficos tras NA-omit en transbordos y exportación ---

library(dplyr)
library(ggplot2)
library(rlang)

# Nombres de columnas en BASE_FINAL
col_transb <- "Numero total de medios utilizados_preclean"
col_punt   <- "Frecuencia con que llegas puntual"
col_dias   <- "dias_FINAL"

# 1) Construye el dataset con NA-omit SOLO en transbordos
U2_df <- BASE_FINAL %>%
  transmute(
    transb = suppressWarnings(as.integer(!!sym(col_transb))),
    punt   = suppressWarnings(as.numeric(!!sym(col_punt))),
    dias   = suppressWarnings(as.integer(!!sym(col_dias)))
  ) %>%
  filter(!is.na(transb))   # <- AQUÍ quitamos los 5 NA estructurales de transbordos

cat("N después de NA-omit en transbordos:", nrow(U2_df), "\n")

# 2) Gráfico 1: Histograma (binwidth=1) SOLO con válidos
#    DIFERENCIA vs sin filtrar: desaparece la "barra de NA" implícita; 
#    el histograma refleja únicamente los conteos válidos, por eso las alturas
#    pueden subir levemente (mismo numerador por categoría, menor denominador total).
p_hist_transb_FILT <- ggplot(U2_df, aes(x = transb)) +
  geom_histogram(binwidth = 1, boundary = -0.5, closed = "right") +
  scale_x_continuous(breaks = 0:max(U2_df$transb, na.rm = TRUE)) +
  labs(
    title = "Transbordos (histograma, NA-omit aplicado en transbordos)",
    x = "N° de transbordos", y = "Frecuencia"
  ) +
  theme_minimal(base_size = 12)

# 3) Gráfico 2: Barras (conteo discreto por categoría)
#    DIFERENCIA vs sin filtrar: ya no existe categoría 'NA'; las proporciones
#    relativas por barra se interpretan sobre el N filtrado (p.ej., 740 en vez de 745).
p_bar_transb_FILT <- ggplot(U2_df, aes(x = factor(transb))) +
  geom_bar() +
  labs(
    title = "Transbordos (barras, NA-omit aplicado en transbordos)",
    x = "N° de transbordos (factor)", y = "Frecuencia"
  ) +
  theme_minimal(base_size = 12)

# 4) Exporta ambos gráficos a PNG
dir.create("OE1_univariado", showWarnings = FALSE)

ggsave(
  filename = file.path("OE1_univariado", "U2_hist_transbordos_filtrado.png"),
  plot = p_hist_transb_FILT, width = 9, height = 6, dpi = 120
)
ggsave(
  filename = file.path("OE1_univariado", "U2_bar_transbordos_filtrado.png"),
  plot = p_bar_transb_FILT, width = 9, height = 6, dpi = 120
)

cat("Exportados:\n",
    "- OE1_univariado/U2_hist_transbordos_filtrado.png\n",
    "- OE1_univariado/U2_bar_transbordos_filtrado.png\n", sep = "")

```

![](images/clipboard-615627666.png)

![](images/clipboard-791685369.png)

Distribución de **Puntualidad** (escala 1–5)

```{r}
# --- OE1 | Univariado de Puntualidad (robusto a texto/etiquetas) ---

library(dplyr)
library(stringr)
library(ggplot2)
library(rlang)

outdir <- "OE1_univariado"
dir.create(outdir, showWarnings = FALSE)

# 1) Nombre exacto (ajústalo si difiere)
col_punt <- "Frecuencia con que llegas puntual"

stopifnot(col_punt %in% names(BASE_FINAL))

# 2) Función robusta para parsear a escala 1–5
parse_puntualidad_1a5 <- function(x) {
  # a) si ya es numérico 1–5
  x_num <- suppressWarnings(as.numeric(x))
  ok_num <- !is.na(x_num) & x_num >= 1 & x_num <= 5
  if (sum(ok_num) >= 0.6 * length(x)) return(x_num)  # mayoría ya usable
  
  # b) intenta extraer dígitos 1–5 de cadenas tipo "Siempre (5)" o "5 - Siempre"
  dig <- suppressWarnings(as.numeric(str_extract(as.character(x), "[1-5]")))
  ok_dig <- !is.na(dig)
  
  # c) mapear palabras a 1–5 (ajusta vocabulario si usas otras etiquetas)
  map_pal <- c(
    "nunca" = 1,
    "casi nunca" = 2,
    "algunas veces" = 3, "a veces" = 3, "ocasionalmente" = 3,
    "casi siempre" = 4,
    "siempre" = 5
  )
  low <- str_trim(str_to_lower(as.character(x)))
  # normaliza algunas variantes comunes
  low <- str_replace_all(low, "cas\\s+i", "casi ")
  low <- str_replace_all(low, "alguna[s]?\\s+veces", "algunas veces")
  low <- str_replace_all(low, "aveces", "a veces")
  
  map_val <- unname(map_pal[low])
  map_val <- suppressWarnings(as.numeric(map_val))
  
  # combinar: prioridad dígitos; si no hay, usa mapeo palabras
  out <- ifelse(ok_dig, dig, map_val)
  # si aún quedan NA y x_num era válido en parte, rescata esos
  out[is.na(out)] <- x_num[is.na(out)]
  # filtra fuera de rango
  out[!(out >= 1 & out <= 5)] <- NA_real_
  out
}

# 3) Construir df de puntualidad ya parseado
df_punt <- BASE_FINAL %>%
  transmute(punt_raw = .data[[col_punt]]) %>%
  mutate(punt = parse_puntualidad_1a5(punt_raw))

cat("Puntualidad — N total:", nrow(df_punt),
    " | N válidos:", sum(!is.na(df_punt$punt)),
    " | NA:", sum(is.na(df_punt$punt)), "\n")

# 4) Filtrar válidos (NA-omit SOLO para esta variable)
df_punt_val <- df_punt %>% filter(!is.na(punt))

# 5) Gráficos
p_bar_punt <- ggplot(df_punt_val, aes(x = factor(punt, levels = 1:5))) +
  geom_bar() +
  labs(title = "Distribución de puntualidad (1–5)",
       x = "Puntualidad (1–5)", y = "Frecuencia") +
  theme_minimal(base_size = 12)

p_box_punt <- ggplot(df_punt_val, aes(y = punt, x = 1)) +
  geom_boxplot(width = .25, outlier.alpha = 0.25) +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = 1:5, limits = c(1,5)) +
  labs(title = "Puntualidad — Caja univariada",
       x = NULL, y = "Puntualidad (1–5)") +
  theme_minimal(base_size = 12)

# 6) Exportar (usa ggsave, no save_plot)
ggsave(file.path(outdir, "OE1_bar_puntualidad.png"),
       p_bar_punt, width = 8, height = 5, dpi = 120)
ggsave(file.path(outdir, "OE1_box_puntualidad.png"),
       p_box_punt, width = 6, height = 5, dpi = 120)

cat("Exportados:\n- ", file.path(outdir, "OE1_bar_puntualidad.png"),
    "\n- ", file.path(outdir, "OE1_box_puntualidad.png"), "\n", sep = "")


```

![](images/clipboard-2241658233.png)

![](images/clipboard-1757665018.png)

Distribución de **Días de asistencia** (0–7)

```{r}
df_dias <- BASE_FINAL %>%
  transmute(dias = suppressWarnings(as.integer(.data[[col_dias]]))) %>%
  drop_na(dias)

p_bar_dias <- ggplot(df_dias, aes(x = factor(dias))) +
  geom_bar() +
  labs(title = "Distribución de días de asistencia semanal",
       x = "Días (semana)", y = "Frecuencia") +
  theme_minimal(base_size = 12)

p_box_dias <- ggplot(df_dias, aes(y = dias, x = 1)) +
  geom_boxplot(width = .25, outlier.alpha = 0.25) +
  scale_x_continuous(breaks = NULL) +
  labs(title = "Días de asistencia — Caja univariada",
       x = NULL, y = "Días por semana") +
  theme_minimal(base_size = 12)

save_plot(p_bar_dias, file.path(outdir, "OE1_bar_dias.png"))
save_plot(p_box_dias, file.path(outdir, "OE1_box_dias.png"))

```

![](images/clipboard-183929038.png)

![](images/clipboard-790864581.png)

Analisis Bivariado:

```{r}
# === OE1 — Bivariado: Puntualidad × Transbordos (ajustando por días) ===
# Requisitos previos:
# - Objeto BASE_FINAL en memoria
# - La columna "Frecuencia con que llegas puntual" puede estar en texto o 1–5
# - Transbordos se derivan de: Numero total de medios utilizados_preclean - 1
# - NA-omit SOLO para transbordos (5 NA estructurales)

library(dplyr)
library(ggplot2)
library(stringr)
library(rlang)
library(readr)
library(broom)
library(sandwich)
library(lmtest)

dir.create("OE1_bivariado", showWarnings = FALSE)

# --- B0. Helpers y nombres de columnas ---------------------------------------

col_punt   <- "Frecuencia con que llegas puntual"
col_medios <- "Numero total de medios utilizados_preclean"
col_dias   <- "dias_FINAL"

stopifnot(col_punt %in% names(BASE_FINAL),
          col_medios %in% names(BASE_FINAL),
          col_dias %in% names(BASE_FINAL))

# Conversor robusto de puntualidad a escala 1–5 (mismo del univariado)
parse_puntualidad_1a5 <- function(x) {
  x_num <- suppressWarnings(as.numeric(x))
  ok_num <- !is.na(x_num) & x_num >= 1 & x_num <= 5
  if (sum(ok_num) >= 0.6 * length(x)) return(x_num)
  dig <- suppressWarnings(as.numeric(stringr::str_extract(as.character(x), "[1-5]")))
  map_pal <- c(
    "nunca" = 1,
    "casi nunca" = 2,
    "algunas veces" = 3, "a veces" = 3, "ocasionalmente" = 3,
    "casi siempre" = 4,
    "siempre" = 5
  )
  low <- stringr::str_trim(stringr::str_to_lower(as.character(x)))
  low <- stringr::str_replace_all(low, "cas\\s+i", "casi ")
  low <- stringr::str_replace_all(low, "alguna[s]?\\s+veces", "algunas veces")
  low <- stringr::str_replace_all(low, "aveces", "a veces")
  map_val <- unname(map_pal[low]); map_val <- suppressWarnings(as.numeric(map_val))
  out <- ifelse(!is.na(dig), dig, map_val)
  out[is.na(out)] <- x_num[is.na(out)]
  out[!(out >= 1 & out <= 5)] <- NA_real_
  out
}

```

```{r}
# --- B1. Armar dataset analítico (NA-omit SOLO en transbordos) ---------------

OE1 <- BASE_FINAL %>%
  transmute(
    punt_15 = parse_puntualidad_1a5(.data[[col_punt]]),      # 1–5 (ordinal numérica)
    medios  = suppressWarnings(as.numeric(.data[[col_medios]])),
    # TRANSBORDOS = medios - 1 (mínimo 0). *** Aquí aplicamos NA-omit SOLO si transbordos es NA ***
    transb  = ifelse(is.na(medios), NA_real_, pmax(medios - 1, 0)),
    dias    = suppressWarnings(as.numeric(.data[[col_dias]])) # esperado 1–7
  )

# Resumen de faltantes y N
cat("OE1 — N total:", nrow(OE1), "\n")
cat("NA en transb:", sum(is.na(OE1$transb)), " (deben ser 5 NA estructurales)\n")
cat("NA en punt_15:", sum(is.na(OE1$punt_15)), "\n")
cat("NA en dias   :", sum(is.na(OE1$dias)), "\n")

# Vista FILTRADA para análisis (omitimos SOLO transbordos NA, según política)
OE1_FILT <- OE1 %>% filter(!is.na(transb))

cat("OE1 — N analítico (tras NA-omit SOLO en transb):", nrow(OE1_FILT), "\n")

```

```{r}
#install.packages("Hmisc")
library(Hmisc)
```

```{r}

# --- B2. Gráficos previos: transbordos por puntualidad -----------------------
# a) Boxplot transbordos ~ punt_15
g_box <- ggplot(OE1_FILT, aes(x = factor(punt_15, levels = 1:5), y = transb)) +
  geom_boxplot(outlier.alpha = 0.25) +
  labs(title = "Transbordos por nivel de puntualidad",
       x = "Puntualidad (1–5)", y = "Transbordos (conteo)") +
  theme_minimal(base_size = 12)

# b) Jitter + medias por nivel de puntualidad
g_jit <- ggplot(OE1_FILT, aes(x = factor(punt_15, levels = 1:5), y = transb)) +
  geom_jitter(width = 0.15, height = 0.05, alpha = 0.25) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  stat_summary(fun.data = mean_cl_normal, geom = "errorbar", width = .15) +
  labs(title = "Transbordos por puntualidad (puntos y medias ± IC)",
       x = "Puntualidad (1–5)", y = "Transbordos") +
  theme_minimal(base_size = 12)

ggsave("OE1_bivariado/OE1_box_transb_por_punt.png", g_box, width = 8, height = 5, dpi = 120)
ggsave("OE1_bivariado/OE1_jitter_transb_por_punt.png", g_jit, width = 8, height = 5, dpi = 120)

```

![](images/clipboard-3068277546.png)

```{r}
# --- B3. Asociación monotónica simple: Spearman ------------------------------
# (útil porque punt_15 es ordinal y transb es conteo posiblemente no normal)

spearman_res <- suppressWarnings(cor.test(
  ~ punt_15 + transb,
  data = OE1_FILT,
  method = "spearman",
  exact  = FALSE
))

print(spearman_res)

# Guardar a CSV resumen rápido
readr::write_csv(
  tibble::tibble(
    metodo = "Spearman",
    rho    = unname(spearman_res$estimate),
    pval   = spearman_res$p.value,
    n      = sum(complete.cases(OE1_FILT[, c("punt_15","transb")]))
  ),
  "OE1_bivariado/OE1_spearman_punt_vs_transb.csv"
)

```

```{r}
# --- B4. Modelo de conteo: Poisson / quasi-Poisson / NB con días como covar ---
# transb ~ punt_15 + dias   (sin interacciones, lectura clara)
# - Usamos enlaces log y reportamos IRR = exp(beta)
# - SE robustos (HC3) cuando es posible; si hay sobredispersión fuerte, quasi-Poisson o NB.

# 1) Poisson básico
mod_pois <- glm(transb ~ punt_15 + dias, data = OE1_FILT, family = poisson(link = "log"))

# 2) Chequeo de sobredispersión
phi <- sum(residuals(mod_pois, type = "pearson")^2) / mod_pois$df.residual
cat("Dispersion (phi) en Poisson:", round(phi, 3), "\n")

usar_quasi <- is.finite(phi) && phi > 1.5

if (usar_quasi) {
  # Quasi-Poisson (ajusta var = phi*mu)
  mod_qp <- glm(transb ~ punt_15 + dias, data = OE1_FILT, family = quasipoisson(link = "log"))
  coefs_qp <- broom::tidy(lmtest::coeftest(mod_qp, vcov. = sandwich::vcovHC(mod_qp, type = "HC3")))
  coefs_qp <- coefs_qp %>%
    mutate(IRR = exp(estimate),
           conf.low  = exp(estimate - 1.96*std.error),
           conf.high = exp(estimate + 1.96*std.error),
           modelo = "quasi-poisson (HC3)")
  
  readr::write_csv(coefs_qp, "OE1_bivariado/OE1_modelo_quasi_poisson_IRR.csv")
  print(coefs_qp)
  
} else {
  # Poisson con SE robustos HC3
  coefs_pois <- broom::tidy(lmtest::coeftest(mod_pois, vcov. = sandwich::vcovHC(mod_pois, type = "HC3")))
  coefs_pois <- coefs_pois %>%
    mutate(IRR = exp(estimate),
           conf.low  = exp(estimate - 1.96*std.error),
           conf.high = exp(estimate + 1.96*std.error),
           modelo = "poisson (HC3)")
  
  readr::write_csv(coefs_pois, "OE1_bivariado/OE1_modelo_poisson_IRR.csv")
  print(coefs_pois)
}

# 3) (Opcional) Respaldo NB si quieres comparativo
suppressWarnings({
  if (requireNamespace("MASS", quietly = TRUE)) {
    mod_nb <- try(MASS::glm.nb(transb ~ punt_15 + dias, data = OE1_FILT), silent = TRUE)
    if (!inherits(mod_nb, "try-error")) {
      coefs_nb <- broom::tidy(mod_nb) %>%
        mutate(IRR = exp(estimate),
               conf.low  = exp(estimate - 1.96*std.error),
               conf.high = exp(estimate + 1.96*std.error),
               modelo = "neg-bin")
      readr::write_csv(coefs_nb, "OE1_bivariado/OE1_modelo_nb_IRR.csv")
      print(coefs_nb)
    } else {
      message("NB no se pudo ajustar; probablemente N o varianza insuficientes.")
    }
  }
})

```

```{r}
# --- B5. Gráfico de predicciones ajustadas (si el Poisson/quasi converge) ----
# Curva de transbordos esperados vs puntualidad, a días fijos (mediana)
if (!all(is.na(OE1_FILT$punt_15)) && !all(is.na(OE1_FILT$dias))) {
  dias_ref <- stats::median(OE1_FILT$dias, na.rm = TRUE)
  grid <- tidyr::expand_grid(
    punt_15 = 1:5,
    dias    = dias_ref
  )
  # Usa el mejor modelo disponible (quasi si hubo sobredispersión)
  if (exists("mod_qp")) {
    grid$fit <- predict(mod_qp, newdata = grid, type = "response")
    lbl <- "quasi-poisson"
  } else {
    grid$fit <- predict(mod_pois, newdata = grid, type = "response")
    lbl <- "poisson"
  }
  
  g_pred <- ggplot(grid, aes(x = punt_15, y = fit)) +
    geom_line() +
    geom_point(size = 2) +
    scale_x_continuous(breaks = 1:5) +
    labs(title = paste0("Transbordos esperados vs Puntualidad (", lbl, ")"),
         subtitle = paste0("Ajustado por días = ", dias_ref),
         x = "Puntualidad (1–5)", y = "Transbordos esperados") +
    theme_minimal(base_size = 12)
  
  ggsave("OE1_bivariado/OE1_pred_transb_vs_punt.png", g_pred, width = 7, height = 5, dpi = 120)
}

```

------------------------------------------------------------------------

## Objetivo especifico 2:

### Analisis Univariado:

```{r}
#####################################################################
# OE2-U0 — Paquetes y nombres de columnas
#####################################################################
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(forcats)

# === Ajusta estos nombres si difieren en tu BASE_FINAL ===
col_punt   <- "Frecuencia con que llegas puntual"     # 1–5 (puede venir como texto/char)
col_medios <- "Numero total de medios utilizados_preclean"  # conteo entero (tiene 5 NA estructurales)
col_dias   <- "dias_FINAL"                             # 1–7 (entero)

# Carpeta de salidas
dir.create("OE2_univar", showWarnings = FALSE)

```

```{r}
#####################################################################
# OE2-U1 — Construcción de variables y vistas (con NA-omit SOLO en Transbordos)
#####################################################################

# 1) Normaliza puntualidad a (1–5) numérico robusto
puntualidad_a_15 <- function(x){
  # Si ya es numérico 1–5:
  x_num <- suppressWarnings(as.numeric(x))
  if (all(is.na(x_num)) || any(x_num < 1 | x_num > 5, na.rm = TRUE)) {
    # Mapea textos comunes a 1–5 (ajusta si usaste etiquetas locales)
    x_chr <- as.character(x)
    rec <- case_when(
      str_detect(x_chr, regex("^siempre$|^muy alta$|^5$",  TRUE)) ~ 5,
      str_detect(x_chr, regex("^casi siempre$|^alta$|^4$", TRUE)) ~ 4,
      str_detect(x_chr, regex("^a veces$|^media$|^3$",     TRUE)) ~ 3,
      str_detect(x_chr, regex("^raramente$|^baja$|^2$",    TRUE)) ~ 2,
      str_detect(x_chr, regex("^nunca$|^muy baja$|^1$",    TRUE)) ~ 1,
      TRUE ~ NA_real_
    )
    return(as.numeric(rec))
  } else {
    return(as.numeric(x_num))
  }
}

# 2) Construye dataset base con las 3 variables clave
OE2_base <- BASE_FINAL %>%
  transmute(
    punt_15 = puntualidad_a_15(.data[[col_punt]]),     # 1–5
    medios  = suppressWarnings(as.numeric(.data[[col_medios]])), # entero
    dias    = suppressWarnings(as.numeric(.data[[col_dias]]))    # entero
  ) %>%
  mutate(
    transb = pmax(medios - 1, 0)  # transbordos >= 0
  )

# 3) Vista CON NA-omit SOLO para transbordos (5 NA estructurales fuera)
OE2_transb_FILT <- OE2_base %>%
  filter(!is.na(transb))

# 4) Guardar “foto” rápida de conteos
resumen_N <- tibble::tibble(
  N_total       = nrow(OE2_base),
  N_transb_val  = sum(!is.na(OE2_base$transb)),
  N_transb_NA   = sum(is.na(OE2_base$transb)),
  N_punt_val    = sum(!is.na(OE2_base$punt_15)),
  N_dias_val    = sum(!is.na(OE2_base$dias))
)
write_csv(resumen_N, file.path("OE2_univar","OE2_U1_resumen_N.csv"))
print(resumen_N)

```

```{r}
#####################################################################
# OE2-U2 — Tablas univariadas (estadística descriptiva) — FIX
# - No usar nombres de columna "NA" (rompe el parser).
# - Reportamos N_total, N_valid y NA_count.
# - Para transbordos usamos el objeto ya filtrado (NA-omit aplicado).
#####################################################################

library(dplyr)
library(readr)
library(tidyr)

dir.create("OE2_univar", showWarnings = FALSE)

# Función resumen numérica robusta (recibe un vector, cuenta NA y resume)
resumen_num <- function(x){
  x_raw    <- suppressWarnings(as.numeric(x))
  N_total  <- length(x_raw)
  NA_count <- sum(is.na(x_raw))
  x_ok     <- x_raw[!is.na(x_raw)]

  if (length(x_ok) == 0) {
    return(tibble::tibble(
      N_total  = N_total,
      N_valid  = 0,
      NA_count = NA_count,
      Min = NA_real_, Q1 = NA_real_, Med = NA_real_, Mean = NA_real_,
      Q3  = NA_real_, Max = NA_real_
    ))
  }

  tibble::tibble(
    N_total  = N_total,
    N_valid  = length(x_ok),
    NA_count = NA_count,
    Min  = min(x_ok),
    Q1   = quantile(x_ok, 0.25, names = FALSE),
    Med  = median(x_ok),
    Mean = mean(x_ok),
    Q3   = quantile(x_ok, 0.75, names = FALSE),
    Max  = max(x_ok)
  )
}

# a) Transbordos — YA FILTRADO (NA-omit aplicado a transbordos)
#    -> si OE2_transb_FILT$transb no existe, usa OE2_base$transb y deja que la función cuente NA.
stopifnot(exists("OE2_transb_FILT"))
tab_transb <- resumen_num(OE2_transb_FILT$transb)
write_csv(tab_transb, file.path("OE2_univar","OE2_U2_transbordos_stats.csv"))

# b) Puntualidad — NA-omit SOLO de puntualidad (para describirla)
stopifnot(exists("OE2_base"))
tab_punt <- resumen_num(OE2_base$punt_15)
write_csv(tab_punt, file.path("OE2_univar","OE2_U2_puntualidad_stats.csv"))

# c) Días — NA-omit SOLO de días (para describirla)
tab_dias <- resumen_num(OE2_base$dias)
write_csv(tab_dias, file.path("OE2_univar","OE2_U2_dias_stats.csv"))

tab_transb; tab_punt; tab_dias


```

```{r}
#####################################################################
# OE2-U3 — Gráficos univariados y export (PNG)
#   - Transbordos: versión FILTRADA (NA-omit aplicado a transbordos)
#   - Puntualidad y Días: se omite NA SOLO de la propia variable
#####################################################################

save_plot <- function(p, path, w=10, h=6, dpi=120){
  ggsave(filename = path, plot = p, width = w, height = h, dpi = dpi)
}

## --- Transbordos (FILTRADO) ---
df_transb <- OE2_transb_FILT %>%
  transmute(transb = as.integer(round(transb))) %>%
  filter(!is.na(transb)) %>%
  mutate(transb_f = factor(transb, levels = sort(unique(transb))))

p_bar_transb_FILT <- ggplot(df_transb, aes(x = transb_f)) +
  geom_bar() +
  labs(title = "OE2 — Distribución de Transbordos (NA-omit aplicado a transbordos)",
       x = "Transbordos (conteo)", y = "Frecuencia") +
  theme_minimal(base_size = 12)

p_box_transb_FILT <- ggplot(df_transb, aes(y = transb, x = 1)) +
  geom_boxplot(width = .25, outlier.alpha = 0.25) +
  scale_x_continuous(breaks = NULL) +
  labs(title = "OE2 — Caja univariada de Transbordos (con NA-omit en transbordos)",
       x = NULL, y = "Transbordos") +
  theme_minimal(base_size = 12)

save_plot(p_bar_transb_FILT, file.path("OE2_univar","OE2_bar_transbordos_FILTRADO.png"))
save_plot(p_box_transb_FILT, file.path("OE2_univar","OE2_box_transbordos_FILTRADO.png"))

## --- Puntualidad (solo omito NA de puntualidad para graficar) ---
df_punt <- OE2_base %>%
  transmute(punt = punt_15) %>%
  drop_na(punt) %>%
  mutate(punt_f = factor(punt, levels = sort(unique(punt))))

p_bar_punt <- ggplot(df_punt, aes(x = punt_f)) +
  geom_bar() +
  labs(title = "OE2 — Distribución de Puntualidad",
       x = "Puntualidad (1–5)", y = "Frecuencia") +
  theme_minimal(base_size = 12)

p_box_punt <- ggplot(df_punt, aes(y = punt, x = 1)) +
  geom_boxplot(width = .25, outlier.alpha = 0.25) +
  scale_x_continuous(breaks = NULL) +
  labs(title = "OE2 — Caja univariada de Puntualidad",
       x = NULL, y = "Puntualidad (1–5)") +
  theme_minimal(base_size = 12)

save_plot(p_bar_punt, file.path("OE2_univar","OE2_bar_puntualidad.png"))
save_plot(p_box_punt, file.path("OE2_univar","OE2_box_puntualidad.png"))

## --- Días (solo omito NA de días para graficar) ---
df_dias <- OE2_base %>%
  transmute(dias = dias) %>%
  drop_na(dias) %>%
  mutate(dias_f = factor(dias, levels = sort(unique(dias))))

p_bar_dias <- ggplot(df_dias, aes(x = dias_f)) +
  geom_bar() +
  labs(title = "OE2 — Distribución de Días de asistencia",
       x = "Días/semana", y = "Frecuencia") +
  theme_minimal(base_size = 12)

p_box_dias <- ggplot(df_dias, aes(y = dias, x = 1)) +
  geom_boxplot(width = .25, outlier.alpha = 0.25) +
  scale_x_continuous(breaks = NULL) +
  labs(title = "OE2 — Caja univariada de Días de asistencia",
       x = NULL, y = "Días/semana") +
  theme_minimal(base_size = 12)

save_plot(p_bar_dias, file.path("OE2_univar","OE2_bar_dias.png"))
save_plot(p_box_dias, file.path("OE2_univar","OE2_box_dias.png"))

```

### Analisis Bivariado:

```{r}
#install.packages("performance")
```

```{r}
#####################################################################
# OE2-B0 — Setup (rutas, paquetes, nombres de columnas)
#####################################################################
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(broom)
library(ppcor)     # para Spearman parcial
library(MASS)      # glm.nb y polr
library(performance) # para chequeo de sobredispersión (opcional)
library(forcats)
library(stringr)

# === Nombres en tu BASE_FINAL (ajusta si difieren) ===
col_punt   <- "Frecuencia con que llegas puntual"             # 1–5 (puede venir como texto/char)
col_medios <- "Numero total de medios utilizados_preclean"    # conteo entero (tiene 5 NA estructurales)
col_dias   <- "dias_FINAL"                                    # 1–7

# Carpeta de salidas bivariado
dir.create("OE2_bivar", showWarnings = FALSE)

# Función: normaliza puntualidad a 1–5 numérico (misma que usamos)
puntualidad_a_15 <- function(x){
  x_num <- suppressWarnings(as.numeric(x))
  if (all(is.na(x_num)) || any(x_num < 1 | x_num > 5, na.rm = TRUE)) {
    x_chr <- as.character(x)
    rec <- dplyr::case_when(
      str_detect(x_chr, regex("^siempre$|^muy alta$|^5$",  TRUE)) ~ 5,
      str_detect(x_chr, regex("^casi siempre$|^alta$|^4$", TRUE)) ~ 4,
      str_detect(x_chr, regex("^a veces$|^media$|^3$",     TRUE)) ~ 3,
      str_detect(x_chr, regex("^raramente$|^baja$|^2$",    TRUE)) ~ 2,
      str_detect(x_chr, regex("^nunca$|^muy baja$|^1$",    TRUE)) ~ 1,
      TRUE ~ NA_real_
    )
    return(as.numeric(rec))
  } else {
    return(as.numeric(x_num))
  }
}

```

```{r}
#####################################################################
# OE2-B1 — Construir dataset analítico (NA-omit SOLO en transbordos)
#####################################################################
OE2_b <- BASE_FINAL %>%
  transmute(
    punt_15 = puntualidad_a_15(.data[[col_punt]]),                 # 1–5
    medios  = suppressWarnings(as.numeric(.data[[col_medios]])),    # entero (puede traer NA estructural)
    dias    = suppressWarnings(as.numeric(.data[[col_dias]]))       # entero 1–7
  ) %>%
  mutate(
    transb = pmax(medios - 1, 0) # transbordos (conteo >= 0)
  )

# NA-omit SOLO en transbordos:
OE2_b_TR <- OE2_b %>% filter(!is.na(transb))

# Foto de N
N_bivar <- tibble::tibble(
  N_total      = nrow(OE2_b),
  N_transb_val = sum(!is.na(OE2_b$transb)),
  N_transb_NA  = sum(is.na(OE2_b$transb)),
  N_punt_val   = sum(!is.na(OE2_b$punt_15)),
  N_dias_val   = sum(!is.na(OE2_b$dias))
)
readr::write_csv(N_bivar, file.path("OE2_bivar","OE2_B1_resumen_N.csv"))
print(N_bivar)

```

```{r}
#####################################################################
# OE2-B2 — Correlación Spearman (bruta y parcial controlando por días)
#   - Bruta : cor(punt_15, transb)
#   - Parcial: pcor.test(punt_15, transb | dias)
# Notas:
#   * NA-omit SOLO en las variables usadas por cada contraste
#   * Namespacing explícito (dplyr::) para evitar choques con otras libs
#####################################################################

# Paquetes necesarios
library(dplyr)
library(tidyr)
library(broom)
library(ppcor)
library(readr)
library(tibble)

# (A) Spearman BRUTA (punt_15 vs transb)
dat_s_b <- OE2_b %>%
  dplyr::select(punt_15, transb) %>%     # <- nombres exactos en OE2_b
  tidyr::drop_na()                        # NA-omit solo en estas 2

# Aviso rápido de N
cat("OE2-B2 | Spearman BRUTA -> N =", nrow(dat_s_b), "\n")

cor_b <- suppressWarnings(
  cor.test(dat_s_b$punt_15, dat_s_b$transb,
           method = "spearman", exact = FALSE)
)

out_b <- broom::tidy(cor_b) %>%
  dplyr::mutate(tipo = "Spearman_bruta",
                N = nrow(dat_s_b))

readr::write_csv(out_b, file.path("OE2_bivar","OE2_B2_Spearman_bruta.csv"))
print(out_b)

# (B) Spearman PARCIAL (controlando por días)
dat_s_p <- OE2_b %>%
  dplyr::select(punt_15, transb, dias) %>%  # <- incluye el control
  tidyr::drop_na()                           # NA-omit en las 3

cat("OE2-B2 | Spearman PARCIAL -> N =", nrow(dat_s_p), "\n")

pc <- tryCatch(
  ppcor::pcor.test(x = dat_s_p$punt_15,
                   y = dat_s_p$transb,
                   z = dplyr::select(dat_s_p, dias),
                   method = "spearman"),
  error = function(e) { message("pcor.test error: ", e$message); NULL }
)

if (!is.null(pc)) {
  out_p <- tibble::tibble(
    estimate   = unname(pc$estimate),
    statistic  = unname(pc$statistic),
    p.value    = unname(pc$p.value),
    method     = "Spearman parcial (control dias)",
    parameter  = NA_real_,
    conf.low   = NA_real_,
    conf.high  = NA_real_,
    tipo       = "Spearman_parcial",
    N          = nrow(dat_s_p)
  )
  readr::write_csv(out_p, file.path("OE2_bivar","OE2_B2_Spearman_parcial.csv"))
  print(out_p)
} else {
  cat("Spearman parcial no calculable (n insuficiente o problema numérico).\n")
}


```

```{r}
#####################################################################
# OE2-B3 — Gráfico 1: Boxplot de transbordos por nivel de puntualidad
#   - NA-omit en transbordos
#   - Se anotan Ns por categoría de puntualidad
#####################################################################
df_bx <- OE2_b_TR %>%
  transmute(
    transb = as.integer(round(transb)),
    punt_f = factor(punt_15, levels = sort(unique(punt_15)))
  ) %>%
  tidyr::drop_na(punt_f) # sólo para este gráfico

# Conteo por grupo (para anotar)
n_por_punt <- df_bx %>%
  count(punt_f, name = "n") %>%
  mutate(label = paste0("n=", n))

p_box <- ggplot(df_bx, aes(x = punt_f, y = transb)) +
  geom_boxplot(outlier.alpha = 0.25) +
  geom_text(data = n_por_punt, aes(x = punt_f, y = -0.3, label = label),
            inherit.aes = FALSE, size = 3) +
  labs(title = "OE2 — Transbordos por nivel de puntualidad",
       x = "Puntualidad (1–5)", y = "Transbordos (conteo)") +
  theme_minimal(base_size = 12)

ggsave(file.path("OE2_bivar","OE2_B3_box_transb_por_punt.png"),
       plot = p_box, width = 10, height = 6, dpi = 120)
p_box

```

```{r}
#####################################################################
# OE2 — PREÁMBULO ROBUSTO para (transb, punt_15, dias)
# Evita choques con select(); valida nombres; aplica NA-omit focal.
#####################################################################

# 0) Verifica objeto base
stopifnot(exists("OE2_b"))

# 1) Chequeo de nombres disponibles (te ayuda a ver si hay typos)
cat("Columnas en OE2_b:\n")
print(names(OE2_b))

# 2) Conjunto requerido
need <- c("transb", "punt_15", "dias")
have <- intersect(need, names(OE2_b))
miss <- setdiff(need, have)

if (length(miss) > 0) {
  stop("Faltan columnas en OE2_b: ", paste(shQuote(miss), collapse = ", "),
       "\nRevisa el naming (p. ej., punt_15, transb, dias).")
}

# 3) Subset en base R (sin dplyr::select) + coerciones seguras
dat_bivar <- OE2_b[, have, drop = FALSE]

# Coerción: puntualidad y días numéricos; transbordos entero (conteo)
dat_bivar$punt_15 <- suppressWarnings(as.numeric(dat_bivar$punt_15))
dat_bivar$dias    <- suppressWarnings(as.numeric(dat_bivar$dias))
dat_bivar$transb  <- suppressWarnings(as.integer(dat_bivar$transb))

# 4) NA-omit SOLO en estas 3 variables (política focal)
dat_bivar <- stats::na.omit(dat_bivar)

# 5) Resumen rápido
cat("\n== OE2 PREÁMBULO ==\n")
cat("Filas tras NA-omit focal (punt_15, transb, dias): ", nrow(dat_bivar), "\n")
cat("Rangos:\n")
cat("  punt_15: ", paste(range(dat_bivar$punt_15, na.rm = TRUE), collapse = " – "), "\n")
cat("  transb : ", paste(range(dat_bivar$transb,  na.rm = TRUE), collapse = " – "), "\n")
cat("  dias   : ", paste(range(dat_bivar$dias,    na.rm = TRUE), collapse = " – "), "\n")

# 6) (Opcional) guarda una copia para reutilizar en los siguientes chunks
OE2_dat_tripleta <- dat_bivar


```

```{r}
# Requiere ppcor
if (!requireNamespace("ppcor", quietly = TRUE)) install.packages("ppcor")

pc <- ppcor::pcor.test(
  OE2_dat_tripleta$punt_15,
  OE2_dat_tripleta$transb,
  OE2_dat_tripleta["dias"],
  method = "spearman"
)
print(pc)

```

```{r}
# Poisson
m_pois <- glm(transb ~ punt_15 + dias, family = poisson, data = OE2_dat_tripleta)
# Si hay sobre-dispersión, usa quasi-Poisson:
phi <- sum(residuals(m_pois, type="pearson")^2) / m_pois$df.residual
if (is.finite(phi) && phi > 1.5) {
  m_qp <- glm(transb ~ punt_15 + dias, family = quasipoisson, data = OE2_dat_tripleta)
  cat("Usando quasi-Poisson (phi ~", round(phi,2), ")\n")
  print(summary(m_qp))
} else {
  print(summary(m_pois))
}

```

```{r}
####################################################################################################################################
# OE2-B5 — Modelo de conteo: Transbordos ~ Puntualidad + Días
#   - Poisson; si hay sobredispersión, pasa a NegBin (MASS::glm.nb)
#   - Exporta: overdisp, coeficientes (IRR), predicciones y gráfico
#   - Sin dplyr::select (evita choques). NA-omit focal (3 vars).
#####################################################################

# Paquetes necesarios
if (!requireNamespace("broom", quietly = TRUE)) install.packages("broom")
if (!requireNamespace("MASS",  quietly = TRUE)) install.packages("MASS")
if (!requireNamespace("readr", quietly = TRUE)) install.packages("readr")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")

# 0) Verificaciones básicas
stopifnot(exists("OE2_b"))
need <- c("transb","punt_15","dias")
miss <- setdiff(need, names(OE2_b))
if (length(miss) > 0) {
  stop("Faltan columnas en OE2_b: ", paste(shQuote(miss), collapse=", "),
       "\nRevisa nombres (transb, punt_15, dias).")
}

# 1) Subset en base R + coerciones seguras
dat_m <- OE2_b[, need, drop = FALSE]
dat_m$transb  <- suppressWarnings(as.integer(dat_m$transb))   # conteo
dat_m$punt_15 <- suppressWarnings(as.numeric(dat_m$punt_15))  # 1–5
dat_m$dias    <- suppressWarnings(as.numeric(dat_m$dias))     # 1–7 típico

# 2) NA-omit SOLO en estas 3 variables
dat_m <- stats::na.omit(dat_m)

# 3) Modelo Poisson
m_pois <- glm(transb ~ punt_15 + dias, family = poisson, data = dat_m)

# 4) Chequeo de sobredispersión (Pearson; más estable que deviance/df)
phi_pearson <- sum(residuals(m_pois, type = "pearson")^2) / m_pois$df.residual
od_tab <- tibble::tibble(modelo = "Poisson", phi_pearson = phi_pearson)
readr::write_csv(od_tab, file.path("OE2_bivar","OE2_B5_overdispersion_check.csv"))
print(od_tab)

# 5) Escoger modelo final (umbral típico: phi > 1.5)
usa_nb  <- is.finite(phi_pearson) && phi_pearson > 1.5
if (usa_nb) {
  m_nb   <- MASS::glm.nb(transb ~ punt_15 + dias, data = dat_m)
  sel_mod <- m_nb
  mod_name <- "NegBin"
} else {
  sel_mod <- m_pois
  mod_name <- "Poisson"
}

# 6) Coeficientes como IRR (exp(beta)) + IC
coefs <- broom::tidy(sel_mod, conf.int = TRUE, exponentiate = TRUE)
coefs <- dplyr::mutate(coefs, modelo = mod_name)
readr::write_csv(coefs, file.path("OE2_bivar","OE2_B5_model_coefs_IRR.csv"))
print(coefs)

# 7) Predicción marginal por punt_15 (fijando dias = mediana observada)
dias_ref <- stats::median(dat_m$dias, na.rm = TRUE)
newd <- expand.grid(punt_15 = 1:5, dias = dias_ref)
newd$pred <- predict(sel_mod, newdata = newd, type = "response")
readr::write_csv(newd, file.path("OE2_bivar","OE2_B5_pred_por_punt.csv"))

# 8) Gráfico de predicción
p_pred <- ggplot2::ggplot(newd, ggplot2::aes(x = punt_15, y = pred)) +
  ggplot2::geom_line() + ggplot2::geom_point() +
  ggplot2::scale_x_continuous(breaks = 1:5) +
  ggplot2::labs(
    title = paste0("OE2 — Transbordos esperados vs Puntualidad (", mod_name, ")"),
    subtitle = paste0("Controlando por días = ", dias_ref),
    x = "Puntualidad (1–5)", y = "Transbordos esperados"
  ) +
  ggplot2::theme_minimal(base_size = 12)

ggplot2::ggsave(file.path("OE2_bivar","OE2_B5_pred_linea_por_punt.png"),
                plot = p_pred, width = 9, height = 6, dpi = 120)
p_pred

```

## Objetivo especifico 3:

### Analisis Univariado:

```{r}
#####################################################################
# OE3-U0 — Setup (paquetes, nombres de columnas, carpeta de salida)
#####################################################################
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(purrr)
library(rlang)

# Nombres (ajusta si difieren en tu BASE_FINAL)
col_sat  <- "Satisfacción con el tiempo que te toma llegar (1–5)"
col_est  <- "Estrés percibido durante el viaje (1–5)"
col_cans <- "Cansancio al llegar (1–5)"
col_seg  <- "Seguridad percibida (1–5)"
col_comf <- "Comodidad del transporte (1–5)"

col_transp <- "Tipo de transporte_final"            # para conteos de contexto (no filtramos)
col_turno  <- "Turno de ingreso usual"              # idem

# Carpeta de salidas univariado
dir.create("OE3_univariado", showWarnings = FALSE)

# Helper: coaccionar a escala 1–5 numérica (si viniera en texto)
a_15 <- function(x){
  x_num <- suppressWarnings(as.numeric(x))
  if (all(is.na(x_num)) || any(x_num < 1 | x_num > 5, na.rm = TRUE)) {
    x_chr <- as.character(x)
    rec <- dplyr::case_when(
      str_detect(x_chr, regex("^5$|^muy alto|^muy satisfech", TRUE)) ~ 5,
      str_detect(x_chr, regex("^4$|^alto|^satisfech", TRUE))         ~ 4,
      str_detect(x_chr, regex("^3$|^medio|^neutral", TRUE))          ~ 3,
      str_detect(x_chr, regex("^2$|^bajo", TRUE))                    ~ 2,
      str_detect(x_chr, regex("^1$|^muy bajo|^insatisfech", TRUE))   ~ 1,
      TRUE ~ NA_real_
    )
    return(as.numeric(rec))
  } else {
    return(as.numeric(x_num))
  }
}

```

```{r}
#####################################################################
# OE3-U1 — Construir vista de bienestar (sin filtrar por “principal”)
#####################################################################
OE3_uni <- BASE_FINAL %>%
  transmute(
    sat  = a_15(.data[[col_sat]]),
    est  = a_15(.data[[col_est]]),
    cans = a_15(.data[[col_cans]]),
    seg  = a_15(.data[[col_seg]]),
    comf = a_15(.data[[col_comf]]),
    transp = .data[[col_transp]],
    turno  = .data[[col_turno]]
  )

# Conteos de contexto (no se usan aún para filtrar)
contexto <- list(
  n_total = nrow(OE3_uni),
  n_transp = OE3_uni %>% count(transp, sort = TRUE),
  n_turno  = OE3_uni %>% count(turno,  sort = TRUE)
)
write_csv(contexto$n_transp, file.path("OE3_univariado","OE3_U1_conteo_transporte.csv"))
write_csv(contexto$n_turno,  file.path("OE3_univariado","OE3_U1_conteo_turno.csv"))

```

```{r}
#####################################################################
# OE3-U2 — Tabla de resumen (N, N_NA, Mediana, IQR, Min, Max) por variable
#           NA-omit por variable (solo para el cálculo de cada métrica)
#####################################################################

# Usa namespaces para evitar depender de library()
resumen_var <- function(v){
  x  <- OE3_uni[[v]]
  xv <- x[!is.na(x)]
  tibble::tibble(
    var     = v,
    N_total = length(x),
    N_valid = length(xv),
    N_NA    = sum(is.na(x)),                                 # <- antes: NA
    Median  = if (length(xv)>0) median(xv) else NA_real_,
    Q1      = if (length(xv)>0) quantile(xv, 0.25, names=FALSE) else NA_real_,
    Q3      = if (length(xv)>0) quantile(xv, 0.75, names=FALSE) else NA_real_,
    IQR     = if (length(xv)>0) IQR(xv) else NA_real_,
    Min     = if (length(xv)>0) min(xv) else NA_real_,
    Max     = if (length(xv)>0) max(xv) else NA_real_
  )
}

# Comprobar que OE3_uni existe y trae las columnas esperadas
stopifnot(exists("OE3_uni"))
faltan <- setdiff(c("sat","est","cans","seg","comf"), names(OE3_uni))
if (length(faltan) > 0) stop("Faltan columnas en OE3_uni: ", paste(faltan, collapse=", "))

tab_resumen <- purrr::map_dfr(c("sat","est","cans","seg","comf"), resumen_var)

dir.create("OE3_univariado", showWarnings = FALSE)
readr::write_csv(tab_resumen, file.path("OE3_univariado","OE3_U2_resumen_bienestar.csv"))
print(tab_resumen)


```

```{r}
#####################################################################
# OE3-U3 — Gráficos univariados por variable (barra + caja)
#           NA-omit SOLO de la variable graficada
#####################################################################

# Paquetes (usa nombres calificados para evitar conflictos)
library(ggplot2)
dir.create(file.path("OE3_univariado","plots"), showWarnings = FALSE)

plot_uni <- function(v, titulo){
  # Construye df con una sola columna 'x' desde OE3_uni[[v]]
  df <- OE3_uni |>
    dplyr::transmute(x = .data[[v]]) |>
    tidyr::drop_na(x) |>
    dplyr::mutate(xf = factor(x, levels = 1:5))

  # Barras (distribución Likert)
  p_bar <- ggplot(df, aes(x = xf)) +
    geom_bar() +
    labs(title = paste("Distribución de", titulo),
         x = paste0(titulo, " (1–5)"), y = "Frecuencia") +
    theme_minimal(base_size = 12)
  ggsave(file.path("OE3_univariado","plots", paste0("OE3_", v, "_bar.png")),
         p_bar, width = 8, height = 5, dpi = 120)

  # Caja (univariada)
  p_box <- ggplot(df, aes(y = x, x = 1)) +
    geom_boxplot(width = .25, outlier.alpha = 0.25) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = 1:5, limits = c(1,5)) +
    labs(title = paste("Caja univariada —", titulo),
         x = NULL, y = paste0(titulo, " (1–5)")) +
    theme_minimal(base_size = 12)
  ggsave(file.path("OE3_univariado","plots", paste0("OE3_", v, "_box.png")),
         p_box, width = 6, height = 5, dpi = 120)

  invisible(list(bar = p_bar, box = p_box))
}

# Ejecuta para las 5 variables de bienestar
plots_sat  <- plot_uni("sat",  "Satisfacción")
plots_est  <- plot_uni("est",  "Estrés")
plots_cans <- plot_uni("cans", "Cansancio")
plots_seg  <- plot_uni("seg",  "Seguridad")
plots_comf <- plot_uni("comf", "Comodidad")

cat("Exportados PNG: barras y cajas de sat/est/cans/seg/comf en OE3_univariado/plots\n")


```

```{r}
#####################################################################
# OE3-U2 — Tabla de resumen (N, N_NA, Mediana, IQR, Min, Max) por variable
#           NA-omit por variable (solo para calcular cada métrica)
#####################################################################

# Paquetes que usa este chunk
if (!requireNamespace("tibble", quietly = TRUE)) install.packages("tibble")
if (!requireNamespace("purrr",  quietly = TRUE)) install.packages("purrr")
if (!requireNamespace("readr",  quietly = TRUE)) install.packages("readr")

resumen_var <- function(v){
  x  <- OE3_uni[[v]]
  xv <- x[!is.na(x)]
  tibble::tibble(
    var     = v,
    N_total = length(x),
    N_valid = length(xv),
    N_NA    = sum(is.na(x)),                                # <- renombrado (antes `NA`)
    Median  = if (length(xv) > 0) median(xv) else NA_real_,
    Q1      = if (length(xv) > 0) quantile(xv, 0.25, names = FALSE) else NA_real_,
    Q3      = if (length(xv) > 0) quantile(xv, 0.75, names = FALSE) else NA_real_,
    IQR     = if (length(xv) > 0) IQR(xv) else NA_real_,
    Min     = if (length(xv) > 0) min(xv) else NA_real_,
    Max     = if (length(xv) > 0) max(xv) else NA_real_
  )
}

# Asegúrate de que OE3_uni existe y tiene las columnas
stopifnot(exists("OE3_uni"))
faltan <- setdiff(c("sat","est","cans","seg","comf"), names(OE3_uni))
if (length(faltan) > 0) stop("Faltan columnas en OE3_uni: ", paste(faltan, collapse=", "))

tab_resumen <- purrr::map_dfr(c("sat","est","cans","seg","comf"), resumen_var)

# Exporta y muestra
dir.create("OE3_univariado", showWarnings = FALSE)
readr::write_csv(tab_resumen, file.path("OE3_univariado","OE3_U2_resumen_bienestar.csv"))
print(tab_resumen)

```

### BIVARIADO:

```{r}
# CH-BASE_ALIGN — Construir BASE_ALIGN desde BASE_FINAL (robusto)
stopifnot(exists("BASE_FINAL"))

library(dplyr)
library(stringr)

# 0) Nombres fuente (ajusta si tus columnas tienen otros nombres)
col_transporte <- "Tipo de transporte_final"
col_turno      <- "Turno de ingreso usual"

# Verificaciones mínimas
if (!(col_transporte %in% names(BASE_FINAL))) {
  stop("No encuentro la columna ", shQuote(col_transporte), " en BASE_FINAL.")
}
if (!(col_turno %in% names(BASE_FINAL))) {
  stop("No encuentro la columna ", shQuote(col_turno), " en BASE_FINAL.")
}

# 1) Crear/asegurar transp_macro y turno en BASE_FINAL
BASE_FINAL <- BASE_FINAL %>%
  mutate(
    transp_macro = case_when(
      str_detect(.data[[col_transporte]], regex("auto|taxi|uber", ignore_case = TRUE))            ~ "Auto/Taxi",
      str_detect(.data[[col_transporte]], regex("metro|tren", ignore_case = TRUE))                ~ "Metro/Tren",
      str_detect(.data[[col_transporte]], regex("corredor|metropolitano", ignore_case = TRUE))    ~ "Corredor/Metropolitano",
      str_detect(.data[[col_transporte]], regex("bus|combi|colectivo", ignore_case = TRUE))       ~ "Bus/Combi",
      str_detect(.data[[col_transporte]], regex("bici|bicicleta|scooter|patin", ignore_case=TRUE))~ "Bici/Scooter",
      str_detect(.data[[col_transporte]], regex("camina|pie", ignore_case = TRUE))                ~ "A pie",
      TRUE                                                                                        ~ "Multimodal"
    ),
    turno = as.character(.data[[col_turno]])
  ) %>%
  mutate(
    transp_macro = factor(
      transp_macro,
      levels = c("A pie","Bici/Scooter","Bus/Combi","Corredor/Metropolitano","Metro/Tren","Auto/Taxi","Multimodal")
    ),
    turno = factor(turno)
  )

# 2) Mapeo de variables de bienestar (ajusta si tus nombres difieren)
map_vars <- c(
  sat  = "Satisfacción con el tiempo que te toma llegar (1–5)",
  est  = "Estrés percibido durante el viaje (1–5)",
  cans = "Cansancio al llegar (1–5)",
  seg  = "Seguridad percibida (1–5)",
  comf = "Comodidad del transporte (1–5)"
)

faltan <- setdiff(unname(map_vars), names(BASE_FINAL))
if (length(faltan) > 0) {
  stop("Faltan en BASE_FINAL estas columnas de bienestar:\n - ",
       paste(faltan, collapse = "\n - "))
}

# 3) Construir BASE_ALIGN (incluye alias 'problema' y, si existe, el nombre largo también)
BASE_ALIGN <- BASE_FINAL %>%
  transmute(
    transp_macro,
    turno = factor(turno),
    sat   = suppressWarnings(as.numeric(.data[[map_vars["sat"]]])),
    est   = suppressWarnings(as.numeric(.data[[map_vars["est"]]])),
    cans  = suppressWarnings(as.numeric(.data[[map_vars["cans"]]])),
    seg   = suppressWarnings(as.numeric(.data[[map_vars["seg"]]])),
    comf  = suppressWarnings(as.numeric(.data[[map_vars["comf"]]]))
  )

# 3.1) Inyectar problema (alias y, si quieres, el original para compatibilidad con código viejo)
if ("Principal_problema_al_trasladarte_FINAL" %in% names(BASE_FINAL)) {
  stopifnot(nrow(BASE_ALIGN) == nrow(BASE_FINAL))  # misma N, mismo orden
  BASE_ALIGN$problema <- BASE_FINAL$`Principal_problema_al_trasladarte_FINAL`
  # (Opcional) también agrego la columna con el nombre largo:
  BASE_ALIGN$`Principal_problema_al_trasladarte_FINAL` <- BASE_FINAL$`Principal_problema_al_trasladarte_FINAL`
} else {
  message("Aviso: 'Principal_problema_al_trasladarte_FINAL' no está en BASE_FINAL; BASE_ALIGN se crea sin 'problema'.")
}

# 4) Chequeo rápido
cat("OK: creado BASE_ALIGN.\n")
cat("Dimensiones: ", nrow(BASE_ALIGN), " x ", ncol(BASE_ALIGN), "\n", sep = "")
print(dplyr::count(BASE_ALIGN, turno, transp_macro))

```

```{r}
# --- OE3-1 (robusto): Tabla transporte × problema y mosaico con NA-omit por-variable ---

library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(forcats)
library(scales)

stopifnot(exists("BASE_ALIGN"))

# 0) Asegurar que 'problema' exista en BASE_ALIGN (alias del nombre largo en BASE_FINAL)
if (!("problema" %in% names(BASE_ALIGN))) {
  if ("Principal_problema_al_trasladarte_FINAL" %in% names(BASE_FINAL)) {
    stopifnot(nrow(BASE_ALIGN) == nrow(BASE_FINAL))  # alineación 1:1
    BASE_ALIGN$problema <- BASE_FINAL$`Principal_problema_al_trasladarte_FINAL`
    message("Incluida columna 'problema' en BASE_ALIGN a partir de BASE_FINAL.")
  } else {
    stop("No encuentro 'problema' ni 'Principal_problema_al_trasladarte_FINAL'. Revisa nombres.")
  }
}

# 1) Construir tabla de frecuencias (NA-omit SOLO en 'problema' y 'transp_macro' para este objetivo)
tab_tp <- BASE_ALIGN %>%
  filter(!is.na(problema), !is.na(transp_macro)) %>%  # NA-omit por-variable (política OE3)
  count(transp_macro, problema, name = "n") %>%
  group_by(problema) %>%
  mutate(p = 100 * n / sum(n)) %>%
  ungroup()

# Exportar tabla de frecuencias
write_csv(tab_tp, "OE3_tab_transporte_x_problema.csv")

# (Opcional) ordenar niveles para lectura: por tamaño total de cada problema y de cada transporte
orden_problema <- tab_tp %>% group_by(problema) %>% summarise(N = sum(n), .groups = "drop") %>%
  arrange(desc(N)) %>% pull(problema)
orden_transp   <- tab_tp %>% group_by(transp_macro) %>% summarise(N = sum(n), .groups = "drop") %>%
  arrange(N) %>% pull(transp_macro)  # ascendente para que quede de abajo a arriba

df_mosaic <- tab_tp %>%
  mutate(
    problema     = factor(problema, levels = orden_problema),
    transp_macro = factor(transp_macro, levels = orden_transp),
    p_col        = n / as.numeric(tapply(n, problema, sum)[as.character(problema)]) # proporción por columna
  )

# 2) Mosaico (proporción por columna de problema)
p_mosaic <- ggplot(
  df_mosaic,
  aes(x = problema, y = transp_macro, fill = p_col)
) +
  geom_tile(color = "white") +
  scale_fill_gradient(name = "Proporción\n(columna)", labels = percent_format(accuracy = 1)) +
  labs(
    title = "OE3 — Mosaico: Transporte (macro) × Problema principal",
    subtitle = "Celdas coloreadas por proporción dentro de cada problema (columna)",
    x = "Problema principal",
    y = "Transporte (macro)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 25, hjust = 1),
    panel.grid = element_blank()
  )

ggsave("OE3_mosaico_transporte_x_problema.png", p_mosaic, width = 12, height = 7, dpi = 120)

# Mensaje de cierre
cat("Exportados:\n - OE3_tab_transporte_x_problema.csv\n - OE3_mosaico_transporte_x_problema.png\n")

```

![](images/clipboard-1595583654.png)

```{r}
# --- OE3-3: Resumen (Mediana, IQR, Min, Max) por turno × transporte y por cada outcome ---

res_oe3 <- function(y){
  BASE_ALIGN %>%
    dplyr::select(y = all_of(y), transp_macro, turno) %>%
    tidyr::drop_na(y, transp_macro, turno) %>%
    dplyr::group_by(turno, transp_macro) %>%
    dplyr::summarise(resumen_num(y), .groups = "drop") %>%
    dplyr::mutate(var = y, .before = 1)
}

tabla_resumen_oe3 <- dplyr::bind_rows(
  res_oe3("sat"), res_oe3("est"), res_oe3("cans"),
  res_oe3("seg"), res_oe3("comf")
)

readr::write_csv(tabla_resumen_oe3, "OE3_resumen_bienestar_por_grupo.csv")
print(head(tabla_resumen_oe3, 15))

```

```{r}
# --- OE3-MOSAICO — Transporte (macro) × Problema principal -------------------
# Requisitos: haber corrido el CH-BASE_ALIGN (BASE_ALIGN con 'transp_macro' y
# 'Principal_problema_al_trasladarte_FINAL' creados/inyectados)

stopifnot(exists("BASE_ALIGN"))

library(dplyr)
library(ggplot2)
library(scales)
library(forcats)

# 0) Nombres defensivos
col_prob <- "Principal_problema_al_trasladarte_FINAL"
col_tran <- "transp_macro"

if (!all(c(col_prob, col_tran) %in% names(BASE_ALIGN))) {
  stop("Faltan columnas en BASE_ALIGN: ",
       paste(setdiff(c(col_prob, col_tran), names(BASE_ALIGN)), collapse = ", "))
}

# 1) Tabla de frecuencias y proporciones (por columna de problema)
tab_tp <- BASE_ALIGN %>%
  # Quitamos NA en ambos para el mosaico
  filter(!is.na(.data[[col_prob]]), !is.na(.data[[col_tran]])) %>%
  count(
    !!sym(col_tran),
    !!sym(col_prob),
    name = "n"
  ) %>%
  group_by(!!sym(col_prob)) %>%
  mutate(
    p_col = n / sum(n)  # Proporción dentro de cada problema (la columna)
  ) %>%
  ungroup()

# 2) (Opcional) Orden de ejes para lectura: problema por tamaño y transporte por tamaño global
orden_prob <- tab_tp %>%
  group_by(!!sym(col_prob)) %>%
  summarise(N = sum(n), .groups = "drop") %>%
  arrange(desc(N)) %>%
  pull(!!sym(col_prob))

orden_tran <- tab_tp %>%
  group_by(!!sym(col_tran)) %>%
  summarise(N = sum(n), .groups = "drop") %>%
  arrange(N) %>%            # de menor a mayor en el eje Y (sube visualmente)
  pull(!!sym(col_tran))

df_mosaic <- tab_tp %>%
  mutate(
    !!col_prob := factor(.data[[col_prob]], levels = orden_prob),
    !!col_tran := factor(.data[[col_tran]], levels = orden_tran)
  )

# 3) Gráfico mosaico (geom_tile con fill = proporción dentro del problema)
p_mosaic <- ggplot(
  df_mosaic,
  aes(x = .data[[col_prob]], y = .data[[col_tran]], fill = p_col)
) +
  geom_tile(color = "white", linewidth = 0.25) +
  # (Opcional) anotar recuentos en cada celda si n >= 3
  geom_text(aes(label = ifelse(n >= 3, n, "")), size = 3, color = "black") +
  scale_fill_gradient(
    name   = "Proporción dentro del problema",
    labels = label_percent(accuracy = 1),
    low = "#f1eef6", high = "#045a8d", na.value = "grey90"
  ) +
  labs(
    title    = "OE3 — Mosaico: Transporte (macro) × Problema principal",
    subtitle = "El color representa la proporción de cada tipo de transporte DENTRO de cada problema principal",
    x        = "Problema principal",
    y        = "Transporte (macro)",
    caption  = paste(
      "Lectura de la leyenda: para cada problema (columna), el color indica la proporción relativa de transporte.",
      "Las cifras dentro de cada celda (si aparecen) son los recuentos n≥3. NA omitidos en ambas variables."
    )
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x  = element_text(angle = 25, hjust = 1),
    panel.grid   = element_blank(),
    legend.position = "right"
  )

# 4) Exportar PNG
ggsave("OE3_mosaico_transporte_x_problema_final.png", p_mosaic, width = 13, height = 7.5, dpi = 120)
p_mosaic

```

![](images/clipboard-348833676.png)

```{r}

```
